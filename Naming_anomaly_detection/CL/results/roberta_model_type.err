Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2024-08-26 20:13:30.988 | INFO     | __main__:add_tokens:808 - We have added 117 tokens
Token indices sequence length is longer than the specified maximum sequence length for this model (1003 > 512). Running this sequence through the model will result in indexing errors
2024-08-26 20:13:37.972 | INFO     | __main__:main:832 - trim: 416
2024-08-26 20:13:37.973 | INFO     | __main__:main:841 - tau: 50
2024-08-26 20:13:37.973 | INFO     | __main__:main:852 - Single-label training mode. model_type
2024-08-26 20:13:44.731 | INFO     | __main__:train:142 - Training with model: roberta, lr: 5e-05, batch_size: 64, epoch: 30, loss_fn:CLCE, lambd:0.3
2024-08-26 20:13:44.732 | INFO     | __main__:train:157 - ====epoch #1====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<04:06,  5.88s/it]
Training:   5%|▍         | 2/43 [00:10<03:38,  5.32s/it]
Training:   7%|▋         | 3/43 [00:16<03:30,  5.27s/it]
Training:   9%|▉         | 4/43 [00:20<03:19,  5.11s/it]
Training:  12%|█▏        | 5/43 [00:25<03:10,  5.01s/it]
Training:  14%|█▍        | 6/43 [00:30<03:05,  5.03s/it]
Training:  16%|█▋        | 7/43 [00:35<03:01,  5.03s/it]
Training:  19%|█▊        | 8/43 [00:40<02:55,  5.02s/it]
Training:  21%|██        | 9/43 [00:45<02:51,  5.03s/it]
Training:  23%|██▎       | 10/43 [00:50<02:45,  5.02s/it]
Training:  26%|██▌       | 11/43 [00:55<02:39,  4.99s/it]
Training:  28%|██▊       | 12/43 [01:00<02:35,  5.01s/it]
Training:  30%|███       | 13/43 [01:05<02:29,  4.99s/it]
Training:  33%|███▎      | 14/43 [01:10<02:22,  4.92s/it]
Training:  35%|███▍      | 15/43 [01:15<02:16,  4.88s/it]
Training:  37%|███▋      | 16/43 [01:20<02:10,  4.85s/it]
Training:  40%|███▉      | 17/43 [01:24<02:05,  4.82s/it]
Training:  42%|████▏     | 18/43 [01:29<01:59,  4.79s/it]
Training:  44%|████▍     | 19/43 [01:34<01:54,  4.79s/it]
Training:  47%|████▋     | 20/43 [01:39<01:49,  4.78s/it]
Training:  49%|████▉     | 21/43 [01:44<01:46,  4.86s/it]
Training:  51%|█████     | 22/43 [01:49<01:42,  4.90s/it]
Training:  53%|█████▎    | 23/43 [01:53<01:37,  4.86s/it]
Training:  56%|█████▌    | 24/43 [01:58<01:31,  4.84s/it]
Training:  58%|█████▊    | 25/43 [02:03<01:26,  4.82s/it]
Training:  60%|██████    | 26/43 [02:08<01:21,  4.80s/it]
Training:  63%|██████▎   | 27/43 [02:13<01:17,  4.87s/it]
Training:  65%|██████▌   | 28/43 [02:18<01:13,  4.88s/it]
Training:  67%|██████▋   | 29/43 [02:23<01:10,  5.02s/it]
Training:  70%|██████▉   | 30/43 [02:28<01:05,  5.01s/it]
Training:  72%|███████▏  | 31/43 [02:33<00:59,  4.94s/it]
Training:  74%|███████▍  | 32/43 [02:38<00:53,  4.89s/it]
Training:  77%|███████▋  | 33/43 [02:43<00:49,  4.93s/it]
Training:  79%|███████▉  | 34/43 [02:47<00:44,  4.90s/it]
Training:  81%|████████▏ | 35/43 [02:52<00:39,  4.89s/it]
Training:  84%|████████▎ | 36/43 [02:57<00:33,  4.85s/it]
Training:  86%|████████▌ | 37/43 [03:02<00:28,  4.83s/it]
Training:  88%|████████▊ | 38/43 [03:07<00:24,  4.81s/it]
Training:  91%|█████████ | 39/43 [03:12<00:19,  4.87s/it]
Training:  93%|█████████▎| 40/43 [03:16<00:14,  4.86s/it]
Training:  95%|█████████▌| 41/43 [03:21<00:09,  4.92s/it]
Training:  98%|█████████▊| 42/43 [03:27<00:04,  4.95s/it]
Training: 100%|██████████| 43/43 [03:28<00:00,  3.92s/it]
Training: 100%|██████████| 43/43 [03:28<00:00,  4.85s/it]
2024-08-26 20:17:13.287 | INFO     | __main__:train:206 - Epoch 1 Average Loss: 5.60809
2024-08-26 20:17:14.971 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL1*****
2024-08-26 20:17:14.971 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:17:14.971 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.38s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.26s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.21s/it]
Evaluating:  36%|███▋      | 4/11 [00:04<00:08,  1.19s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.17s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.16s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.16s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.15s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.15s/it]
Evaluating:  91%|█████████ | 10/11 [00:11<00:01,  1.15s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.03it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.12s/it]
2024-08-26 20:17:27.333 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.4211    0.7273    0.5333        33
audio-spectrogram-transformer     0.7500    0.6000    0.6667         5
                         bart     0.3830    0.8182    0.5217        22
                         beit        nan    0.0000       nan         6
                         bert     0.8382    0.8261    0.8321        69
                     big_bird     0.0000    0.0000       nan        22
              bigbird_pegasus     1.0000    0.1250    0.2222         8
                   blenderbot        nan    0.0000       nan         5
                    camembert     0.1667    0.0400    0.0645        25
                       canine     1.0000    1.0000    1.0000         2
                         clip        nan    0.0000       nan         4
                      codegen        nan    0.0000       nan         4
                     convnext        nan    0.0000       nan         7
                   convnextv2        nan    0.0000       nan         2
                         ctrl        nan    0.0000       nan         1
                      deberta     0.8824    1.0000    0.9375        15
                   deberta-v2     0.1472    0.9231    0.2540        26
                         detr        nan    0.0000       nan         3
                   distilbert     0.3857    0.7714    0.5143        35
                          dpr        nan    0.0000       nan         9
                      electra     0.2000    0.0645    0.0976        31
                          esm        nan    0.0000       nan         7
                         fnet        nan    0.0000       nan         2
                         glpn        nan    0.0000       nan         1
                         gpt2        nan    0.0000       nan         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo        nan    0.0000       nan         3
                     gpt_neox        nan    0.0000       nan         2
                         gptj        nan    0.0000       nan         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3        nan    0.0000       nan         2
                          led        nan    0.0000       nan         5
                         lilt        nan    0.0000       nan         4
                        llama        nan    0.0000       nan         3
                   longformer     0.6765    1.0000    0.8070        23
                       longt5        nan    0.0000       nan         3
                      m2m_100        nan    0.0000       nan         4
                       marian        nan    0.0000       nan         5
                  mask2former        nan    0.0000       nan         8
                   maskformer        nan    0.0000       nan         5
                        mbart        nan    0.0000       nan         2
                   mobilebert        nan    0.0000       nan         5
                 mobilenet_v2        nan    0.0000       nan         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    0.4118    0.5833        17
                          mpt        nan    0.0000       nan         6
                          mt5        nan    0.0000       nan         6
                          opt        nan    0.0000       nan         5
                      pegasus        nan    0.0000       nan         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart        nan    0.0000       nan         6
                       regnet        nan    0.0000       nan         3
                       resnet        nan    0.0000       nan         4
                      roberta     0.3810    0.6154    0.4706        52
                         rwkv        nan    0.0000       nan         5
                          sam        nan    0.0000       nan         2
                    segformer        nan    0.0000       nan         3
               speech_to_text        nan    0.0000       nan         4
                     speecht5        nan    0.0000       nan         2
                         swin        nan    0.0000       nan         6
                           t5     0.3333    0.1429    0.2000        14
                    unispeech        nan    0.0000       nan         4
                unispeech-sat        nan    0.0000       nan         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit        nan    0.0000       nan         3
                     wav2vec2     0.1667    0.1429    0.1538        28
                        wavlm     0.3462    0.7500    0.4737        12
                      whisper        nan    0.0000       nan         8
                         xglm        nan    0.0000       nan         4
                  xlm-roberta     0.2642    0.4667    0.3373        30
                        xlnet        nan    0.0000       nan        11
                        yolos        nan    0.0000       nan         3

                     accuracy                         0.3906       681
                    macro avg     0.5171    0.1587    0.5089       681
                 weighted avg     0.4460    0.3906    0.4772       681

2024-08-26 20:17:27.333 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.3906
2024-08-26 20:17:27.343 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:17:27.343 | INFO     | __main__:eval:453 - Validation loss: 5.390675176273692
2024-08-26 20:17:27.354 | INFO     | __main__:train:255 - epochs: 1
2024-08-26 20:17:27.355 | INFO     | __main__:train:256 - train_loss: [5.608091232388518]
2024-08-26 20:17:27.355 | INFO     | __main__:train:257 - test_loss: [5.390675176273692]
2024-08-26 20:17:27.355 | INFO     | __main__:train:157 - ====epoch #2====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:33,  5.09s/it]
Training:   5%|▍         | 2/43 [00:10<03:25,  5.01s/it]
Training:   7%|▋         | 3/43 [00:15<03:19,  4.99s/it]
Training:   9%|▉         | 4/43 [00:19<03:13,  4.97s/it]
Training:  12%|█▏        | 5/43 [00:24<03:08,  4.96s/it]
Training:  14%|█▍        | 6/43 [00:29<03:03,  4.95s/it]
Training:  16%|█▋        | 7/43 [00:34<02:57,  4.93s/it]
Training:  19%|█▊        | 8/43 [00:39<02:52,  4.93s/it]
Training:  21%|██        | 9/43 [00:44<02:47,  4.91s/it]
Training:  23%|██▎       | 10/43 [00:49<02:43,  4.94s/it]
Training:  26%|██▌       | 11/43 [00:54<02:35,  4.87s/it]
Training:  28%|██▊       | 12/43 [00:58<02:29,  4.84s/it]
Training:  30%|███       | 13/43 [01:03<02:24,  4.81s/it]
Training:  33%|███▎      | 14/43 [01:08<02:19,  4.79s/it]
Training:  35%|███▍      | 15/43 [01:13<02:13,  4.77s/it]
Training:  37%|███▋      | 16/43 [01:18<02:10,  4.84s/it]
Training:  40%|███▉      | 17/43 [01:22<02:05,  4.82s/it]
Training:  42%|████▏     | 18/43 [01:27<02:00,  4.81s/it]
Training:  44%|████▍     | 19/43 [01:32<01:54,  4.79s/it]
Training:  47%|████▋     | 20/43 [01:37<01:49,  4.78s/it]
Training:  49%|████▉     | 21/43 [01:41<01:44,  4.76s/it]
Training:  51%|█████     | 22/43 [01:46<01:39,  4.75s/it]
Training:  53%|█████▎    | 23/43 [01:51<01:35,  4.75s/it]
Training:  56%|█████▌    | 24/43 [01:56<01:30,  4.75s/it]
Training:  58%|█████▊    | 25/43 [02:00<01:25,  4.74s/it]
Training:  60%|██████    | 26/43 [02:05<01:20,  4.75s/it]
Training:  63%|██████▎   | 27/43 [02:10<01:16,  4.76s/it]
Training:  65%|██████▌   | 28/43 [02:15<01:11,  4.75s/it]
Training:  67%|██████▋   | 29/43 [02:19<01:06,  4.75s/it]
Training:  70%|██████▉   | 30/43 [02:24<01:01,  4.75s/it]
Training:  72%|███████▏  | 31/43 [02:29<00:56,  4.74s/it]
Training:  74%|███████▍  | 32/43 [02:34<00:52,  4.74s/it]
Training:  77%|███████▋  | 33/43 [02:38<00:47,  4.73s/it]
Training:  79%|███████▉  | 34/43 [02:43<00:42,  4.74s/it]
Training:  81%|████████▏ | 35/43 [02:48<00:37,  4.75s/it]
Training:  84%|████████▎ | 36/43 [02:53<00:33,  4.75s/it]
Training:  86%|████████▌ | 37/43 [02:57<00:28,  4.75s/it]
Training:  88%|████████▊ | 38/43 [03:02<00:23,  4.75s/it]
Training:  91%|█████████ | 39/43 [03:07<00:19,  4.75s/it]
Training:  93%|█████████▎| 40/43 [03:12<00:14,  4.75s/it]
Training:  95%|█████████▌| 41/43 [03:16<00:09,  4.73s/it]
Training:  98%|█████████▊| 42/43 [03:21<00:04,  4.79s/it]
Training: 100%|██████████| 43/43 [03:23<00:00,  3.76s/it]
Training: 100%|██████████| 43/43 [03:23<00:00,  4.72s/it]
2024-08-26 20:20:50.518 | INFO     | __main__:train:206 - Epoch 2 Average Loss: 4.29877
2024-08-26 20:20:52.110 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL2*****
2024-08-26 20:20:52.110 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:20:52.110 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.40s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.30s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.26s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.25s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.22s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.20s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.19s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.19s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.18s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.17s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.00it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.16s/it]
2024-08-26 20:21:04.835 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9429    1.0000    0.9706        33
audio-spectrogram-transformer     0.6250    1.0000    0.7692         5
                         bart     0.6000    0.9545    0.7368        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.7692    0.8696    0.8163        69
                     big_bird     0.7857    0.5000    0.6111        22
              bigbird_pegasus     1.0000    0.8750    0.9333         8
                   blenderbot        nan    0.0000       nan         5
                    camembert     0.2308    0.6000    0.3333        25
                       canine     0.2500    1.0000    0.4000         2
                         clip        nan    0.0000       nan         4
                      codegen     1.0000    0.7500    0.8571         4
                     convnext     0.7778    1.0000    0.8750         7
                   convnextv2        nan    0.0000       nan         2
                         ctrl        nan    0.0000       nan         1
                      deberta     0.7500    1.0000    0.8571        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr        nan    0.0000       nan         3
                   distilbert     0.8649    0.9143    0.8889        35
                          dpr        nan    0.0000       nan         9
                      electra     0.3056    0.3548    0.3284        31
                          esm     1.0000    0.8571    0.9231         7
                         fnet        nan    0.0000       nan         2
                         glpn        nan    0.0000       nan         1
                         gpt2        nan    0.0000       nan         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.5000    0.6667    0.5714         3
                     gpt_neox        nan    0.0000       nan         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     0.6667    1.0000    0.8000         2
                          led     1.0000    0.8000    0.8889         5
                         lilt        nan    0.0000       nan         4
                        llama        nan    0.0000       nan         3
                   longformer     0.9583    1.0000    0.9787        23
                       longt5     1.0000    0.6667    0.8000         3
                      m2m_100        nan    0.0000       nan         4
                       marian     0.0000    0.0000       nan         5
                  mask2former     0.3333    0.1250    0.1818         8
                   maskformer     1.0000    0.2000    0.3333         5
                        mbart        nan    0.0000       nan         2
                   mobilebert     0.6667    0.8000    0.7273         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     0.3333    1.0000    0.5000         1
                        mpnet     0.9444    1.0000    0.9714        17
                          mpt     1.0000    0.1667    0.2857         6
                          mt5        nan    0.0000       nan         6
                          opt     0.1667    1.0000    0.2857         5
                      pegasus        nan    0.0000       nan         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     0.5556    0.8333    0.6667         6
                       regnet     0.3333    1.0000    0.5000         3
                       resnet        nan    0.0000       nan         4
                      roberta     0.5625    0.5192    0.5400        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam        nan    0.0000       nan         2
                    segformer     0.6667    0.6667    0.6667         3
               speech_to_text        nan    0.0000       nan         4
                     speecht5        nan    0.0000       nan         2
                         swin     0.5455    1.0000    0.7059         6
                           t5     0.2857    0.5714    0.3810        14
                    unispeech        nan    0.0000       nan         4
                unispeech-sat     0.1667    0.5000    0.2500         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit        nan    0.0000       nan         3
                     wav2vec2     0.5238    0.7857    0.6286        28
                        wavlm        nan    0.0000       nan        12
                      whisper        nan    0.0000       nan         8
                         xglm        nan    0.0000       nan         4
                  xlm-roberta     0.9091    0.3333    0.4878        30
                        xlnet     0.8182    0.8182    0.8182        11
                        yolos        nan    0.0000       nan         3

                     accuracy                         0.6241       681
                    macro avg     0.6866    0.4359    0.6788       681
                 weighted avg     0.7091    0.6241    0.6994       681

2024-08-26 20:21:04.836 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.6241
2024-08-26 20:21:04.839 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:21:04.839 | INFO     | __main__:eval:453 - Validation loss: 4.626526594161987
2024-08-26 20:21:04.852 | INFO     | __main__:train:255 - epochs: 2
2024-08-26 20:21:04.852 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743]
2024-08-26 20:21:04.852 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987]
2024-08-26 20:21:04.852 | INFO     | __main__:train:157 - ====epoch #3====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:27,  4.94s/it]
Training:   5%|▍         | 2/43 [00:09<03:19,  4.87s/it]
Training:   7%|▋         | 3/43 [00:14<03:12,  4.82s/it]
Training:   9%|▉         | 4/43 [00:19<03:06,  4.79s/it]
Training:  12%|█▏        | 5/43 [00:24<03:02,  4.79s/it]
Training:  14%|█▍        | 6/43 [00:29<03:01,  4.90s/it]
Training:  16%|█▋        | 7/43 [00:34<02:57,  4.92s/it]
Training:  19%|█▊        | 8/43 [00:38<02:50,  4.88s/it]
Training:  21%|██        | 9/43 [00:43<02:44,  4.84s/it]
Training:  23%|██▎       | 10/43 [00:48<02:42,  4.93s/it]
Training:  26%|██▌       | 11/43 [00:53<02:38,  4.96s/it]
Training:  28%|██▊       | 12/43 [00:58<02:33,  4.96s/it]
Training:  30%|███       | 13/43 [01:03<02:28,  4.96s/it]
Training:  33%|███▎      | 14/43 [01:08<02:24,  4.97s/it]
Training:  35%|███▍      | 15/43 [01:13<02:19,  4.99s/it]
Training:  37%|███▋      | 16/43 [01:18<02:14,  4.97s/it]
Training:  40%|███▉      | 17/43 [01:23<02:08,  4.95s/it]
Training:  42%|████▏     | 18/43 [01:28<02:03,  4.94s/it]
Training:  44%|████▍     | 19/43 [01:33<01:57,  4.90s/it]
Training:  47%|████▋     | 20/43 [01:38<01:51,  4.84s/it]
Training:  49%|████▉     | 21/43 [01:42<01:46,  4.82s/it]
Training:  51%|█████     | 22/43 [01:47<01:40,  4.80s/it]
Training:  53%|█████▎    | 23/43 [01:52<01:35,  4.80s/it]
Training:  56%|█████▌    | 24/43 [01:57<01:32,  4.87s/it]
Training:  58%|█████▊    | 25/43 [02:02<01:27,  4.88s/it]
Training:  60%|██████    | 26/43 [02:07<01:22,  4.85s/it]
Training:  63%|██████▎   | 27/43 [02:11<01:17,  4.83s/it]
Training:  65%|██████▌   | 28/43 [02:16<01:12,  4.83s/it]
Training:  67%|██████▋   | 29/43 [02:21<01:08,  4.87s/it]
Training:  70%|██████▉   | 30/43 [02:26<01:03,  4.85s/it]
Training:  72%|███████▏  | 31/43 [02:31<00:57,  4.83s/it]
Training:  74%|███████▍  | 32/43 [02:36<00:53,  4.87s/it]
Training:  77%|███████▋  | 33/43 [02:40<00:48,  4.85s/it]
Training:  79%|███████▉  | 34/43 [02:45<00:43,  4.83s/it]
Training:  81%|████████▏ | 35/43 [02:50<00:38,  4.79s/it]
Training:  84%|████████▎ | 36/43 [02:55<00:33,  4.78s/it]
Training:  86%|████████▌ | 37/43 [02:59<00:28,  4.77s/it]
Training:  88%|████████▊ | 38/43 [03:04<00:23,  4.77s/it]
Training:  91%|█████████ | 39/43 [03:09<00:19,  4.76s/it]
Training:  93%|█████████▎| 40/43 [03:14<00:14,  4.74s/it]
Training:  95%|█████████▌| 41/43 [03:18<00:09,  4.74s/it]
Training:  98%|█████████▊| 42/43 [03:23<00:04,  4.75s/it]
Training: 100%|██████████| 43/43 [03:25<00:00,  3.74s/it]
Training: 100%|██████████| 43/43 [03:25<00:00,  4.77s/it]
2024-08-26 20:24:29.971 | INFO     | __main__:train:206 - Epoch 3 Average Loss: 3.81906
2024-08-26 20:24:31.560 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL3*****
2024-08-26 20:24:31.560 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:24:31.560 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.31s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.28s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.24s/it]
Evaluating:  36%|███▋      | 4/11 [00:04<00:08,  1.21s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.18s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.17s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.19s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.19s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.17s/it]
Evaluating:  91%|█████████ | 10/11 [00:11<00:01,  1.17s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.01it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.14s/it]
2024-08-26 20:24:44.101 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9429    1.0000    0.9706        33
audio-spectrogram-transformer     1.0000    0.8000    0.8889         5
                         bart     0.6875    1.0000    0.8148        22
                         beit     0.8000    0.6667    0.7273         6
                         bert     0.8108    0.8696    0.8392        69
                     big_bird     0.5250    0.9545    0.6774        22
              bigbird_pegasus     1.0000    0.7500    0.8571         8
                   blenderbot        nan    0.0000       nan         5
                    camembert        nan    0.0000       nan        25
                       canine     0.6667    1.0000    0.8000         2
                         clip        nan    0.0000       nan         4
                      codegen     1.0000    0.7500    0.8571         4
                     convnext     0.7000    1.0000    0.8235         7
                   convnextv2        nan    0.0000       nan         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     1.0000    0.6667    0.8000         3
                   distilbert     0.9706    0.9429    0.9565        35
                          dpr        nan    0.0000       nan         9
                      electra     0.3226    0.3226    0.3226        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet        nan    0.0000       nan         2
                         glpn        nan    0.0000       nan         1
                         gpt2        nan    0.0000       nan         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.3333    0.6667    0.4444         3
                     gpt_neox        nan    0.0000       nan         2
                         gptj     0.8000    0.6667    0.7273         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     0.5000    0.8000    0.6154         5
                         lilt        nan    0.0000       nan         4
                        llama        nan    0.0000       nan         3
                   longformer     0.9524    0.8696    0.9091        23
                       longt5     1.0000    0.6667    0.8000         3
                      m2m_100     1.0000    0.7500    0.8571         4
                       marian        nan    0.0000       nan         5
                  mask2former     1.0000    0.1250    0.2222         8
                   maskformer     1.0000    0.6000    0.7500         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     0.8000    0.8000    0.8000         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     0.5000    1.0000    0.6667         1
                        mpnet     0.9444    1.0000    0.9714        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5        nan    0.0000       nan         6
                          opt     0.4167    1.0000    0.5882         5
                      pegasus     1.0000    0.8571    0.9231         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     0.5455    1.0000    0.7059         6
                       regnet     0.5000    1.0000    0.6667         3
                       resnet     0.3333    0.2500    0.2857         4
                      roberta     0.4368    0.7308    0.5468        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam        nan    0.0000       nan         2
                    segformer     0.7500    1.0000    0.8571         3
               speech_to_text     0.0000    0.0000       nan         4
                     speecht5        nan    0.0000       nan         2
                         swin     0.5455    1.0000    0.7059         6
                           t5     0.4286    0.8571    0.5714        14
                    unispeech     1.0000    0.5000    0.6667         4
                unispeech-sat     0.0000    0.0000       nan         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    0.3333    0.5000         3
                     wav2vec2     0.6250    0.8929    0.7353        28
                        wavlm        nan    0.0000       nan        12
                      whisper     1.0000    0.6250    0.7692         8
                         xglm     0.4286    0.7500    0.5455         4
                  xlm-roberta     0.4667    0.2333    0.3111        30
                        xlnet     0.5263    0.9091    0.6667        11
                        yolos     0.7500    1.0000    0.8571         3

                     accuracy                         0.6887       681
                    macro avg     0.7453    0.5490    0.7543       681
                 weighted avg     0.7313    0.6887    0.7447       681

2024-08-26 20:24:44.101 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.6887
2024-08-26 20:24:44.104 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:24:44.104 | INFO     | __main__:eval:453 - Validation loss: 4.27214258367365
2024-08-26 20:24:44.111 | INFO     | __main__:train:255 - epochs: 3
2024-08-26 20:24:44.111 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476]
2024-08-26 20:24:44.112 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365]
2024-08-26 20:24:44.112 | INFO     | __main__:train:157 - ====epoch #4====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:29,  4.98s/it]
Training:   5%|▍         | 2/43 [00:10<03:25,  5.01s/it]
Training:   7%|▋         | 3/43 [00:15<03:21,  5.03s/it]
Training:   9%|▉         | 4/43 [00:20<03:16,  5.03s/it]
Training:  12%|█▏        | 5/43 [00:25<03:11,  5.04s/it]
Training:  14%|█▍        | 6/43 [00:30<03:06,  5.03s/it]
Training:  16%|█▋        | 7/43 [00:35<02:59,  4.99s/it]
Training:  19%|█▊        | 8/43 [00:39<02:52,  4.92s/it]
Training:  21%|██        | 9/43 [00:44<02:45,  4.87s/it]
Training:  23%|██▎       | 10/43 [00:49<02:39,  4.83s/it]
Training:  26%|██▌       | 11/43 [00:54<02:33,  4.81s/it]
Training:  28%|██▊       | 12/43 [00:58<02:28,  4.79s/it]
Training:  30%|███       | 13/43 [01:03<02:24,  4.82s/it]
Training:  33%|███▎      | 14/43 [01:08<02:18,  4.79s/it]
Training:  35%|███▍      | 15/43 [01:13<02:13,  4.78s/it]
Training:  37%|███▋      | 16/43 [01:17<02:08,  4.76s/it]
Training:  40%|███▉      | 17/43 [01:22<02:03,  4.75s/it]
Training:  42%|████▏     | 18/43 [01:27<01:58,  4.75s/it]
Training:  44%|████▍     | 19/43 [01:32<01:53,  4.74s/it]
Training:  47%|████▋     | 20/43 [01:36<01:48,  4.73s/it]
Training:  49%|████▉     | 21/43 [01:41<01:44,  4.73s/it]
Training:  51%|█████     | 22/43 [01:46<01:39,  4.73s/it]
Training:  53%|█████▎    | 23/43 [01:51<01:34,  4.74s/it]
Training:  56%|█████▌    | 24/43 [01:55<01:29,  4.73s/it]
Training:  58%|█████▊    | 25/43 [02:00<01:24,  4.71s/it]
Training:  60%|██████    | 26/43 [02:05<01:20,  4.71s/it]
Training:  63%|██████▎   | 27/43 [02:09<01:15,  4.72s/it]
Training:  65%|██████▌   | 28/43 [02:14<01:10,  4.73s/it]
Training:  67%|██████▋   | 29/43 [02:19<01:06,  4.73s/it]
Training:  70%|██████▉   | 30/43 [02:24<01:01,  4.74s/it]
Training:  72%|███████▏  | 31/43 [02:28<00:56,  4.73s/it]
Training:  74%|███████▍  | 32/43 [02:33<00:52,  4.74s/it]
Training:  77%|███████▋  | 33/43 [02:38<00:47,  4.72s/it]
Training:  79%|███████▉  | 34/43 [02:43<00:42,  4.73s/it]
Training:  81%|████████▏ | 35/43 [02:47<00:37,  4.72s/it]
Training:  84%|████████▎ | 36/43 [02:52<00:33,  4.72s/it]
Training:  86%|████████▌ | 37/43 [02:57<00:28,  4.73s/it]
Training:  88%|████████▊ | 38/43 [03:01<00:23,  4.72s/it]
Training:  91%|█████████ | 39/43 [03:06<00:18,  4.73s/it]
Training:  93%|█████████▎| 40/43 [03:11<00:14,  4.73s/it]
Training:  95%|█████████▌| 41/43 [03:16<00:09,  4.73s/it]
Training:  98%|█████████▊| 42/43 [03:20<00:04,  4.74s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  3.73s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  4.70s/it]
2024-08-26 20:28:06.342 | INFO     | __main__:train:206 - Epoch 4 Average Loss: 3.48744
2024-08-26 20:28:07.926 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL4*****
2024-08-26 20:28:07.926 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:28:07.926 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.42s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.27s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.22s/it]
Evaluating:  36%|███▋      | 4/11 [00:04<00:08,  1.20s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.18s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.17s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.17s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.16s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.16s/it]
Evaluating:  91%|█████████ | 10/11 [00:11<00:01,  1.19s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.01s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.14s/it]
2024-08-26 20:28:20.482 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9167    1.0000    0.9565        33
audio-spectrogram-transformer     1.0000    0.2000    0.3333         5
                         bart     0.8462    1.0000    0.9167        22
                         beit     0.8000    0.6667    0.7273         6
                         bert     0.8000    0.8696    0.8333        69
                     big_bird     0.9474    0.8182    0.8780        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot        nan    0.0000       nan         5
                    camembert        nan    0.0000       nan        25
                       canine     0.6667    1.0000    0.8000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     0.7500    0.7500    0.7500         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.4000    0.6667    0.5000         3
                   distilbert     1.0000    0.9429    0.9706        35
                          dpr        nan    0.0000       nan         9
                      electra     0.3396    0.5806    0.4286        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet        nan    0.0000       nan         2
                         glpn        nan    0.0000       nan         1
                         gpt2        nan    0.0000       nan         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.6000    1.0000    0.7500         3
                     gpt_neox        nan    0.0000       nan         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    0.4000    0.5714         5
                         lilt        nan    0.0000       nan         4
                        llama        nan    0.0000       nan         3
                   longformer     0.8846    1.0000    0.9388        23
                       longt5     0.5000    0.6667    0.5714         3
                      m2m_100     1.0000    0.5000    0.6667         4
                       marian        nan    0.0000       nan         5
                  mask2former     1.0000    0.5000    0.6667         8
                   maskformer     1.0000    0.4000    0.5714         5
                        mbart     0.6667    1.0000    0.8000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.1667    0.2857         6
                          opt     0.8333    1.0000    0.9091         5
                      pegasus     0.6000    0.8571    0.7059         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     0.8333    0.8333    0.8333         6
                       regnet     0.4000    0.6667    0.5000         3
                       resnet     0.5000    0.2500    0.3333         4
                      roberta     0.4301    0.7692    0.5517        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam        nan    0.0000       nan         2
                    segformer     0.7500    1.0000    0.8571         3
               speech_to_text     0.3333    0.7500    0.4615         4
                     speecht5        nan    0.0000       nan         2
                         swin     0.7500    1.0000    0.8571         6
                           t5     0.4583    0.7857    0.5789        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.0000    0.0000       nan         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    0.6667    0.8000         3
                     wav2vec2     0.5952    0.8929    0.7143        28
                        wavlm        nan    0.0000       nan        12
                      whisper     0.8750    0.8750    0.8750         8
                         xglm     1.0000    0.7500    0.8571         4
                  xlm-roberta     1.0000    0.2333    0.3784        30
                        xlnet     0.4583    1.0000    0.6286        11
                        yolos     0.4286    1.0000    0.6000         3

                     accuracy                         0.7122       681
                    macro avg     0.7931    0.5795    0.7522       681
                 weighted avg     0.7906    0.7122    0.7624       681

2024-08-26 20:28:20.482 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.7122
2024-08-26 20:28:20.486 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:28:20.486 | INFO     | __main__:eval:453 - Validation loss: 4.099447033622048
2024-08-26 20:28:20.496 | INFO     | __main__:train:255 - epochs: 4
2024-08-26 20:28:20.496 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346]
2024-08-26 20:28:20.497 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048]
2024-08-26 20:28:20.497 | INFO     | __main__:train:157 - ====epoch #5====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:30,  5.01s/it]
Training:   5%|▍         | 2/43 [00:10<03:25,  5.00s/it]
Training:   7%|▋         | 3/43 [00:14<03:19,  4.99s/it]
Training:   9%|▉         | 4/43 [00:19<03:12,  4.92s/it]
Training:  12%|█▏        | 5/43 [00:24<03:04,  4.86s/it]
Training:  14%|█▍        | 6/43 [00:29<02:59,  4.84s/it]
Training:  16%|█▋        | 7/43 [00:34<02:53,  4.82s/it]
Training:  19%|█▊        | 8/43 [00:38<02:48,  4.80s/it]
Training:  21%|██        | 9/43 [00:43<02:43,  4.80s/it]
Training:  23%|██▎       | 10/43 [00:48<02:38,  4.79s/it]
Training:  26%|██▌       | 11/43 [00:53<02:32,  4.77s/it]
Training:  28%|██▊       | 12/43 [00:57<02:27,  4.76s/it]
Training:  30%|███       | 13/43 [01:02<02:22,  4.75s/it]
Training:  33%|███▎      | 14/43 [01:07<02:17,  4.73s/it]
Training:  35%|███▍      | 15/43 [01:12<02:15,  4.83s/it]
Training:  37%|███▋      | 16/43 [01:17<02:11,  4.88s/it]
Training:  40%|███▉      | 17/43 [01:22<02:07,  4.92s/it]
Training:  42%|████▏     | 18/43 [01:27<02:02,  4.91s/it]
Training:  44%|████▍     | 19/43 [01:32<01:57,  4.88s/it]
Training:  47%|████▋     | 20/43 [01:36<01:51,  4.85s/it]
Training:  49%|████▉     | 21/43 [01:41<01:46,  4.84s/it]
Training:  51%|█████     | 22/43 [01:46<01:41,  4.83s/it]
Training:  53%|█████▎    | 23/43 [01:51<01:36,  4.81s/it]
Training:  56%|█████▌    | 24/43 [01:55<01:30,  4.77s/it]
Training:  58%|█████▊    | 25/43 [02:00<01:25,  4.76s/it]
Training:  60%|██████    | 26/43 [02:05<01:21,  4.81s/it]
Training:  63%|██████▎   | 27/43 [02:10<01:16,  4.79s/it]
Training:  65%|██████▌   | 28/43 [02:15<01:11,  4.78s/it]
Training:  67%|██████▋   | 29/43 [02:19<01:06,  4.78s/it]
Training:  70%|██████▉   | 30/43 [02:24<01:03,  4.87s/it]
Training:  72%|███████▏  | 31/43 [02:30<00:59,  4.93s/it]
Training:  74%|███████▍  | 32/43 [02:34<00:54,  4.94s/it]
Training:  77%|███████▋  | 33/43 [02:39<00:48,  4.90s/it]
Training:  79%|███████▉  | 34/43 [02:44<00:43,  4.86s/it]
Training:  81%|████████▏ | 35/43 [02:49<00:39,  4.89s/it]
Training:  84%|████████▎ | 36/43 [02:54<00:33,  4.86s/it]
Training:  86%|████████▌ | 37/43 [02:59<00:28,  4.82s/it]
Training:  88%|████████▊ | 38/43 [03:03<00:24,  4.82s/it]
Training:  91%|█████████ | 39/43 [03:08<00:19,  4.83s/it]
Training:  93%|█████████▎| 40/43 [03:13<00:14,  4.83s/it]
Training:  95%|█████████▌| 41/43 [03:18<00:09,  4.86s/it]
Training:  98%|█████████▊| 42/43 [03:23<00:04,  4.84s/it]
Training: 100%|██████████| 43/43 [03:24<00:00,  3.82s/it]
Training: 100%|██████████| 43/43 [03:24<00:00,  4.76s/it]
2024-08-26 20:31:45.212 | INFO     | __main__:train:206 - Epoch 5 Average Loss: 3.44284
2024-08-26 20:31:46.809 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL5*****
2024-08-26 20:31:46.809 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:31:46.809 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.43s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.28s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.23s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.24s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.24s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.24s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.22s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.20s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.18s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.17s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.02it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.16s/it]
2024-08-26 20:31:59.531 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.8462    1.0000    0.9167        33
audio-spectrogram-transformer     0.8333    1.0000    0.9091         5
                         bart     0.8077    0.9545    0.8750        22
                         beit     0.8000    0.6667    0.7273         6
                         bert     0.8356    0.8841    0.8592        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.8750    0.9333         8
                   blenderbot     0.6667    0.4000    0.5000         5
                    camembert        nan    0.0000       nan        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     1.0000    1.0000    1.0000         4
                      codegen     0.8000    1.0000    0.8889         4
                     convnext     0.8750    1.0000    0.9333         7
                   convnextv2     0.5000    0.5000    0.5000         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.5000    0.6667    0.5714         3
                   distilbert     0.9714    0.9714    0.9714        35
                          dpr        nan    0.0000       nan         9
                      electra     0.4255    0.6452    0.5128        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet        nan    0.0000       nan         2
                         glpn        nan    0.0000       nan         1
                         gpt2     1.0000    1.0000    1.0000         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.5000    1.0000    0.6667         3
                     gpt_neox        nan    0.0000       nan         2
                         gptj     1.0000    0.8333    0.9091         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     0.8000    0.8000    0.8000         5
                         lilt        nan    0.0000       nan         4
                        llama        nan    0.0000       nan         3
                   longformer     0.9583    1.0000    0.9787        23
                       longt5     0.5000    0.6667    0.5714         3
                      m2m_100     0.7500    0.7500    0.7500         4
                       marian        nan    0.0000       nan         5
                  mask2former     1.0000    0.8750    0.9333         8
                   maskformer     0.7500    0.6000    0.6667         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5     0.7143    0.8333    0.7692         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.8571    0.8571    0.8571         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     0.7500    1.0000    0.8571         6
                       regnet     0.5000    1.0000    0.6667         3
                       resnet     0.3333    0.2500    0.2857         4
                      roberta     0.4737    0.6923    0.5625        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    0.5000    0.6667         2
                    segformer     0.7500    1.0000    0.8571         3
               speech_to_text     0.6000    0.7500    0.6667         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.7143    0.7143    0.7143        14
                    unispeech     1.0000    0.5000    0.6667         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    0.6667    0.8000         3
                     wav2vec2     0.7600    0.6786    0.7170        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     1.0000    1.0000    1.0000         8
                         xglm     1.0000    0.7500    0.8571         4
                  xlm-roberta     0.5000    0.4667    0.4828        30
                        xlnet     0.8333    0.9091    0.8696        11
                        yolos     1.0000    0.6667    0.8000         3

                     accuracy                         0.7665       681
                    macro avg     0.8201    0.6713    0.8141       681
                 weighted avg     0.7947    0.7665    0.8085       681

2024-08-26 20:31:59.531 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.7665
2024-08-26 20:31:59.534 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:31:59.534 | INFO     | __main__:eval:453 - Validation loss: 3.8981552340767602
2024-08-26 20:31:59.543 | INFO     | __main__:train:255 - epochs: 5
2024-08-26 20:31:59.544 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692]
2024-08-26 20:31:59.544 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602]
2024-08-26 20:31:59.544 | INFO     | __main__:train:157 - ====epoch #6====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:29,  4.99s/it]
Training:   5%|▍         | 2/43 [00:09<03:20,  4.90s/it]
Training:   7%|▋         | 3/43 [00:14<03:12,  4.82s/it]
Training:   9%|▉         | 4/43 [00:19<03:06,  4.79s/it]
Training:  12%|█▏        | 5/43 [00:23<03:00,  4.75s/it]
Training:  14%|█▍        | 6/43 [00:28<02:55,  4.75s/it]
Training:  16%|█▋        | 7/43 [00:33<02:50,  4.74s/it]
Training:  19%|█▊        | 8/43 [00:38<02:49,  4.84s/it]
Training:  21%|██        | 9/43 [00:43<02:49,  4.98s/it]
Training:  23%|██▎       | 10/43 [00:48<02:44,  4.99s/it]
Training:  26%|██▌       | 11/43 [00:53<02:39,  4.99s/it]
Training:  28%|██▊       | 12/43 [00:58<02:35,  5.00s/it]
Training:  30%|███       | 13/43 [01:03<02:30,  5.00s/it]
Training:  33%|███▎      | 14/43 [01:08<02:25,  5.02s/it]
Training:  35%|███▍      | 15/43 [01:13<02:20,  5.03s/it]
Training:  37%|███▋      | 16/43 [01:18<02:15,  5.02s/it]
Training:  40%|███▉      | 17/43 [01:23<02:10,  5.03s/it]
Training:  42%|████▏     | 18/43 [01:28<02:05,  5.01s/it]
Training:  44%|████▍     | 19/43 [01:33<01:58,  4.94s/it]
Training:  47%|████▋     | 20/43 [01:38<01:52,  4.88s/it]
Training:  49%|████▉     | 21/43 [01:43<01:48,  4.94s/it]
Training:  51%|█████     | 22/43 [01:48<01:44,  4.97s/it]
Training:  53%|█████▎    | 23/43 [01:53<01:39,  4.99s/it]
Training:  56%|█████▌    | 24/43 [01:58<01:34,  4.99s/it]
Training:  58%|█████▊    | 25/43 [02:03<01:30,  5.02s/it]
Training:  60%|██████    | 26/43 [02:08<01:24,  5.00s/it]
Training:  63%|██████▎   | 27/43 [02:13<01:20,  5.00s/it]
Training:  65%|██████▌   | 28/43 [02:18<01:15,  5.01s/it]
Training:  67%|██████▋   | 29/43 [02:23<01:10,  5.01s/it]
Training:  70%|██████▉   | 30/43 [02:28<01:04,  4.99s/it]
Training:  72%|███████▏  | 31/43 [02:33<01:00,  5.02s/it]
Training:  74%|███████▍  | 32/43 [02:38<00:54,  4.99s/it]
Training:  77%|███████▋  | 33/43 [02:43<00:49,  4.91s/it]
Training:  79%|███████▉  | 34/43 [02:48<00:43,  4.86s/it]
Training:  81%|████████▏ | 35/43 [02:52<00:38,  4.85s/it]
Training:  84%|████████▎ | 36/43 [02:57<00:34,  4.88s/it]
Training:  86%|████████▌ | 37/43 [03:03<00:29,  4.99s/it]
Training:  88%|████████▊ | 38/43 [03:08<00:24,  4.98s/it]
Training:  91%|█████████ | 39/43 [03:13<00:19,  4.98s/it]
Training:  93%|█████████▎| 40/43 [03:18<00:14,  4.98s/it]
Training:  95%|█████████▌| 41/43 [03:23<00:09,  4.97s/it]
Training:  98%|█████████▊| 42/43 [03:27<00:04,  4.97s/it]
Training: 100%|██████████| 43/43 [03:29<00:00,  3.92s/it]
Training: 100%|██████████| 43/43 [03:29<00:00,  4.87s/it]
2024-08-26 20:35:29.027 | INFO     | __main__:train:206 - Epoch 6 Average Loss: 3.34032
2024-08-26 20:35:30.640 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL6*****
2024-08-26 20:35:30.640 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:35:30.640 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.39s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.31s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.25s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.23s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.20s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.18s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.19s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.18s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.17s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.17s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.00it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.15s/it]
2024-08-26 20:35:43.303 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9706    1.0000    0.9851        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.8800    1.0000    0.9362        22
                         beit     0.8000    0.6667    0.7273         6
                         bert     0.8592    0.8841    0.8714        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.0000    0.0000       nan        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.6000    0.7500    0.6667         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.5000    1.0000    0.6667         3
                   distilbert     0.9714    0.9714    0.9714        35
                          dpr        nan    0.0000       nan         9
                      electra     0.4583    0.7097    0.5570        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet        nan    0.0000       nan         2
                         glpn        nan    0.0000       nan         1
                         gpt2     1.0000    1.0000    1.0000         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.6000    1.0000    0.7500         3
                     gpt_neox     0.5000    0.5000    0.5000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt        nan    0.0000       nan         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    0.6667    0.8000         3
                      m2m_100     1.0000    0.5000    0.6667         4
                       marian     0.3333    0.2000    0.2500         5
                  mask2former     1.0000    0.5000    0.6667         8
                   maskformer     1.0000    0.6000    0.7500         5
                        mbart     0.6667    1.0000    0.8000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     0.9444    1.0000    0.9714        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     0.7143    0.8333    0.7692         6
                          opt     0.5556    1.0000    0.7143         5
                      pegasus     0.6667    0.8571    0.7500         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    0.6667    0.8000         6
                       regnet     0.7500    1.0000    0.8571         3
                       resnet     1.0000    0.7500    0.8571         4
                      roberta     0.4615    0.8077    0.5874        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam        nan    0.0000       nan         2
                    segformer     0.6000    1.0000    0.7500         3
               speech_to_text     0.2500    0.5000    0.3333         4
                     speecht5        nan    0.0000       nan         2
                         swin     0.7500    1.0000    0.8571         6
                           t5     0.7000    0.5000    0.5833        14
                    unispeech        nan    0.0000       nan         4
                unispeech-sat     0.2000    0.5000    0.2857         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    0.6667    0.8000         3
                     wav2vec2     0.7500    0.6429    0.6923        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    0.7500    0.8571         4
                  xlm-roberta     0.8333    0.3333    0.4762        30
                        xlnet     0.5789    1.0000    0.7333        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.7651       681
                    macro avg     0.8047    0.6874    0.8007       681
                 weighted avg     0.7843    0.7651    0.8032       681

2024-08-26 20:35:43.304 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.7651
2024-08-26 20:35:43.309 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:35:43.310 | INFO     | __main__:eval:453 - Validation loss: 3.767307259819724
2024-08-26 20:35:43.321 | INFO     | __main__:train:255 - epochs: 6
2024-08-26 20:35:43.322 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925]
2024-08-26 20:35:43.322 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724]
2024-08-26 20:35:43.322 | INFO     | __main__:train:157 - ====epoch #7====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:28,  4.97s/it]
Training:   5%|▍         | 2/43 [00:09<03:24,  4.99s/it]
Training:   7%|▋         | 3/43 [00:14<03:16,  4.92s/it]
Training:   9%|▉         | 4/43 [00:19<03:09,  4.85s/it]
Training:  12%|█▏        | 5/43 [00:24<03:02,  4.81s/it]
Training:  14%|█▍        | 6/43 [00:29<02:57,  4.80s/it]
Training:  16%|█▋        | 7/43 [00:33<02:52,  4.81s/it]
Training:  19%|█▊        | 8/43 [00:38<02:48,  4.81s/it]
Training:  21%|██        | 9/43 [00:43<02:42,  4.79s/it]
Training:  23%|██▎       | 10/43 [00:48<02:37,  4.78s/it]
Training:  26%|██▌       | 11/43 [00:52<02:33,  4.78s/it]
Training:  28%|██▊       | 12/43 [00:57<02:27,  4.77s/it]
Training:  30%|███       | 13/43 [01:02<02:22,  4.76s/it]
Training:  33%|███▎      | 14/43 [01:07<02:18,  4.76s/it]
Training:  35%|███▍      | 15/43 [01:12<02:13,  4.77s/it]
Training:  37%|███▋      | 16/43 [01:16<02:08,  4.75s/it]
Training:  40%|███▉      | 17/43 [01:21<02:03,  4.74s/it]
Training:  42%|████▏     | 18/43 [01:26<01:58,  4.74s/it]
Training:  44%|████▍     | 19/43 [01:31<01:56,  4.83s/it]
Training:  47%|████▋     | 20/43 [01:35<01:50,  4.79s/it]
Training:  49%|████▉     | 21/43 [01:40<01:45,  4.78s/it]
Training:  51%|█████     | 22/43 [01:45<01:40,  4.77s/it]
Training:  53%|█████▎    | 23/43 [01:50<01:35,  4.77s/it]
Training:  56%|█████▌    | 24/43 [01:54<01:30,  4.77s/it]
Training:  58%|█████▊    | 25/43 [01:59<01:25,  4.77s/it]
Training:  60%|██████    | 26/43 [02:04<01:21,  4.77s/it]
Training:  63%|██████▎   | 27/43 [02:09<01:16,  4.76s/it]
Training:  65%|██████▌   | 28/43 [02:13<01:11,  4.76s/it]
Training:  67%|██████▋   | 29/43 [02:18<01:06,  4.74s/it]
Training:  70%|██████▉   | 30/43 [02:23<01:01,  4.75s/it]
Training:  72%|███████▏  | 31/43 [02:28<00:57,  4.76s/it]
Training:  74%|███████▍  | 32/43 [02:33<00:52,  4.76s/it]
Training:  77%|███████▋  | 33/43 [02:37<00:47,  4.76s/it]
Training:  79%|███████▉  | 34/43 [02:42<00:42,  4.77s/it]
Training:  81%|████████▏ | 35/43 [02:47<00:38,  4.76s/it]
Training:  84%|████████▎ | 36/43 [02:52<00:33,  4.76s/it]
Training:  86%|████████▌ | 37/43 [02:56<00:28,  4.77s/it]
Training:  88%|████████▊ | 38/43 [03:01<00:23,  4.76s/it]
Training:  91%|█████████ | 39/43 [03:06<00:19,  4.76s/it]
Training:  93%|█████████▎| 40/43 [03:11<00:14,  4.76s/it]
Training:  95%|█████████▌| 41/43 [03:15<00:09,  4.76s/it]
Training:  98%|█████████▊| 42/43 [03:20<00:04,  4.76s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  3.75s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  4.70s/it]
2024-08-26 20:39:05.416 | INFO     | __main__:train:206 - Epoch 7 Average Loss: 3.16262
2024-08-26 20:39:07.012 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL7*****
2024-08-26 20:39:07.012 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:39:07.013 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.32s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.29s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.27s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.27s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.26s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.25s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.23s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.21s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.19s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.19s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.00s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.17s/it]
2024-08-26 20:39:19.913 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.8919    1.0000    0.9429        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.8800    1.0000    0.9362        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.8312    0.9275    0.8767        69
                     big_bird     0.9545    0.9545    0.9545        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.2667    0.3200    0.2909        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     1.0000    1.0000    1.0000         4
                      codegen     1.0000    0.7500    0.8571         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.5000    0.6667    0.5714         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr        nan    0.0000       nan         9
                      electra     0.5294    0.8710    0.6585        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.0000    0.0000       nan         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     1.0000    1.0000    1.0000         1
                  gpt_bigcode     0.0000    0.0000       nan         1
                      gpt_neo     0.6000    1.0000    0.7500         3
                     gpt_neox        nan    0.0000       nan         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt        nan    0.0000       nan         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    0.6667    0.8000         3
                      m2m_100     1.0000    1.0000    1.0000         4
                       marian     1.0000    0.2000    0.3333         5
                  mask2former     1.0000    0.8750    0.9333         8
                   maskformer     0.8333    1.0000    0.9091         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     0.6250    1.0000    0.7692         5
                      pegasus     0.8571    0.8571    0.8571         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     0.8571    1.0000    0.9231         6
                       regnet     1.0000    0.6667    0.8000         3
                       resnet     0.8000    1.0000    0.8889         4
                      roberta     0.5849    0.5962    0.5905        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    0.5000    0.6667         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.5000    0.5000    0.5000         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.6316    0.8571    0.7273        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.7600    0.6786    0.7170        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    0.7500    0.8571         4
                  xlm-roberta     0.9167    0.3667    0.5238        30
                        xlnet     0.9167    1.0000    0.9565        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.7988       681
                    macro avg     0.8467    0.7362    0.8376       681
                 weighted avg     0.8272    0.7988    0.8131       681

2024-08-26 20:39:19.913 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.7988
2024-08-26 20:39:19.919 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:39:19.919 | INFO     | __main__:eval:453 - Validation loss: 3.715103344483809
2024-08-26 20:39:19.928 | INFO     | __main__:train:255 - epochs: 7
2024-08-26 20:39:19.929 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463]
2024-08-26 20:39:19.929 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809]
2024-08-26 20:39:19.929 | INFO     | __main__:train:157 - ====epoch #8====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:33,  5.08s/it]
Training:   5%|▍         | 2/43 [00:09<03:23,  4.95s/it]
Training:   7%|▋         | 3/43 [00:14<03:16,  4.91s/it]
Training:   9%|▉         | 4/43 [00:19<03:09,  4.87s/it]
Training:  12%|█▏        | 5/43 [00:24<03:03,  4.84s/it]
Training:  14%|█▍        | 6/43 [00:29<03:01,  4.92s/it]
Training:  16%|█▋        | 7/43 [00:34<02:55,  4.88s/it]
Training:  19%|█▊        | 8/43 [00:39<02:49,  4.86s/it]
Training:  21%|██        | 9/43 [00:43<02:44,  4.84s/it]
Training:  23%|██▎       | 10/43 [00:48<02:39,  4.83s/it]
Training:  26%|██▌       | 11/43 [00:53<02:34,  4.83s/it]
Training:  28%|██▊       | 12/43 [00:58<02:29,  4.83s/it]
Training:  30%|███       | 13/43 [01:03<02:24,  4.82s/it]
Training:  33%|███▎      | 14/43 [01:07<02:19,  4.81s/it]
Training:  35%|███▍      | 15/43 [01:12<02:14,  4.81s/it]
Training:  37%|███▋      | 16/43 [01:17<02:09,  4.81s/it]
Training:  40%|███▉      | 17/43 [01:22<02:04,  4.80s/it]
Training:  42%|████▏     | 18/43 [01:27<02:00,  4.81s/it]
Training:  44%|████▍     | 19/43 [01:31<01:55,  4.81s/it]
Training:  47%|████▋     | 20/43 [01:36<01:50,  4.80s/it]
Training:  49%|████▉     | 21/43 [01:41<01:45,  4.78s/it]
Training:  51%|█████     | 22/43 [01:46<01:39,  4.76s/it]
Training:  53%|█████▎    | 23/43 [01:50<01:35,  4.75s/it]
Training:  56%|█████▌    | 24/43 [01:55<01:30,  4.75s/it]
Training:  58%|█████▊    | 25/43 [02:00<01:25,  4.75s/it]
Training:  60%|██████    | 26/43 [02:05<01:20,  4.74s/it]
Training:  63%|██████▎   | 27/43 [02:09<01:15,  4.75s/it]
Training:  65%|██████▌   | 28/43 [02:14<01:11,  4.74s/it]
Training:  67%|██████▋   | 29/43 [02:19<01:06,  4.73s/it]
Training:  70%|██████▉   | 30/43 [02:24<01:01,  4.73s/it]
Training:  72%|███████▏  | 31/43 [02:28<00:56,  4.74s/it]
Training:  74%|███████▍  | 32/43 [02:33<00:52,  4.74s/it]
Training:  77%|███████▋  | 33/43 [02:38<00:47,  4.77s/it]
Training:  79%|███████▉  | 34/43 [02:43<00:42,  4.76s/it]
Training:  81%|████████▏ | 35/43 [02:47<00:37,  4.74s/it]
Training:  84%|████████▎ | 36/43 [02:52<00:33,  4.74s/it]
Training:  86%|████████▌ | 37/43 [02:57<00:28,  4.74s/it]
Training:  88%|████████▊ | 38/43 [03:02<00:23,  4.74s/it]
Training:  91%|█████████ | 39/43 [03:06<00:18,  4.73s/it]
Training:  93%|█████████▎| 40/43 [03:11<00:14,  4.73s/it]
Training:  95%|█████████▌| 41/43 [03:16<00:09,  4.73s/it]
Training:  98%|█████████▊| 42/43 [03:20<00:04,  4.73s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  3.73s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  4.71s/it]
2024-08-26 20:42:42.285 | INFO     | __main__:train:206 - Epoch 8 Average Loss: 3.08630
2024-08-26 20:42:43.877 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL8*****
2024-08-26 20:42:43.877 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:42:43.877 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.42s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.31s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.25s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.23s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.21s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.22s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.20s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.19s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.18s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.17s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.00it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.16s/it]
2024-08-26 20:42:56.609 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9444    0.7391    0.8293        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.7500    0.8571         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.2708    0.5200    0.3562        25
                       canine     0.6667    1.0000    0.8000         2
                         clip     1.0000    0.7500    0.8571         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     1.0000    1.0000    1.0000         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.4000    0.6667    0.5000         3
                   distilbert     0.9714    0.9714    0.9714        35
                          dpr     0.4091    1.0000    0.5806         9
                      electra     0.4857    0.5484    0.5152        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.6667    1.0000    0.8000         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.7500    1.0000    0.8571         3
                     gpt_neox     0.3333    0.5000    0.4000         2
                         gptj     1.0000    0.8333    0.9091         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt        nan    0.0000       nan         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    0.6667    0.8000         3
                      m2m_100     1.0000    1.0000    1.0000         4
                       marian     1.0000    0.8000    0.8889         5
                  mask2former     1.0000    0.8750    0.9333         8
                   maskformer     0.7143    1.0000    0.8333         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     0.8947    1.0000    0.9444        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5        nan    0.0000       nan         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     1.0000    0.8571    0.9231         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     0.8571    1.0000    0.9231         6
                       regnet     1.0000    0.3333    0.5000         3
                       resnet     0.6667    1.0000    0.8000         4
                      roberta     0.5263    0.5769    0.5505        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    0.5000    0.6667         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.5714    1.0000    0.7273         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    0.8333    0.9091         6
                           t5     0.6316    0.8571    0.7273        14
                    unispeech        nan    0.0000       nan         4
                unispeech-sat     0.2000    0.5000    0.2857         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6047    0.9286    0.7324        28
                        wavlm        nan    0.0000       nan        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    0.7500    0.8571         4
                  xlm-roberta     1.0000    0.2333    0.3784        30
                        xlnet     0.7333    1.0000    0.8462        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.7768       681
                    macro avg     0.8535    0.7537    0.8364       681
                 weighted avg     0.8356    0.7768    0.8032       681

2024-08-26 20:42:56.610 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.7768
2024-08-26 20:42:56.614 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:42:56.614 | INFO     | __main__:eval:453 - Validation loss: 3.6822332468899814
2024-08-26 20:42:56.626 | INFO     | __main__:train:255 - epochs: 8
2024-08-26 20:42:56.626 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313]
2024-08-26 20:42:56.626 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814]
2024-08-26 20:42:56.626 | INFO     | __main__:train:157 - ====epoch #9====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:30,  5.01s/it]
Training:   5%|▍         | 2/43 [00:09<03:24,  4.99s/it]
Training:   7%|▋         | 3/43 [00:15<03:20,  5.01s/it]
Training:   9%|▉         | 4/43 [00:19<03:14,  4.98s/it]
Training:  12%|█▏        | 5/43 [00:24<03:08,  4.96s/it]
Training:  14%|█▍        | 6/43 [00:29<03:03,  4.95s/it]
Training:  16%|█▋        | 7/43 [00:34<02:58,  4.96s/it]
Training:  19%|█▊        | 8/43 [00:39<02:53,  4.96s/it]
Training:  21%|██        | 9/43 [00:44<02:48,  4.96s/it]
Training:  23%|██▎       | 10/43 [00:49<02:43,  4.96s/it]
Training:  26%|██▌       | 11/43 [00:54<02:38,  4.96s/it]
Training:  28%|██▊       | 12/43 [00:59<02:32,  4.94s/it]
Training:  30%|███       | 13/43 [01:04<02:28,  4.94s/it]
Training:  33%|███▎      | 14/43 [01:09<02:22,  4.93s/it]
Training:  35%|███▍      | 15/43 [01:14<02:17,  4.93s/it]
Training:  37%|███▋      | 16/43 [01:19<02:13,  4.93s/it]
Training:  40%|███▉      | 17/43 [01:24<02:08,  4.94s/it]
Training:  42%|████▏     | 18/43 [01:29<02:04,  4.97s/it]
Training:  44%|████▍     | 19/43 [01:34<01:59,  4.98s/it]
Training:  47%|████▋     | 20/43 [01:39<01:55,  5.04s/it]
Training:  49%|████▉     | 21/43 [01:44<01:50,  5.02s/it]
Training:  51%|█████     | 22/43 [01:49<01:44,  4.99s/it]
Training:  53%|█████▎    | 23/43 [01:54<01:40,  5.01s/it]
Training:  56%|█████▌    | 24/43 [01:59<01:35,  5.02s/it]
Training:  58%|█████▊    | 25/43 [02:04<01:30,  5.01s/it]
Training:  60%|██████    | 26/43 [02:09<01:24,  4.98s/it]
Training:  63%|██████▎   | 27/43 [02:14<01:18,  4.92s/it]
Training:  65%|██████▌   | 28/43 [02:18<01:13,  4.88s/it]
Training:  67%|██████▋   | 29/43 [02:23<01:08,  4.92s/it]
Training:  70%|██████▉   | 30/43 [02:28<01:03,  4.92s/it]
Training:  72%|███████▏  | 31/43 [02:33<00:58,  4.90s/it]
Training:  74%|███████▍  | 32/43 [02:38<00:53,  4.86s/it]
Training:  77%|███████▋  | 33/43 [02:43<00:48,  4.82s/it]
Training:  79%|███████▉  | 34/43 [02:47<00:43,  4.80s/it]
Training:  81%|████████▏ | 35/43 [02:52<00:38,  4.77s/it]
Training:  84%|████████▎ | 36/43 [02:57<00:33,  4.77s/it]
Training:  86%|████████▌ | 37/43 [03:02<00:29,  4.85s/it]
Training:  88%|████████▊ | 38/43 [03:07<00:24,  4.89s/it]
Training:  91%|█████████ | 39/43 [03:12<00:19,  4.91s/it]
Training:  93%|█████████▎| 40/43 [03:17<00:14,  4.91s/it]
Training:  95%|█████████▌| 41/43 [03:21<00:09,  4.86s/it]
Training:  98%|█████████▊| 42/43 [03:26<00:04,  4.84s/it]
Training: 100%|██████████| 43/43 [03:28<00:00,  3.85s/it]
Training: 100%|██████████| 43/43 [03:28<00:00,  4.85s/it]
2024-08-26 20:46:25.005 | INFO     | __main__:train:206 - Epoch 9 Average Loss: 2.98362
2024-08-26 20:46:26.621 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL9*****
2024-08-26 20:46:26.621 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:46:26.622 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.42s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.31s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.27s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.26s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.25s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.25s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.24s/it]
Evaluating:  73%|███████▎  | 8/11 [00:10<00:03,  1.24s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.24s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.24s/it]
Evaluating: 100%|██████████| 11/11 [00:13<00:00,  1.04s/it]
Evaluating: 100%|██████████| 11/11 [00:13<00:00,  1.20s/it]
2024-08-26 20:46:39.806 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.9167    1.0000    0.9565        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.7204    0.9710    0.8272        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.8000    0.8000    0.8000         5
                    camembert     0.3333    0.0800    0.1290        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     1.0000    1.0000    1.0000         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     1.0000    1.0000    1.0000         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.5000    0.6667    0.5714         3
                   distilbert     0.9714    0.9714    0.9714        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.5667    0.5484    0.5574        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.6667    0.6667    0.6667         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.8333    0.9091         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt        nan    0.0000       nan         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     1.0000    0.5000    0.6667         4
                       marian     1.0000    0.4000    0.5714         5
                  mask2former     1.0000    0.8750    0.9333         8
                   maskformer     0.8333    1.0000    0.9091         5
                        mbart     1.0000    0.5000    0.6667         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     0.9444    1.0000    0.9714        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5        nan    0.0000       nan         6
                          opt     0.8333    1.0000    0.9091         5
                      pegasus     0.5833    1.0000    0.7368         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     0.8571    1.0000    0.9231         6
                       regnet     0.7500    1.0000    0.8571         3
                       resnet     1.0000    0.7500    0.8571         4
                      roberta     0.5507    0.7308    0.6281        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    0.5000    0.6667         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.8000    1.0000    0.8889         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.6316    0.8571    0.7273        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6190    0.9286    0.7429        28
                        wavlm     1.0000    0.0833    0.1538        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    0.7500    0.8571         4
                  xlm-roberta     0.8235    0.4667    0.5957        30
                        xlnet     0.9167    1.0000    0.9565        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8091       681
                    macro avg     0.8710    0.7629    0.8318       681
                 weighted avg     0.8249    0.8091    0.8036       681

2024-08-26 20:46:39.806 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8091
2024-08-26 20:46:39.817 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:46:39.817 | INFO     | __main__:eval:453 - Validation loss: 3.558532303029841
2024-08-26 20:46:39.826 | INFO     | __main__:train:255 - epochs: 9
2024-08-26 20:46:39.827 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526]
2024-08-26 20:46:39.827 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841]
2024-08-26 20:46:39.827 | INFO     | __main__:train:157 - ====epoch #10====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:32,  5.06s/it]
Training:   5%|▍         | 2/43 [00:10<03:24,  4.99s/it]
Training:   7%|▋         | 3/43 [00:15<03:20,  5.02s/it]
Training:   9%|▉         | 4/43 [00:20<03:17,  5.05s/it]
Training:  12%|█▏        | 5/43 [00:24<03:07,  4.95s/it]
Training:  14%|█▍        | 6/43 [00:29<03:00,  4.87s/it]
Training:  16%|█▋        | 7/43 [00:34<02:53,  4.82s/it]
Training:  19%|█▊        | 8/43 [00:39<02:47,  4.79s/it]
Training:  21%|██        | 9/43 [00:43<02:42,  4.77s/it]
Training:  23%|██▎       | 10/43 [00:48<02:37,  4.76s/it]
Training:  26%|██▌       | 11/43 [00:53<02:31,  4.75s/it]
Training:  28%|██▊       | 12/43 [00:57<02:26,  4.73s/it]
Training:  30%|███       | 13/43 [01:02<02:22,  4.74s/it]
Training:  33%|███▎      | 14/43 [01:07<02:16,  4.72s/it]
Training:  35%|███▍      | 15/43 [01:12<02:12,  4.73s/it]
Training:  37%|███▋      | 16/43 [01:17<02:10,  4.83s/it]
Training:  40%|███▉      | 17/43 [01:22<02:06,  4.86s/it]
Training:  42%|████▏     | 18/43 [01:27<02:02,  4.91s/it]
Training:  44%|████▍     | 19/43 [01:32<01:58,  4.94s/it]
Training:  47%|████▋     | 20/43 [01:37<01:54,  4.97s/it]
Training:  49%|████▉     | 21/43 [01:42<01:49,  4.98s/it]
Training:  51%|█████     | 22/43 [01:47<01:43,  4.95s/it]
Training:  53%|█████▎    | 23/43 [01:51<01:37,  4.88s/it]
Training:  56%|█████▌    | 24/43 [01:56<01:32,  4.85s/it]
Training:  58%|█████▊    | 25/43 [02:01<01:26,  4.81s/it]
Training:  60%|██████    | 26/43 [02:06<01:21,  4.78s/it]
Training:  63%|██████▎   | 27/43 [02:10<01:16,  4.77s/it]
Training:  65%|██████▌   | 28/43 [02:15<01:11,  4.74s/it]
Training:  67%|██████▋   | 29/43 [02:20<01:06,  4.74s/it]
Training:  70%|██████▉   | 30/43 [02:24<01:01,  4.75s/it]
Training:  72%|███████▏  | 31/43 [02:29<00:57,  4.75s/it]
Training:  74%|███████▍  | 32/43 [02:34<00:52,  4.74s/it]
Training:  77%|███████▋  | 33/43 [02:39<00:47,  4.78s/it]
Training:  79%|███████▉  | 34/43 [02:43<00:42,  4.75s/it]
Training:  81%|████████▏ | 35/43 [02:48<00:38,  4.75s/it]
Training:  84%|████████▎ | 36/43 [02:53<00:33,  4.74s/it]
Training:  86%|████████▌ | 37/43 [02:58<00:28,  4.73s/it]
Training:  88%|████████▊ | 38/43 [03:02<00:23,  4.74s/it]
Training:  91%|█████████ | 39/43 [03:07<00:18,  4.73s/it]
Training:  93%|█████████▎| 40/43 [03:12<00:14,  4.73s/it]
Training:  95%|█████████▌| 41/43 [03:17<00:09,  4.72s/it]
Training:  98%|█████████▊| 42/43 [03:21<00:04,  4.73s/it]
Training: 100%|██████████| 43/43 [03:23<00:00,  3.73s/it]
Training: 100%|██████████| 43/43 [03:23<00:00,  4.73s/it]
2024-08-26 20:50:03.097 | INFO     | __main__:train:206 - Epoch 10 Average Loss: 2.92712
2024-08-26 20:50:04.695 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL10*****
2024-08-26 20:50:04.696 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:50:04.696 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.40s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.31s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.27s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.26s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.25s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.22s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.21s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.19s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.18s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.20s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.02s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.17s/it]
2024-08-26 20:50:17.584 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9275    0.9275    0.9275        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.6250    0.2000    0.3030        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     0.6667    1.0000    0.8000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     1.0000    1.0000    1.0000         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     1.0000    0.6667    0.8000         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.6774    0.6774    0.6774        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     1.0000    1.0000    1.0000         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.6667    0.6667    0.6667         3
                     gpt_neox     0.5000    0.5000    0.5000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt        nan    0.0000       nan         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     1.0000    0.7500    0.8571         4
                       marian     0.8333    1.0000    0.9091         5
                  mask2former     1.0000    0.6250    0.7692         8
                   maskformer     0.8333    1.0000    0.9091         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     0.8947    1.0000    0.9444        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7778    1.0000    0.8750         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    0.8333    0.9091         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.4574    0.8269    0.5890        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    0.5000    0.6667         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.4000    0.5000    0.4444         4
                     speecht5        nan    0.0000       nan         2
                         swin     0.7500    1.0000    0.8571         6
                           t5     0.7059    0.8571    0.7742        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6000    0.9643    0.7397        28
                        wavlm        nan    0.0000       nan        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    0.7500    0.8571         4
                  xlm-roberta     0.9167    0.3667    0.5238        30
                        xlnet     0.8462    1.0000    0.9167        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8223       681
                    macro avg     0.8840    0.7741    0.8532       681
                 weighted avg     0.8594    0.8223    0.8335       681

2024-08-26 20:50:17.585 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8223
2024-08-26 20:50:17.587 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:50:17.588 | INFO     | __main__:eval:453 - Validation loss: 3.5820599902759898
2024-08-26 20:50:17.597 | INFO     | __main__:train:255 - epochs: 10
2024-08-26 20:50:17.598 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412]
2024-08-26 20:50:17.598 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898]
2024-08-26 20:50:17.598 | INFO     | __main__:train:157 - ====epoch #11====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:31,  5.05s/it]
Training:   5%|▍         | 2/43 [00:09<03:24,  4.98s/it]
Training:   7%|▋         | 3/43 [00:14<03:17,  4.93s/it]
Training:   9%|▉         | 4/43 [00:19<03:12,  4.94s/it]
Training:  12%|█▏        | 5/43 [00:24<03:07,  4.93s/it]
Training:  14%|█▍        | 6/43 [00:29<03:01,  4.91s/it]
Training:  16%|█▋        | 7/43 [00:34<02:56,  4.91s/it]
Training:  19%|█▊        | 8/43 [00:39<02:51,  4.90s/it]
Training:  21%|██        | 9/43 [00:44<02:46,  4.89s/it]
Training:  23%|██▎       | 10/43 [00:49<02:41,  4.89s/it]
Training:  26%|██▌       | 11/43 [00:54<02:36,  4.89s/it]
Training:  28%|██▊       | 12/43 [00:58<02:30,  4.86s/it]
Training:  30%|███       | 13/43 [01:03<02:26,  4.88s/it]
Training:  33%|███▎      | 14/43 [01:08<02:20,  4.83s/it]
Training:  35%|███▍      | 15/43 [01:13<02:14,  4.81s/it]
Training:  37%|███▋      | 16/43 [01:18<02:10,  4.85s/it]
Training:  40%|███▉      | 17/43 [01:22<02:05,  4.83s/it]
Training:  42%|████▏     | 18/43 [01:27<02:00,  4.80s/it]
Training:  44%|████▍     | 19/43 [01:32<01:54,  4.78s/it]
Training:  47%|████▋     | 20/43 [01:37<01:49,  4.77s/it]
Training:  49%|████▉     | 21/43 [01:41<01:44,  4.77s/it]
Training:  51%|█████     | 22/43 [01:46<01:39,  4.76s/it]
Training:  53%|█████▎    | 23/43 [01:51<01:36,  4.85s/it]
Training:  56%|█████▌    | 24/43 [01:56<01:33,  4.91s/it]
Training:  58%|█████▊    | 25/43 [02:01<01:28,  4.93s/it]
Training:  60%|██████    | 26/43 [02:06<01:23,  4.91s/it]
Training:  63%|██████▎   | 27/43 [02:11<01:18,  4.88s/it]
Training:  65%|██████▌   | 28/43 [02:16<01:12,  4.84s/it]
Training:  67%|██████▋   | 29/43 [02:20<01:07,  4.80s/it]
Training:  70%|██████▉   | 30/43 [02:25<01:02,  4.78s/it]
Training:  72%|███████▏  | 31/43 [02:30<00:57,  4.78s/it]
Training:  74%|███████▍  | 32/43 [02:35<00:52,  4.77s/it]
Training:  77%|███████▋  | 33/43 [02:39<00:47,  4.76s/it]
Training:  79%|███████▉  | 34/43 [02:44<00:42,  4.75s/it]
Training:  81%|████████▏ | 35/43 [02:49<00:37,  4.74s/it]
Training:  84%|████████▎ | 36/43 [02:54<00:33,  4.76s/it]
Training:  86%|████████▌ | 37/43 [02:58<00:28,  4.76s/it]
Training:  88%|████████▊ | 38/43 [03:03<00:23,  4.75s/it]
Training:  91%|█████████ | 39/43 [03:08<00:18,  4.74s/it]
Training:  93%|█████████▎| 40/43 [03:13<00:14,  4.74s/it]
Training:  95%|█████████▌| 41/43 [03:17<00:09,  4.75s/it]
Training:  98%|█████████▊| 42/43 [03:22<00:04,  4.79s/it]
Training: 100%|██████████| 43/43 [03:24<00:00,  3.77s/it]
Training: 100%|██████████| 43/43 [03:24<00:00,  4.75s/it]
2024-08-26 20:53:41.708 | INFO     | __main__:train:206 - Epoch 11 Average Loss: 3.00712
2024-08-26 20:53:43.306 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL11*****
2024-08-26 20:53:43.306 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:53:43.306 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.38s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.29s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.27s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.23s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.20s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.18s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.20s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.19s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.17s/it]
Evaluating:  91%|█████████ | 10/11 [00:11<00:01,  1.17s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.02it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.15s/it]
2024-08-26 20:53:55.916 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9167    1.0000    0.9565        33
audio-spectrogram-transformer     0.7143    1.0000    0.8333         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9275    0.9275    0.9275        69
                     big_bird        nan    0.0000       nan        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.8000    0.8000    0.8000         5
                    camembert     0.2727    0.3600    0.3103        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     1.0000    1.0000    1.0000         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.5000    1.0000    0.6667         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.4000    0.6667    0.5000         3
                   distilbert     1.0000    0.9429    0.9706        35
                          dpr     1.0000    0.8889    0.9412         9
                      electra     0.5800    0.9355    0.7160        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.6667    1.0000    0.8000         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.7500    1.0000    0.8571         3
                     gpt_neox     1.0000    1.0000    1.0000         2
                         gptj     1.0000    0.8333    0.9091         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt        nan    0.0000       nan         4
                        llama     0.5000    0.3333    0.4000         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     1.0000    1.0000    1.0000         4
                       marian     0.7143    1.0000    0.8333         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7500    0.8571    0.8000         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    0.6667    0.8000         6
                       regnet     1.0000    0.6667    0.8000         3
                       resnet     0.8000    1.0000    0.8889         4
                      roberta     0.5147    0.6731    0.5833        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.7500    0.7500    0.7500         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.9167    0.7857    0.8462        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2000    0.5000    0.2857         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6047    0.9286    0.7324        28
                        wavlm        nan    0.0000       nan        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     0.7500    0.7500    0.7500         4
                  xlm-roberta     0.8235    0.4667    0.5957        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8062       681
                    macro avg     0.8765    0.7893    0.8674       681
                 weighted avg     0.8435    0.8062    0.8408       681

2024-08-26 20:53:55.916 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8062
2024-08-26 20:53:55.919 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:53:55.919 | INFO     | __main__:eval:453 - Validation loss: 3.5776077183810147
2024-08-26 20:53:55.926 | INFO     | __main__:train:255 - epochs: 11
2024-08-26 20:53:55.926 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965]
2024-08-26 20:53:55.926 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147]
2024-08-26 20:53:55.926 | INFO     | __main__:train:157 - ====epoch #12====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:31,  5.03s/it]
Training:   5%|▍         | 2/43 [00:10<03:25,  5.01s/it]
Training:   7%|▋         | 3/43 [00:14<03:18,  4.96s/it]
Training:   9%|▉         | 4/43 [00:19<03:10,  4.89s/it]
Training:  12%|█▏        | 5/43 [00:24<03:04,  4.85s/it]
Training:  14%|█▍        | 6/43 [00:29<02:58,  4.82s/it]
Training:  16%|█▋        | 7/43 [00:33<02:52,  4.79s/it]
Training:  19%|█▊        | 8/43 [00:38<02:47,  4.77s/it]
Training:  21%|██        | 9/43 [00:43<02:41,  4.76s/it]
Training:  23%|██▎       | 10/43 [00:48<02:36,  4.76s/it]
Training:  26%|██▌       | 11/43 [00:52<02:32,  4.76s/it]
Training:  28%|██▊       | 12/43 [00:57<02:27,  4.75s/it]
Training:  30%|███       | 13/43 [01:02<02:22,  4.74s/it]
Training:  33%|███▎      | 14/43 [01:07<02:17,  4.75s/it]
Training:  35%|███▍      | 15/43 [01:11<02:12,  4.75s/it]
Training:  37%|███▋      | 16/43 [01:16<02:08,  4.75s/it]
Training:  40%|███▉      | 17/43 [01:21<02:03,  4.75s/it]
Training:  42%|████▏     | 18/43 [01:26<01:57,  4.71s/it]
Training:  44%|████▍     | 19/43 [01:30<01:52,  4.71s/it]
Training:  47%|████▋     | 20/43 [01:35<01:48,  4.70s/it]
Training:  49%|████▉     | 21/43 [01:40<01:43,  4.68s/it]
Training:  51%|█████     | 22/43 [01:44<01:38,  4.69s/it]
Training:  53%|█████▎    | 23/43 [01:49<01:33,  4.69s/it]
Training:  56%|█████▌    | 24/43 [01:54<01:29,  4.69s/it]
Training:  58%|█████▊    | 25/43 [01:58<01:24,  4.71s/it]
Training:  60%|██████    | 26/43 [02:03<01:19,  4.70s/it]
Training:  63%|██████▎   | 27/43 [02:08<01:15,  4.72s/it]
Training:  65%|██████▌   | 28/43 [02:13<01:10,  4.71s/it]
Training:  67%|██████▋   | 29/43 [02:18<01:07,  4.83s/it]
Training:  70%|██████▉   | 30/43 [02:22<01:02,  4.82s/it]
Training:  72%|███████▏  | 31/43 [02:27<00:57,  4.80s/it]
Training:  74%|███████▍  | 32/43 [02:32<00:52,  4.80s/it]
Training:  77%|███████▋  | 33/43 [02:37<00:47,  4.80s/it]
Training:  79%|███████▉  | 34/43 [02:41<00:42,  4.77s/it]
Training:  81%|████████▏ | 35/43 [02:46<00:38,  4.76s/it]
Training:  84%|████████▎ | 36/43 [02:51<00:33,  4.76s/it]
Training:  86%|████████▌ | 37/43 [02:56<00:28,  4.76s/it]
Training:  88%|████████▊ | 38/43 [03:00<00:23,  4.75s/it]
Training:  91%|█████████ | 39/43 [03:05<00:19,  4.76s/it]
Training:  93%|█████████▎| 40/43 [03:10<00:14,  4.75s/it]
Training:  95%|█████████▌| 41/43 [03:15<00:09,  4.73s/it]
Training:  98%|█████████▊| 42/43 [03:19<00:04,  4.73s/it]
Training: 100%|██████████| 43/43 [03:21<00:00,  3.73s/it]
Training: 100%|██████████| 43/43 [03:21<00:00,  4.68s/it]
2024-08-26 20:57:17.227 | INFO     | __main__:train:206 - Epoch 12 Average Loss: 2.92536
2024-08-26 20:57:18.823 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL12*****
2024-08-26 20:57:18.823 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 20:57:18.823 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.31s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:10,  1.22s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.18s/it]
Evaluating:  36%|███▋      | 4/11 [00:04<00:08,  1.21s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.22s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.22s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.23s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.21s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.19s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.18s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.01it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.15s/it]
2024-08-26 20:57:31.461 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9706    1.0000    0.9851        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.9565    1.0000    0.9778        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9552    0.9275    0.9412        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.6667    0.8000    0.7273         5
                    camembert     0.5000    0.2400    0.3243        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.5000    0.6667    0.5714         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    0.8889    0.9412         9
                      electra     0.5600    0.4516    0.5000        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    1.0000    1.0000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt        nan    0.0000       nan         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     0.8000    1.0000    0.8889         4
                       marian     1.0000    0.8000    0.8889         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     0.8333    1.0000    0.9091         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5     1.0000    0.6667    0.8000         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7500    0.8571    0.8000         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    1.0000    1.0000         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.4632    0.8462    0.5986        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.8000    1.0000    0.8889         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    0.8333    0.9091         6
                           t5     0.7857    0.7857    0.7857        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6136    0.9643    0.7500        28
                        wavlm        nan    0.0000       nan        12
                      whisper     1.0000    1.0000    1.0000         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.9333    0.4667    0.6222        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8311       681
                    macro avg     0.8905    0.8317    0.8776       681
                 weighted avg     0.8635    0.8311    0.8439       681

2024-08-26 20:57:31.462 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8311
2024-08-26 20:57:31.466 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 20:57:31.466 | INFO     | __main__:eval:453 - Validation loss: 3.516594409942627
2024-08-26 20:57:31.473 | INFO     | __main__:train:255 - epochs: 12
2024-08-26 20:57:31.474 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393]
2024-08-26 20:57:31.474 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627]
2024-08-26 20:57:31.474 | INFO     | __main__:train:157 - ====epoch #13====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:32,  5.06s/it]
Training:   5%|▍         | 2/43 [00:09<03:23,  4.97s/it]
Training:   7%|▋         | 3/43 [00:14<03:19,  4.98s/it]
Training:   9%|▉         | 4/43 [00:19<03:13,  4.95s/it]
Training:  12%|█▏        | 5/43 [00:24<03:08,  4.95s/it]
Training:  14%|█▍        | 6/43 [00:29<03:03,  4.95s/it]
Training:  16%|█▋        | 7/43 [00:34<02:58,  4.95s/it]
Training:  19%|█▊        | 8/43 [00:39<02:53,  4.96s/it]
Training:  21%|██        | 9/43 [00:44<02:48,  4.96s/it]
Training:  23%|██▎       | 10/43 [00:49<02:43,  4.96s/it]
Training:  26%|██▌       | 11/43 [00:54<02:38,  4.96s/it]
Training:  28%|██▊       | 12/43 [00:59<02:33,  4.95s/it]
Training:  30%|███       | 13/43 [01:04<02:28,  4.95s/it]
Training:  33%|███▎      | 14/43 [01:09<02:25,  5.03s/it]
Training:  35%|███▍      | 15/43 [01:14<02:20,  5.01s/it]
Training:  37%|███▋      | 16/43 [01:19<02:14,  4.99s/it]
Training:  40%|███▉      | 17/43 [01:24<02:09,  4.98s/it]
Training:  42%|████▏     | 18/43 [01:29<02:04,  4.97s/it]
Training:  44%|████▍     | 19/43 [01:34<01:59,  4.98s/it]
Training:  47%|████▋     | 20/43 [01:39<01:54,  4.99s/it]
Training:  49%|████▉     | 21/43 [01:44<01:49,  4.97s/it]
Training:  51%|█████     | 22/43 [01:49<01:44,  4.95s/it]
Training:  53%|█████▎    | 23/43 [01:54<01:39,  4.96s/it]
Training:  56%|█████▌    | 24/43 [01:59<01:33,  4.92s/it]
Training:  58%|█████▊    | 25/43 [02:03<01:27,  4.88s/it]
Training:  60%|██████    | 26/43 [02:08<01:22,  4.87s/it]
Training:  63%|██████▎   | 27/43 [02:13<01:17,  4.86s/it]
Training:  65%|██████▌   | 28/43 [02:18<01:13,  4.91s/it]
Training:  67%|██████▋   | 29/43 [02:23<01:08,  4.92s/it]
Training:  70%|██████▉   | 30/43 [02:28<01:04,  4.95s/it]
Training:  72%|███████▏  | 31/43 [02:33<00:59,  4.95s/it]
Training:  74%|███████▍  | 32/43 [02:38<00:54,  4.96s/it]
Training:  77%|███████▋  | 33/43 [02:43<00:49,  4.96s/it]
Training:  79%|███████▉  | 34/43 [02:48<00:44,  4.98s/it]
Training:  81%|████████▏ | 35/43 [02:53<00:39,  4.97s/it]
Training:  84%|████████▎ | 36/43 [02:58<00:34,  4.97s/it]
Training:  86%|████████▌ | 37/43 [03:03<00:29,  4.97s/it]
Training:  88%|████████▊ | 38/43 [03:08<00:24,  4.96s/it]
Training:  91%|█████████ | 39/43 [03:13<00:19,  4.95s/it]
Training:  93%|█████████▎| 40/43 [03:18<00:14,  4.96s/it]
Training:  95%|█████████▌| 41/43 [03:23<00:09,  4.97s/it]
Training:  98%|█████████▊| 42/43 [03:28<00:05,  5.03s/it]
Training: 100%|██████████| 43/43 [03:29<00:00,  3.97s/it]
Training: 100%|██████████| 43/43 [03:29<00:00,  4.88s/it]
2024-08-26 21:01:01.393 | INFO     | __main__:train:206 - Epoch 13 Average Loss: 2.91083
2024-08-26 21:01:02.985 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL13*****
2024-08-26 21:01:02.985 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:01:02.985 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.42s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.32s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.28s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.25s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.22s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.20s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.19s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.18s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.17s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.17s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.02it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.15s/it]
2024-08-26 21:01:15.671 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9706    1.0000    0.9851        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9275    0.9275    0.9275        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.5333    0.3200    0.4000        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     1.0000    1.0000    1.0000         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     0.8750    1.0000    0.9333         7
                   convnextv2     0.5000    0.5000    0.5000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     0.9375    1.0000    0.9677        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.4000    0.6667    0.5000         3
                   distilbert     1.0000    0.9429    0.9706        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.5682    0.8065    0.6667        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.6667    1.0000    0.8000         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    1.0000    1.0000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.8333    0.9091         6
                     layoutlm     0.5000    1.0000    0.6667         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt        nan    0.0000       nan         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     0.7500    1.0000    0.8571         3
                      m2m_100     1.0000    0.5000    0.6667         4
                       marian     0.7143    1.0000    0.8333         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     0.6667    1.0000    0.8000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5     1.0000    0.5000    0.6667         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.6667    0.8571    0.7500         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     1.0000    0.6667    0.8000         6
                       regnet     1.0000    0.6667    0.8000         3
                       resnet     0.7500    0.7500    0.7500         4
                      roberta     0.5429    0.7308    0.6230        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    0.5000    0.6667         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6667    1.0000    0.8000         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.7500    0.8571    0.8000        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.7692    0.7143    0.7407        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     1.0000    1.0000    1.0000         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.9333    0.4667    0.6222        30
                        xlnet     0.8462    1.0000    0.9167        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8355       681
                    macro avg     0.8634    0.8013    0.8433       681
                 weighted avg     0.8603    0.8355    0.8404       681

2024-08-26 21:01:15.671 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8355
2024-08-26 21:01:15.674 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:01:15.674 | INFO     | __main__:eval:453 - Validation loss: 3.4841770258816807
2024-08-26 21:01:15.685 | INFO     | __main__:train:255 - epochs: 13
2024-08-26 21:01:15.685 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157]
2024-08-26 21:01:15.686 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807]
2024-08-26 21:01:15.686 | INFO     | __main__:train:157 - ====epoch #14====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:23,  4.85s/it]
Training:   5%|▍         | 2/43 [00:09<03:16,  4.79s/it]
Training:   7%|▋         | 3/43 [00:14<03:10,  4.76s/it]
Training:   9%|▉         | 4/43 [00:19<03:09,  4.85s/it]
Training:  12%|█▏        | 5/43 [00:24<03:04,  4.86s/it]
Training:  14%|█▍        | 6/43 [00:29<03:01,  4.92s/it]
Training:  16%|█▋        | 7/43 [00:34<02:56,  4.90s/it]
Training:  19%|█▊        | 8/43 [00:38<02:49,  4.85s/it]
Training:  21%|██        | 9/43 [00:43<02:43,  4.82s/it]
Training:  23%|██▎       | 10/43 [00:48<02:37,  4.78s/it]
Training:  26%|██▌       | 11/43 [00:53<02:32,  4.77s/it]
Training:  28%|██▊       | 12/43 [00:57<02:29,  4.82s/it]
Training:  30%|███       | 13/43 [01:02<02:25,  4.84s/it]
Training:  33%|███▎      | 14/43 [01:07<02:20,  4.83s/it]
Training:  35%|███▍      | 15/43 [01:12<02:15,  4.85s/it]
Training:  37%|███▋      | 16/43 [01:17<02:11,  4.89s/it]
Training:  40%|███▉      | 17/43 [01:22<02:07,  4.89s/it]
Training:  42%|████▏     | 18/43 [01:27<02:01,  4.88s/it]
Training:  44%|████▍     | 19/43 [01:32<01:57,  4.90s/it]
Training:  47%|████▋     | 20/43 [01:37<01:52,  4.91s/it]
Training:  49%|████▉     | 21/43 [01:42<01:48,  4.92s/it]
Training:  51%|█████     | 22/43 [01:47<01:43,  4.92s/it]
Training:  53%|█████▎    | 23/43 [01:51<01:38,  4.91s/it]
Training:  56%|█████▌    | 24/43 [01:56<01:32,  4.89s/it]
Training:  58%|█████▊    | 25/43 [02:01<01:28,  4.91s/it]
Training:  60%|██████    | 26/43 [02:06<01:24,  4.99s/it]
Training:  63%|██████▎   | 27/43 [02:11<01:20,  5.00s/it]
Training:  65%|██████▌   | 28/43 [02:16<01:14,  4.98s/it]
Training:  67%|██████▋   | 29/43 [02:21<01:09,  4.96s/it]
Training:  70%|██████▉   | 30/43 [02:26<01:04,  4.95s/it]
Training:  72%|███████▏  | 31/43 [02:31<00:59,  4.95s/it]
Training:  74%|███████▍  | 32/43 [02:36<00:54,  4.95s/it]
Training:  77%|███████▋  | 33/43 [02:41<00:49,  4.94s/it]
Training:  79%|███████▉  | 34/43 [02:46<00:44,  4.93s/it]
Training:  81%|████████▏ | 35/43 [02:51<00:39,  4.92s/it]
Training:  84%|████████▎ | 36/43 [02:56<00:34,  4.93s/it]
Training:  86%|████████▌ | 37/43 [03:01<00:29,  4.94s/it]
Training:  88%|████████▊ | 38/43 [03:06<00:24,  4.95s/it]
Training:  91%|█████████ | 39/43 [03:11<00:19,  4.96s/it]
Training:  93%|█████████▎| 40/43 [03:16<00:14,  4.96s/it]
Training:  95%|█████████▌| 41/43 [03:21<00:09,  4.96s/it]
Training:  98%|█████████▊| 42/43 [03:26<00:04,  4.97s/it]
Training: 100%|██████████| 43/43 [03:27<00:00,  3.93s/it]
Training: 100%|██████████| 43/43 [03:27<00:00,  4.83s/it]
2024-08-26 21:04:43.336 | INFO     | __main__:train:206 - Epoch 14 Average Loss: 2.81822
2024-08-26 21:04:44.966 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL14*****
2024-08-26 21:04:44.966 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:04:44.966 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.43s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.31s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.28s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.26s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.25s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.24s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.24s/it]
Evaluating:  73%|███████▎  | 8/11 [00:10<00:03,  1.24s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.22s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.23s/it]
Evaluating: 100%|██████████| 11/11 [00:13<00:00,  1.03s/it]
Evaluating: 100%|██████████| 11/11 [00:13<00:00,  1.19s/it]
2024-08-26 21:04:58.094 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9706    1.0000    0.9851        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.9565    1.0000    0.9778        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9697    0.9275    0.9481        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.8750    0.9333         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.3913    0.3600    0.3750        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.6000    1.0000    0.7500         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.6364    0.9032    0.7467        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    1.0000    1.0000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     0.5000    1.0000    0.6667         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    0.2500    0.4000         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     0.8000    1.0000    0.8889         4
                       marian     1.0000    0.6000    0.7500         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.8571    0.8571    0.8571         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     0.8571    1.0000    0.9231         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.6230    0.7308    0.6726        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6000    0.7500    0.6667         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.8571    0.8571    0.8571        14
                    unispeech     0.5000    0.5000    0.5000         4
                unispeech-sat     0.0000    0.0000       nan         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6279    0.9643    0.7606        28
                        wavlm     1.0000    0.0833    0.1538        12
                      whisper     1.0000    1.0000    1.0000         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.8235    0.4667    0.5957        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8561       681
                    macro avg     0.8810    0.8455    0.8757       681
                 weighted avg     0.8749    0.8561    0.8548       681

2024-08-26 21:04:58.094 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8561
2024-08-26 21:04:58.098 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:04:58.098 | INFO     | __main__:eval:453 - Validation loss: 3.4368900169025767
2024-08-26 21:04:58.107 | INFO     | __main__:train:255 - epochs: 14
2024-08-26 21:04:58.107 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116]
2024-08-26 21:04:58.108 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767]
2024-08-26 21:04:58.108 | INFO     | __main__:train:157 - ====epoch #15====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:29,  5.00s/it]
Training:   5%|▍         | 2/43 [00:09<03:24,  4.98s/it]
Training:   7%|▋         | 3/43 [00:14<03:19,  4.98s/it]
Training:   9%|▉         | 4/43 [00:20<03:15,  5.01s/it]
Training:  12%|█▏        | 5/43 [00:24<03:09,  5.00s/it]
Training:  14%|█▍        | 6/43 [00:29<03:05,  5.00s/it]
Training:  16%|█▋        | 7/43 [00:35<03:00,  5.01s/it]
Training:  19%|█▊        | 8/43 [00:39<02:54,  4.99s/it]
Training:  21%|██        | 9/43 [00:44<02:50,  5.00s/it]
Training:  23%|██▎       | 10/43 [00:50<02:45,  5.01s/it]
Training:  26%|██▌       | 11/43 [00:55<02:40,  5.01s/it]
Training:  28%|██▊       | 12/43 [00:59<02:33,  4.95s/it]
Training:  30%|███       | 13/43 [01:04<02:29,  4.99s/it]
Training:  33%|███▎      | 14/43 [01:10<02:27,  5.08s/it]
Training:  35%|███▍      | 15/43 [01:15<02:20,  5.03s/it]
Training:  37%|███▋      | 16/43 [01:20<02:15,  5.02s/it]
Training:  40%|███▉      | 17/43 [01:25<02:09,  4.99s/it]
Training:  42%|████▏     | 18/43 [01:29<02:04,  4.97s/it]
Training:  44%|████▍     | 19/43 [01:35<01:59,  5.00s/it]
Training:  47%|████▋     | 20/43 [01:40<01:55,  5.01s/it]
Training:  49%|████▉     | 21/43 [01:45<01:50,  5.01s/it]
Training:  51%|█████     | 22/43 [01:50<01:45,  5.00s/it]
Training:  53%|█████▎    | 23/43 [01:55<01:40,  5.00s/it]
Training:  56%|█████▌    | 24/43 [02:00<01:34,  4.99s/it]
Training:  58%|█████▊    | 25/43 [02:05<01:29,  5.00s/it]
Training:  60%|██████    | 26/43 [02:10<01:25,  5.01s/it]
Training:  63%|██████▎   | 27/43 [02:15<01:20,  5.00s/it]
Training:  65%|██████▌   | 28/43 [02:20<01:15,  5.01s/it]
Training:  67%|██████▋   | 29/43 [02:25<01:10,  5.02s/it]
Training:  70%|██████▉   | 30/43 [02:30<01:05,  5.00s/it]
Training:  72%|███████▏  | 31/43 [02:35<00:59,  5.00s/it]
Training:  74%|███████▍  | 32/43 [02:40<00:55,  5.00s/it]
Training:  77%|███████▋  | 33/43 [02:45<00:50,  5.01s/it]
Training:  79%|███████▉  | 34/43 [02:50<00:45,  5.01s/it]
Training:  81%|████████▏ | 35/43 [02:55<00:40,  5.02s/it]
Training:  84%|████████▎ | 36/43 [03:00<00:35,  5.01s/it]
Training:  86%|████████▌ | 37/43 [03:05<00:30,  5.01s/it]
Training:  88%|████████▊ | 38/43 [03:10<00:25,  5.01s/it]
Training:  91%|█████████ | 39/43 [03:15<00:20,  5.00s/it]
Training:  93%|█████████▎| 40/43 [03:20<00:15,  5.04s/it]
Training:  95%|█████████▌| 41/43 [03:25<00:10,  5.05s/it]
Training:  98%|█████████▊| 42/43 [03:30<00:05,  5.03s/it]
Training: 100%|██████████| 43/43 [03:31<00:00,  3.97s/it]
Training: 100%|██████████| 43/43 [03:31<00:00,  4.93s/it]
2024-08-26 21:08:30.061 | INFO     | __main__:train:206 - Epoch 15 Average Loss: 2.81609
2024-08-26 21:08:31.676 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL15*****
2024-08-26 21:08:31.677 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:08:31.677 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.44s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.29s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.23s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.25s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.24s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.23s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.24s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.22s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.22s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.20s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.01s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.18s/it]
2024-08-26 21:08:44.626 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9706    1.0000    0.9851        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.9565    1.0000    0.9778        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.8125    0.9420    0.8725        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.8000    0.8000    0.8000         5
                    camembert     0.6000    0.3600    0.4500        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     0.8000    1.0000    0.8889         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.7500    1.0000    0.8571         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    0.5556    0.7143         9
                      electra     0.6122    0.9677    0.7500        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    0.6667    0.8000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     0.5000    1.0000    0.6667         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    0.2500    0.4000         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     0.8000    1.0000    0.8889         4
                       marian     1.0000    0.8000    0.8889         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7500    0.8571    0.8000         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    1.0000    1.0000         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.6071    0.6538    0.6296        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6667    0.5000    0.5714         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.9231    0.8571    0.8889        14
                    unispeech     0.6000    0.7500    0.6667         4
                unispeech-sat     0.0000    0.0000       nan         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     0.7500    1.0000    0.8571         3
                     wav2vec2     0.7407    0.7143    0.7273        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.9333    0.4667    0.6222        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8502       681
                    macro avg     0.8766    0.8437    0.8758       681
                 weighted avg     0.8659    0.8502    0.8506       681

2024-08-26 21:08:44.626 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8502
2024-08-26 21:08:44.630 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:08:44.630 | INFO     | __main__:eval:453 - Validation loss: 3.509556835347956
2024-08-26 21:08:44.643 | INFO     | __main__:train:255 - epochs: 15
2024-08-26 21:08:44.644 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725]
2024-08-26 21:08:44.644 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956]
2024-08-26 21:08:44.644 | INFO     | __main__:train:157 - ====epoch #16====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:33,  5.07s/it]
Training:   5%|▍         | 2/43 [00:10<03:25,  5.01s/it]
Training:   7%|▋         | 3/43 [00:15<03:21,  5.03s/it]
Training:   9%|▉         | 4/43 [00:20<03:15,  5.01s/it]
Training:  12%|█▏        | 5/43 [00:24<03:09,  4.98s/it]
Training:  14%|█▍        | 6/43 [00:29<03:01,  4.91s/it]
Training:  16%|█▋        | 7/43 [00:34<02:54,  4.85s/it]
Training:  19%|█▊        | 8/43 [00:39<02:48,  4.82s/it]
Training:  21%|██        | 9/43 [00:44<02:46,  4.91s/it]
Training:  23%|██▎       | 10/43 [00:49<02:42,  4.91s/it]
Training:  26%|██▌       | 11/43 [00:54<02:36,  4.88s/it]
Training:  28%|██▊       | 12/43 [00:58<02:29,  4.84s/it]
Training:  30%|███       | 13/43 [01:03<02:26,  4.90s/it]
Training:  33%|███▎      | 14/43 [01:08<02:22,  4.91s/it]
Training:  35%|███▍      | 15/43 [01:13<02:17,  4.90s/it]
Training:  37%|███▋      | 16/43 [01:18<02:12,  4.90s/it]
Training:  40%|███▉      | 17/43 [01:23<02:07,  4.89s/it]
Training:  42%|████▏     | 18/43 [01:28<02:01,  4.84s/it]
Training:  44%|████▍     | 19/43 [01:33<01:57,  4.91s/it]
Training:  47%|████▋     | 20/43 [01:38<01:52,  4.89s/it]
Training:  49%|████▉     | 21/43 [01:42<01:47,  4.88s/it]
Training:  51%|█████     | 22/43 [01:47<01:41,  4.85s/it]
Training:  53%|█████▎    | 23/43 [01:52<01:36,  4.82s/it]
Training:  56%|█████▌    | 24/43 [01:57<01:31,  4.80s/it]
Training:  58%|█████▊    | 25/43 [02:02<01:27,  4.86s/it]
Training:  60%|██████    | 26/43 [02:07<01:22,  4.84s/it]
Training:  63%|██████▎   | 27/43 [02:11<01:17,  4.82s/it]
Training:  65%|██████▌   | 28/43 [02:16<01:11,  4.78s/it]
Training:  67%|██████▋   | 29/43 [02:21<01:08,  4.88s/it]
Training:  70%|██████▉   | 30/43 [02:26<01:03,  4.88s/it]
Training:  72%|███████▏  | 31/43 [02:31<00:58,  4.86s/it]
Training:  74%|███████▍  | 32/43 [02:36<00:53,  4.83s/it]
Training:  77%|███████▋  | 33/43 [02:41<00:49,  4.91s/it]
Training:  79%|███████▉  | 34/43 [02:46<00:44,  4.92s/it]
Training:  81%|████████▏ | 35/43 [02:50<00:39,  4.89s/it]
Training:  84%|████████▎ | 36/43 [02:55<00:34,  4.87s/it]
Training:  86%|████████▌ | 37/43 [03:00<00:29,  4.86s/it]
Training:  88%|████████▊ | 38/43 [03:05<00:24,  4.84s/it]
Training:  91%|█████████ | 39/43 [03:10<00:19,  4.91s/it]
Training:  93%|█████████▎| 40/43 [03:15<00:14,  4.91s/it]
Training:  95%|█████████▌| 41/43 [03:20<00:09,  4.89s/it]
Training:  98%|█████████▊| 42/43 [03:24<00:04,  4.87s/it]
Training: 100%|██████████| 43/43 [03:26<00:00,  3.84s/it]
Training: 100%|██████████| 43/43 [03:26<00:00,  4.80s/it]
2024-08-26 21:12:11.131 | INFO     | __main__:train:206 - Epoch 16 Average Loss: 2.90781
2024-08-26 21:12:12.736 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL16*****
2024-08-26 21:12:12.736 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:12:12.737 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.32s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.23s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.19s/it]
Evaluating:  36%|███▋      | 4/11 [00:04<00:08,  1.22s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.23s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.24s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.22s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.20s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.19s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.21s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.03s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.16s/it]
2024-08-26 21:12:25.560 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9167    1.0000    0.9565        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.8400    0.9545    0.8936        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9697    0.9275    0.9481        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.5000    0.2000    0.2857        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.6667    1.0000    0.8000         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    0.7143    0.8333         7
                   convnextv2     0.5000    1.0000    0.6667         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.5000    1.0000    0.6667         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.5741    1.0000    0.7294        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    1.0000    1.0000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    0.7500    0.8571         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     1.0000    0.7500    0.8571         4
                       marian     0.0000    0.0000       nan         5
                  mask2former     1.0000    0.8750    0.9333         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     0.8947    1.0000    0.9444        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.6667    0.8000         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7500    0.8571    0.8000         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     0.8333    0.8333    0.8333         6
                       regnet     1.0000    0.6667    0.8000         3
                       resnet     0.8000    1.0000    0.8889         4
                      roberta     0.6230    0.7308    0.6726        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6000    0.7500    0.6667         4
                     speecht5        nan    0.0000       nan         2
                         swin     0.8571    1.0000    0.9231         6
                           t5     0.8000    0.8571    0.8276        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.7600    0.6786    0.7170        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.8333    0.5000    0.6250        30
                        xlnet     0.9167    1.0000    0.9565        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8399       681
                    macro avg     0.8615    0.8172    0.8588       681
                 weighted avg     0.8537    0.8399    0.8427       681

2024-08-26 21:12:25.560 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8399
2024-08-26 21:12:25.563 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:12:25.563 | INFO     | __main__:eval:453 - Validation loss: 3.486647194082087
2024-08-26 21:12:25.576 | INFO     | __main__:train:255 - epochs: 16
2024-08-26 21:12:25.577 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236]
2024-08-26 21:12:25.577 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087]
2024-08-26 21:12:25.577 | INFO     | __main__:train:157 - ====epoch #17====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:29,  4.99s/it]
Training:   5%|▍         | 2/43 [00:09<03:18,  4.83s/it]
Training:   7%|▋         | 3/43 [00:14<03:12,  4.80s/it]
Training:   9%|▉         | 4/43 [00:19<03:06,  4.79s/it]
Training:  12%|█▏        | 5/43 [00:23<03:01,  4.77s/it]
Training:  14%|█▍        | 6/43 [00:28<02:56,  4.78s/it]
Training:  16%|█▋        | 7/43 [00:33<02:55,  4.89s/it]
Training:  19%|█▊        | 8/43 [00:38<02:51,  4.89s/it]
Training:  21%|██        | 9/43 [00:43<02:44,  4.84s/it]
Training:  23%|██▎       | 10/43 [00:48<02:41,  4.90s/it]
Training:  26%|██▌       | 11/43 [00:53<02:35,  4.86s/it]
Training:  28%|██▊       | 12/43 [00:58<02:29,  4.81s/it]
Training:  30%|███       | 13/43 [01:03<02:26,  4.90s/it]
Training:  33%|███▎      | 14/43 [01:07<02:21,  4.89s/it]
Training:  35%|███▍      | 15/43 [01:12<02:16,  4.86s/it]
Training:  37%|███▋      | 16/43 [01:17<02:10,  4.82s/it]
Training:  40%|███▉      | 17/43 [01:22<02:07,  4.89s/it]
Training:  42%|████▏     | 18/43 [01:27<02:01,  4.86s/it]
Training:  44%|████▍     | 19/43 [01:32<01:57,  4.91s/it]
Training:  47%|████▋     | 20/43 [01:37<01:52,  4.90s/it]
Training:  49%|████▉     | 21/43 [01:42<01:47,  4.87s/it]
Training:  51%|█████     | 22/43 [01:46<01:41,  4.85s/it]
Training:  53%|█████▎    | 23/43 [01:51<01:38,  4.91s/it]
Training:  56%|█████▌    | 24/43 [01:56<01:33,  4.90s/it]
Training:  58%|█████▊    | 25/43 [02:01<01:27,  4.85s/it]
Training:  60%|██████    | 26/43 [02:06<01:22,  4.83s/it]
Training:  63%|██████▎   | 27/43 [02:11<01:18,  4.89s/it]
Training:  65%|██████▌   | 28/43 [02:16<01:13,  4.91s/it]
Training:  67%|██████▋   | 29/43 [02:21<01:09,  4.95s/it]
Training:  70%|██████▉   | 30/43 [02:26<01:04,  4.94s/it]
Training:  72%|███████▏  | 31/43 [02:31<00:59,  4.96s/it]
Training:  74%|███████▍  | 32/43 [02:36<00:54,  4.97s/it]
Training:  77%|███████▋  | 33/43 [02:41<00:49,  4.92s/it]
Training:  79%|███████▉  | 34/43 [02:45<00:43,  4.84s/it]
Training:  81%|████████▏ | 35/43 [02:50<00:38,  4.80s/it]
Training:  84%|████████▎ | 36/43 [02:55<00:33,  4.79s/it]
Training:  86%|████████▌ | 37/43 [03:00<00:29,  4.84s/it]
Training:  88%|████████▊ | 38/43 [03:04<00:24,  4.83s/it]
Training:  91%|█████████ | 39/43 [03:10<00:20,  5.00s/it]
Training:  93%|█████████▎| 40/43 [03:15<00:14,  4.97s/it]
Training:  95%|█████████▌| 41/43 [03:19<00:09,  4.91s/it]
Training:  98%|█████████▊| 42/43 [03:24<00:04,  4.85s/it]
Training: 100%|██████████| 43/43 [03:26<00:00,  3.86s/it]
Training: 100%|██████████| 43/43 [03:26<00:00,  4.80s/it]
2024-08-26 21:15:51.880 | INFO     | __main__:train:206 - Epoch 17 Average Loss: 3.00597
2024-08-26 21:15:53.488 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL17*****
2024-08-26 21:15:53.488 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:15:53.488 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.34s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.23s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.19s/it]
Evaluating:  36%|███▋      | 4/11 [00:04<00:08,  1.18s/it]
Evaluating:  45%|████▌     | 5/11 [00:05<00:06,  1.16s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.16s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.16s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.16s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.15s/it]
Evaluating:  91%|█████████ | 10/11 [00:11<00:01,  1.15s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.02it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.12s/it]
2024-08-26 21:16:05.811 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9697    0.9275    0.9481        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.3191    0.6000    0.4167        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     1.0000    1.0000    1.0000         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     1.0000    1.0000    1.0000         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.6000    1.0000    0.7500         3
                   distilbert     0.9714    0.9714    0.9714        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.8000    0.5161    0.6275        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.7500    1.0000    0.8571         3
                     gpt_neox     0.5000    1.0000    0.6667         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     0.5000    1.0000    0.6667         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    0.7500    0.8571         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     1.0000    0.5000    0.6667         4
                       marian     1.0000    1.0000    1.0000         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     0.5000    1.0000    0.6667         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     0.9444    1.0000    0.9714        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.6667    0.8000         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.6667    0.8571    0.7500         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    1.0000    1.0000         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.6000    0.6923    0.6429        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.5714    1.0000    0.7273         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.7857    0.7857    0.7857        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6279    0.9643    0.7606        28
                        wavlm     1.0000    0.0833    0.1538        12
                      whisper     1.0000    1.0000    1.0000         8
                         xglm     1.0000    0.7500    0.8571         4
                  xlm-roberta     0.8235    0.4667    0.5957        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8399       681
                    macro avg     0.8862    0.8313    0.8588       681
                 weighted avg     0.8794    0.8399    0.8434       681

2024-08-26 21:16:05.812 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8399
2024-08-26 21:16:05.814 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:16:05.815 | INFO     | __main__:eval:453 - Validation loss: 3.451884161342274
2024-08-26 21:16:05.825 | INFO     | __main__:train:255 - epochs: 17
2024-08-26 21:16:05.825 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567]
2024-08-26 21:16:05.826 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274]
2024-08-26 21:16:05.826 | INFO     | __main__:train:157 - ====epoch #18====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:28,  4.96s/it]
Training:   5%|▍         | 2/43 [00:09<03:18,  4.84s/it]
Training:   7%|▋         | 3/43 [00:14<03:13,  4.83s/it]
Training:   9%|▉         | 4/43 [00:19<03:07,  4.80s/it]
Training:  12%|█▏        | 5/43 [00:24<03:02,  4.79s/it]
Training:  14%|█▍        | 6/43 [00:28<02:56,  4.78s/it]
Training:  16%|█▋        | 7/43 [00:33<02:52,  4.79s/it]
Training:  19%|█▊        | 8/43 [00:38<02:46,  4.76s/it]
Training:  21%|██        | 9/43 [00:43<02:41,  4.76s/it]
Training:  23%|██▎       | 10/43 [00:47<02:36,  4.75s/it]
Training:  26%|██▌       | 11/43 [00:52<02:32,  4.76s/it]
Training:  28%|██▊       | 12/43 [00:57<02:27,  4.76s/it]
Training:  30%|███       | 13/43 [01:02<02:22,  4.76s/it]
Training:  33%|███▎      | 14/43 [01:06<02:18,  4.76s/it]
Training:  35%|███▍      | 15/43 [01:11<02:13,  4.76s/it]
Training:  37%|███▋      | 16/43 [01:16<02:08,  4.74s/it]
Training:  40%|███▉      | 17/43 [01:20<02:02,  4.71s/it]
Training:  42%|████▏     | 18/43 [01:25<01:58,  4.72s/it]
Training:  44%|████▍     | 19/43 [01:30<01:53,  4.73s/it]
Training:  47%|████▋     | 20/43 [01:35<01:49,  4.74s/it]
Training:  49%|████▉     | 21/43 [01:39<01:44,  4.75s/it]
Training:  51%|█████     | 22/43 [01:44<01:39,  4.74s/it]
Training:  53%|█████▎    | 23/43 [01:49<01:34,  4.74s/it]
Training:  56%|█████▌    | 24/43 [01:54<01:31,  4.79s/it]
Training:  58%|█████▊    | 25/43 [01:59<01:25,  4.77s/it]
Training:  60%|██████    | 26/43 [02:03<01:21,  4.77s/it]
Training:  63%|██████▎   | 27/43 [02:08<01:16,  4.77s/it]
Training:  65%|██████▌   | 28/43 [02:13<01:11,  4.77s/it]
Training:  67%|██████▋   | 29/43 [02:18<01:06,  4.77s/it]
Training:  70%|██████▉   | 30/43 [02:22<01:01,  4.76s/it]
Training:  72%|███████▏  | 31/43 [02:27<00:57,  4.75s/it]
Training:  74%|███████▍  | 32/43 [02:32<00:52,  4.74s/it]
Training:  77%|███████▋  | 33/43 [02:37<00:47,  4.75s/it]
Training:  79%|███████▉  | 34/43 [02:41<00:42,  4.75s/it]
Training:  81%|████████▏ | 35/43 [02:46<00:37,  4.75s/it]
Training:  84%|████████▎ | 36/43 [02:51<00:33,  4.76s/it]
Training:  86%|████████▌ | 37/43 [02:56<00:28,  4.75s/it]
Training:  88%|████████▊ | 38/43 [03:00<00:23,  4.76s/it]
Training:  91%|█████████ | 39/43 [03:05<00:19,  4.75s/it]
Training:  93%|█████████▎| 40/43 [03:10<00:14,  4.75s/it]
Training:  95%|█████████▌| 41/43 [03:15<00:09,  4.75s/it]
Training:  98%|█████████▊| 42/43 [03:19<00:04,  4.72s/it]
Training: 100%|██████████| 43/43 [03:21<00:00,  3.71s/it]
Training: 100%|██████████| 43/43 [03:21<00:00,  4.68s/it]
2024-08-26 21:19:27.043 | INFO     | __main__:train:206 - Epoch 18 Average Loss: 2.77641
2024-08-26 21:19:28.646 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL18*****
2024-08-26 21:19:28.647 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:19:28.647 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.39s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.30s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.27s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.26s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.25s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.23s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.24s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.22s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.20s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.22s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.02s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.18s/it]
2024-08-26 21:19:41.634 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.9565    1.0000    0.9778        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9275    0.9275    0.9275        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.2941    0.6000    0.3947        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     0.8000    1.0000    0.8889         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.7500    1.0000    0.8571         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.7647    0.4194    0.5417        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    0.6667    0.8000         3
                     gpt_neox     0.4000    1.0000    0.5714         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     0.3333    1.0000    0.5000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    0.5000    0.6667         4
                        llama     0.5000    0.6667    0.5714         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     0.7500    1.0000    0.8571         3
                      m2m_100     0.8000    1.0000    0.8889         4
                       marian     0.6667    0.8000    0.7273         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7500    0.8571    0.8000         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     1.0000    0.6667    0.8000         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.6379    0.7115    0.6727        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6000    0.7500    0.6667         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.9091    0.7143    0.8000        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.7407    0.7143    0.7273        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.9333    0.4667    0.6222        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8370       681
                    macro avg     0.8739    0.8345    0.8585       681
                 weighted avg     0.8762    0.8370    0.8456       681

2024-08-26 21:19:41.634 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.837
2024-08-26 21:19:41.637 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:19:41.637 | INFO     | __main__:eval:453 - Validation loss: 3.4414474097165195
2024-08-26 21:19:41.645 | INFO     | __main__:train:255 - epochs: 18
2024-08-26 21:19:41.646 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376]
2024-08-26 21:19:41.646 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195]
2024-08-26 21:19:41.646 | INFO     | __main__:train:157 - ====epoch #19====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:34,  5.10s/it]
Training:   5%|▍         | 2/43 [00:10<03:26,  5.03s/it]
Training:   7%|▋         | 3/43 [00:14<03:18,  4.95s/it]
Training:   9%|▉         | 4/43 [00:19<03:11,  4.91s/it]
Training:  12%|█▏        | 5/43 [00:24<03:06,  4.90s/it]
Training:  14%|█▍        | 6/43 [00:29<03:00,  4.88s/it]
Training:  16%|█▋        | 7/43 [00:34<02:55,  4.87s/it]
Training:  19%|█▊        | 8/43 [00:39<02:49,  4.86s/it]
Training:  21%|██        | 9/43 [00:44<02:44,  4.85s/it]
Training:  23%|██▎       | 10/43 [00:48<02:39,  4.82s/it]
Training:  26%|██▌       | 11/43 [00:53<02:35,  4.86s/it]
Training:  28%|██▊       | 12/43 [00:58<02:30,  4.84s/it]
Training:  30%|███       | 13/43 [01:03<02:24,  4.81s/it]
Training:  33%|███▎      | 14/43 [01:07<02:18,  4.78s/it]
Training:  35%|███▍      | 15/43 [01:12<02:13,  4.78s/it]
Training:  37%|███▋      | 16/43 [01:17<02:08,  4.76s/it]
Training:  40%|███▉      | 17/43 [01:22<02:03,  4.76s/it]
Training:  42%|████▏     | 18/43 [01:26<01:58,  4.75s/it]
Training:  44%|████▍     | 19/43 [01:31<01:53,  4.74s/it]
Training:  47%|████▋     | 20/43 [01:36<01:49,  4.74s/it]
Training:  49%|████▉     | 21/43 [01:41<01:44,  4.74s/it]
Training:  51%|█████     | 22/43 [01:45<01:39,  4.76s/it]
Training:  53%|█████▎    | 23/43 [01:50<01:35,  4.76s/it]
Training:  56%|█████▌    | 24/43 [01:55<01:30,  4.75s/it]
Training:  58%|█████▊    | 25/43 [02:00<01:25,  4.75s/it]
Training:  60%|██████    | 26/43 [02:04<01:20,  4.74s/it]
Training:  63%|██████▎   | 27/43 [02:09<01:15,  4.74s/it]
Training:  65%|██████▌   | 28/43 [02:14<01:11,  4.74s/it]
Training:  67%|██████▋   | 29/43 [02:19<01:06,  4.75s/it]
Training:  70%|██████▉   | 30/43 [02:23<01:01,  4.75s/it]
Training:  72%|███████▏  | 31/43 [02:28<00:56,  4.73s/it]
Training:  74%|███████▍  | 32/43 [02:33<00:51,  4.72s/it]
Training:  77%|███████▋  | 33/43 [02:38<00:47,  4.73s/it]
Training:  79%|███████▉  | 34/43 [02:42<00:42,  4.72s/it]
Training:  81%|████████▏ | 35/43 [02:47<00:37,  4.72s/it]
Training:  84%|████████▎ | 36/43 [02:52<00:33,  4.73s/it]
Training:  86%|████████▌ | 37/43 [02:57<00:28,  4.77s/it]
Training:  88%|████████▊ | 38/43 [03:01<00:23,  4.76s/it]
Training:  91%|█████████ | 39/43 [03:06<00:18,  4.74s/it]
Training:  93%|█████████▎| 40/43 [03:11<00:14,  4.75s/it]
Training:  95%|█████████▌| 41/43 [03:16<00:09,  4.74s/it]
Training:  98%|█████████▊| 42/43 [03:20<00:04,  4.73s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  3.73s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  4.70s/it]
2024-08-26 21:23:03.794 | INFO     | __main__:train:206 - Epoch 19 Average Loss: 2.72914
2024-08-26 21:23:05.377 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL19*****
2024-08-26 21:23:05.377 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:23:05.377 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.32s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.23s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.19s/it]
Evaluating:  36%|███▋      | 4/11 [00:04<00:08,  1.19s/it]
Evaluating:  45%|████▌     | 5/11 [00:05<00:07,  1.17s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.16s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.16s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.16s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.16s/it]
Evaluating:  91%|█████████ | 10/11 [00:11<00:01,  1.19s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.01s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.13s/it]
2024-08-26 21:23:17.857 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9706    1.0000    0.9851        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.9565    1.0000    0.9778        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9412    0.9275    0.9343        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.3333    0.4000    0.3636        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     1.0000    1.0000    1.0000         4
                      codegen     0.8000    1.0000    0.8889         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.6000    1.0000    0.7500         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.6765    0.7419    0.7077        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     1.0000    1.0000    1.0000         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    1.0000    1.0000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.8333    0.9091         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    1.0000    1.0000         4
                        llama     0.5000    0.6667    0.5714         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     0.7500    1.0000    0.8571         3
                      m2m_100     0.7500    0.7500    0.7500         4
                       marian     0.6667    0.8000    0.7273         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     0.6667    1.0000    0.8000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.8571    0.8571    0.8571         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     1.0000    0.6667    0.8000         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.6290    0.7500    0.6842        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6667    1.0000    0.8000         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.9231    0.8571    0.8889        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.7692    0.7143    0.7407        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.9333    0.4667    0.6222        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8561       681
                    macro avg     0.8922    0.8526    0.8850       681
                 weighted avg     0.8776    0.8561    0.8612       681

2024-08-26 21:23:17.857 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8561
2024-08-26 21:23:17.863 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:23:17.863 | INFO     | __main__:eval:453 - Validation loss: 3.399580326947299
2024-08-26 21:23:17.873 | INFO     | __main__:train:255 - epochs: 19
2024-08-26 21:23:17.874 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691]
2024-08-26 21:23:17.874 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299]
2024-08-26 21:23:17.874 | INFO     | __main__:train:157 - ====epoch #20====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:31,  5.05s/it]
Training:   5%|▍         | 2/43 [00:09<03:23,  4.97s/it]
Training:   7%|▋         | 3/43 [00:14<03:16,  4.92s/it]
Training:   9%|▉         | 4/43 [00:19<03:12,  4.94s/it]
Training:  12%|█▏        | 5/43 [00:24<03:08,  4.96s/it]
Training:  14%|█▍        | 6/43 [00:29<03:02,  4.94s/it]
Training:  16%|█▋        | 7/43 [00:34<02:58,  4.96s/it]
Training:  19%|█▊        | 8/43 [00:39<02:53,  4.97s/it]
Training:  21%|██        | 9/43 [00:44<02:48,  4.96s/it]
Training:  23%|██▎       | 10/43 [00:49<02:44,  4.97s/it]
Training:  26%|██▌       | 11/43 [00:54<02:38,  4.97s/it]
Training:  28%|██▊       | 12/43 [00:59<02:33,  4.97s/it]
Training:  30%|███       | 13/43 [01:04<02:29,  4.98s/it]
Training:  33%|███▎      | 14/43 [01:09<02:24,  4.98s/it]
Training:  35%|███▍      | 15/43 [01:14<02:18,  4.95s/it]
Training:  37%|███▋      | 16/43 [01:19<02:12,  4.93s/it]
Training:  40%|███▉      | 17/43 [01:24<02:07,  4.90s/it]
Training:  42%|████▏     | 18/43 [01:28<02:02,  4.89s/it]
Training:  44%|████▍     | 19/43 [01:33<01:57,  4.91s/it]
Training:  47%|████▋     | 20/43 [01:39<01:55,  5.02s/it]
Training:  49%|████▉     | 21/43 [01:44<01:49,  4.99s/it]
Training:  51%|█████     | 22/43 [01:49<01:44,  5.00s/it]
Training:  53%|█████▎    | 23/43 [01:54<01:39,  4.97s/it]
Training:  56%|█████▌    | 24/43 [01:58<01:33,  4.94s/it]
Training:  58%|█████▊    | 25/43 [02:03<01:28,  4.90s/it]
Training:  60%|██████    | 26/43 [02:08<01:22,  4.88s/it]
Training:  63%|██████▎   | 27/43 [02:13<01:17,  4.85s/it]
Training:  65%|██████▌   | 28/43 [02:18<01:12,  4.85s/it]
Training:  67%|██████▋   | 29/43 [02:23<01:08,  4.91s/it]
Training:  70%|██████▉   | 30/43 [02:28<01:03,  4.92s/it]
Training:  72%|███████▏  | 31/43 [02:33<00:59,  4.92s/it]
Training:  74%|███████▍  | 32/43 [02:38<00:54,  4.94s/it]
Training:  77%|███████▋  | 33/43 [02:43<00:49,  4.94s/it]
Training:  79%|███████▉  | 34/43 [02:48<00:44,  4.97s/it]
Training:  81%|████████▏ | 35/43 [02:53<00:39,  4.99s/it]
Training:  84%|████████▎ | 36/43 [02:58<00:35,  5.01s/it]
Training:  86%|████████▌ | 37/43 [03:03<00:30,  5.02s/it]
Training:  88%|████████▊ | 38/43 [03:08<00:25,  5.02s/it]
Training:  91%|█████████ | 39/43 [03:13<00:20,  5.02s/it]
Training:  93%|█████████▎| 40/43 [03:18<00:15,  5.03s/it]
Training:  95%|█████████▌| 41/43 [03:23<00:10,  5.02s/it]
Training:  98%|█████████▊| 42/43 [03:28<00:05,  5.01s/it]
Training: 100%|██████████| 43/43 [03:29<00:00,  3.97s/it]
Training: 100%|██████████| 43/43 [03:29<00:00,  4.88s/it]
2024-08-26 21:26:47.720 | INFO     | __main__:train:206 - Epoch 20 Average Loss: 2.87710
2024-08-26 21:26:49.328 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL20*****
2024-08-26 21:26:49.329 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:26:49.329 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.42s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.32s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.29s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.28s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.24s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.22s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.23s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.21s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.20s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.19s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.00it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.17s/it]
2024-08-26 21:27:02.236 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9706    1.0000    0.9851        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.9565    1.0000    0.9778        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.8649    0.9275    0.8951        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.4444    0.4800    0.4615        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.6000    1.0000    0.7500         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.6585    0.8710    0.7500        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    1.0000    1.0000         3
                     gpt_neox     1.0000    1.0000    1.0000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    1.0000    1.0000         4
                        llama     0.2500    0.3333    0.2857         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     0.7500    1.0000    0.8571         3
                      m2m_100     0.6667    1.0000    0.8000         4
                       marian     1.0000    0.8000    0.8889         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.8571    0.8571    0.8571         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     1.0000    1.0000    1.0000         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.7021    0.6346    0.6667        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6000    0.7500    0.6667         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.8571    0.8571    0.8571        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6136    0.9643    0.7500        28
                        wavlm        nan    0.0000       nan        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.8000    0.5333    0.6400        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8546       681
                    macro avg     0.8947    0.8444    0.8887       681
                 weighted avg     0.8723    0.8546    0.8664       681

2024-08-26 21:27:02.236 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8546
2024-08-26 21:27:02.239 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:27:02.239 | INFO     | __main__:eval:453 - Validation loss: 3.4127013466574927
2024-08-26 21:27:02.247 | INFO     | __main__:train:255 - epochs: 20
2024-08-26 21:27:02.248 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889]
2024-08-26 21:27:02.248 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927]
2024-08-26 21:27:02.248 | INFO     | __main__:train:157 - ====epoch #21====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:32,  5.05s/it]
Training:   5%|▍         | 2/43 [00:10<03:25,  5.02s/it]
Training:   7%|▋         | 3/43 [00:14<03:19,  4.98s/it]
Training:   9%|▉         | 4/43 [00:20<03:16,  5.04s/it]
Training:  12%|█▏        | 5/43 [00:24<03:08,  4.95s/it]
Training:  14%|█▍        | 6/43 [00:29<03:00,  4.89s/it]
Training:  16%|█▋        | 7/43 [00:34<02:57,  4.94s/it]
Training:  19%|█▊        | 8/43 [00:39<02:52,  4.93s/it]
Training:  21%|██        | 9/43 [00:44<02:45,  4.88s/it]
Training:  23%|██▎       | 10/43 [00:49<02:39,  4.84s/it]
Training:  26%|██▌       | 11/43 [00:54<02:36,  4.90s/it]
Training:  28%|██▊       | 12/43 [00:59<02:32,  4.93s/it]
Training:  30%|███       | 13/43 [01:04<02:27,  4.90s/it]
Training:  33%|███▎      | 14/43 [01:08<02:20,  4.85s/it]
Training:  35%|███▍      | 15/43 [01:13<02:17,  4.91s/it]
Training:  37%|███▋      | 16/43 [01:18<02:12,  4.91s/it]
Training:  40%|███▉      | 17/43 [01:23<02:07,  4.89s/it]
Training:  42%|████▏     | 18/43 [01:28<02:01,  4.86s/it]
Training:  44%|████▍     | 19/43 [01:33<01:56,  4.84s/it]
Training:  47%|████▋     | 20/43 [01:37<01:50,  4.81s/it]
Training:  49%|████▉     | 21/43 [01:42<01:47,  4.89s/it]
Training:  51%|█████     | 22/43 [01:47<01:42,  4.90s/it]
Training:  53%|█████▎    | 23/43 [01:52<01:37,  4.88s/it]
Training:  56%|█████▌    | 24/43 [01:57<01:32,  4.85s/it]
Training:  58%|█████▊    | 25/43 [02:02<01:28,  4.90s/it]
Training:  60%|██████    | 26/43 [02:07<01:22,  4.87s/it]
Training:  63%|██████▎   | 27/43 [02:12<01:18,  4.93s/it]
Training:  65%|██████▌   | 28/43 [02:17<01:13,  4.93s/it]
Training:  67%|██████▋   | 29/43 [02:22<01:09,  4.96s/it]
Training:  70%|██████▉   | 30/43 [02:27<01:06,  5.08s/it]
Training:  72%|███████▏  | 31/43 [02:32<01:00,  5.08s/it]
Training:  74%|███████▍  | 32/43 [02:37<00:55,  5.03s/it]
Training:  77%|███████▋  | 33/43 [02:42<00:49,  4.94s/it]
Training:  79%|███████▉  | 34/43 [02:47<00:44,  4.89s/it]
Training:  81%|████████▏ | 35/43 [02:51<00:38,  4.85s/it]
Training:  84%|████████▎ | 36/43 [02:56<00:33,  4.82s/it]
Training:  86%|████████▌ | 37/43 [03:01<00:29,  4.89s/it]
Training:  88%|████████▊ | 38/43 [03:06<00:24,  4.92s/it]
Training:  91%|█████████ | 39/43 [03:11<00:19,  4.95s/it]
Training:  93%|█████████▎| 40/43 [03:16<00:14,  4.98s/it]
Training:  95%|█████████▌| 41/43 [03:21<00:09,  5.00s/it]
Training:  98%|█████████▊| 42/43 [03:26<00:04,  4.98s/it]
Training: 100%|██████████| 43/43 [03:28<00:00,  3.96s/it]
Training: 100%|██████████| 43/43 [03:28<00:00,  4.85s/it]
2024-08-26 21:30:30.665 | INFO     | __main__:train:206 - Epoch 21 Average Loss: 2.65983
2024-08-26 21:30:32.283 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL21*****
2024-08-26 21:30:32.283 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:30:32.284 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.38s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.27s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.22s/it]
Evaluating:  36%|███▋      | 4/11 [00:04<00:08,  1.20s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.18s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.17s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.16s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.16s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.18s/it]
Evaluating:  91%|█████████ | 10/11 [00:11<00:01,  1.18s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.01it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.14s/it]
2024-08-26 21:30:44.796 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.8649    0.9275    0.8951        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.3182    0.2800    0.2979        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     1.0000    1.0000    1.0000         4
                      codegen     0.8000    1.0000    0.8889         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.7500    1.0000    0.8571         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.8500    0.5484    0.6667        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     1.0000    1.0000    1.0000         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    1.0000    1.0000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.8333    0.9091         6
                     layoutlm        nan    0.0000       nan         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     0.8000    1.0000    0.8889         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     1.0000    0.5000    0.6667         4
                       marian     1.0000    1.0000    1.0000         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     0.6667    1.0000    0.8000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7000    1.0000    0.8235         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    1.0000    1.0000         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.5797    0.7692    0.6612        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6667    1.0000    0.8000         4
                     speecht5     1.0000    0.5000    0.6667         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.8571    0.8571    0.8571        14
                    unispeech     0.5000    0.5000    0.5000         4
                unispeech-sat     0.0000    0.0000       nan         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6000    0.9643    0.7397        28
                        wavlm        nan    0.0000       nan        12
                      whisper     1.0000    1.0000    1.0000         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.7083    0.5667    0.6296        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8488       681
                    macro avg     0.8909    0.8466    0.8971       681
                 weighted avg     0.8636    0.8488    0.8603       681

2024-08-26 21:30:44.796 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8488
2024-08-26 21:30:44.800 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:30:44.800 | INFO     | __main__:eval:453 - Validation loss: 3.407679232684049
2024-08-26 21:30:44.810 | INFO     | __main__:train:255 - epochs: 21
2024-08-26 21:30:44.811 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889, 2.659833694613257]
2024-08-26 21:30:44.811 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927, 3.407679232684049]
2024-08-26 21:30:44.811 | INFO     | __main__:train:157 - ====epoch #22====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:27,  4.94s/it]
Training:   5%|▍         | 2/43 [00:09<03:19,  4.86s/it]
Training:   7%|▋         | 3/43 [00:14<03:12,  4.82s/it]
Training:   9%|▉         | 4/43 [00:19<03:07,  4.80s/it]
Training:  12%|█▏        | 5/43 [00:24<03:01,  4.78s/it]
Training:  14%|█▍        | 6/43 [00:28<02:56,  4.77s/it]
Training:  16%|█▋        | 7/43 [00:33<02:50,  4.74s/it]
Training:  19%|█▊        | 8/43 [00:38<02:45,  4.74s/it]
Training:  21%|██        | 9/43 [00:42<02:41,  4.74s/it]
Training:  23%|██▎       | 10/43 [00:47<02:36,  4.74s/it]
Training:  26%|██▌       | 11/43 [00:52<02:31,  4.74s/it]
Training:  28%|██▊       | 12/43 [00:57<02:27,  4.75s/it]
Training:  30%|███       | 13/43 [01:01<02:22,  4.74s/it]
Training:  33%|███▎      | 14/43 [01:06<02:17,  4.74s/it]
Training:  35%|███▍      | 15/43 [01:11<02:12,  4.75s/it]
Training:  37%|███▋      | 16/43 [01:16<02:07,  4.73s/it]
Training:  40%|███▉      | 17/43 [01:21<02:04,  4.80s/it]
Training:  42%|████▏     | 18/43 [01:25<01:59,  4.78s/it]
Training:  44%|████▍     | 19/43 [01:30<01:54,  4.77s/it]
Training:  47%|████▋     | 20/43 [01:35<01:49,  4.77s/it]
Training:  49%|████▉     | 21/43 [01:40<01:44,  4.75s/it]
Training:  51%|█████     | 22/43 [01:44<01:39,  4.74s/it]
Training:  53%|█████▎    | 23/43 [01:49<01:35,  4.75s/it]
Training:  56%|█████▌    | 24/43 [01:54<01:29,  4.73s/it]
Training:  58%|█████▊    | 25/43 [01:58<01:25,  4.73s/it]
Training:  60%|██████    | 26/43 [02:03<01:20,  4.72s/it]
Training:  63%|██████▎   | 27/43 [02:08<01:15,  4.71s/it]
Training:  65%|██████▌   | 28/43 [02:13<01:10,  4.73s/it]
Training:  67%|██████▋   | 29/43 [02:17<01:06,  4.73s/it]
Training:  70%|██████▉   | 30/43 [02:22<01:01,  4.73s/it]
Training:  72%|███████▏  | 31/43 [02:27<00:56,  4.73s/it]
Training:  74%|███████▍  | 32/43 [02:31<00:51,  4.72s/it]
Training:  77%|███████▋  | 33/43 [02:36<00:47,  4.72s/it]
Training:  79%|███████▉  | 34/43 [02:41<00:42,  4.72s/it]
Training:  81%|████████▏ | 35/43 [02:46<00:37,  4.71s/it]
Training:  84%|████████▎ | 36/43 [02:50<00:33,  4.72s/it]
Training:  86%|████████▌ | 37/43 [02:55<00:28,  4.72s/it]
Training:  88%|████████▊ | 38/43 [03:00<00:23,  4.73s/it]
Training:  91%|█████████ | 39/43 [03:04<00:18,  4.72s/it]
Training:  93%|█████████▎| 40/43 [03:09<00:14,  4.72s/it]
Training:  95%|█████████▌| 41/43 [03:14<00:09,  4.73s/it]
Training:  98%|█████████▊| 42/43 [03:19<00:04,  4.74s/it]
Training: 100%|██████████| 43/43 [03:20<00:00,  3.73s/it]
Training: 100%|██████████| 43/43 [03:20<00:00,  4.67s/it]
2024-08-26 21:34:05.457 | INFO     | __main__:train:206 - Epoch 22 Average Loss: 2.79246
2024-08-26 21:34:07.066 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL22*****
2024-08-26 21:34:07.066 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:34:07.066 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.42s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.31s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.27s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.27s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.26s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.26s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.23s/it]
Evaluating:  73%|███████▎  | 8/11 [00:10<00:03,  1.22s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.20s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.22s/it]
Evaluating: 100%|██████████| 11/11 [00:13<00:00,  1.03s/it]
Evaluating: 100%|██████████| 11/11 [00:13<00:00,  1.19s/it]
2024-08-26 21:34:20.162 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9167    1.0000    0.9565        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.9565    1.0000    0.9778        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9275    0.9275    0.9275        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.4444    0.1600    0.2353        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.6667    0.6667    0.6667         3
                   distilbert     0.9714    0.9714    0.9714        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.6750    0.8710    0.7606        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet        nan    0.0000       nan         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    1.0000    1.0000         3
                     gpt_neox     0.5000    1.0000    0.6667         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    1.0000    1.0000         4
                        llama     0.5000    0.6667    0.5714         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     0.7500    0.7500    0.7500         4
                       marian     0.8000    0.8000    0.8000         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     0.9444    1.0000    0.9714        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7500    0.8571    0.8000         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    0.8333    0.9091         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.5600    0.8077    0.6614        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6667    0.5000    0.5714         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.9091    0.7143    0.8000        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.7407    0.7143    0.7273        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     0.6667    1.0000    0.8000         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.9333    0.4667    0.6222        30
                        xlnet     0.8462    1.0000    0.9167        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8488       681
                    macro avg     0.8898    0.8383    0.8752       681
                 weighted avg     0.8639    0.8488    0.8468       681

2024-08-26 21:34:20.163 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8488
2024-08-26 21:34:20.167 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:34:20.168 | INFO     | __main__:eval:453 - Validation loss: 3.467908512462269
2024-08-26 21:34:20.178 | INFO     | __main__:train:255 - epochs: 22
2024-08-26 21:34:20.178 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889, 2.659833694613257, 2.7924627481504927]
2024-08-26 21:34:20.178 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927, 3.407679232684049, 3.467908512462269]
2024-08-26 21:34:20.179 | INFO     | __main__:train:157 - ====epoch #23====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:41,  5.29s/it]
Training:   5%|▍         | 2/43 [00:10<03:28,  5.08s/it]
Training:   7%|▋         | 3/43 [00:15<03:19,  5.00s/it]
Training:   9%|▉         | 4/43 [00:20<03:13,  4.96s/it]
Training:  12%|█▏        | 5/43 [00:24<03:08,  4.96s/it]
Training:  14%|█▍        | 6/43 [00:29<03:03,  4.96s/it]
Training:  16%|█▋        | 7/43 [00:34<02:58,  4.95s/it]
Training:  19%|█▊        | 8/43 [00:39<02:51,  4.91s/it]
Training:  21%|██        | 9/43 [00:44<02:46,  4.88s/it]
Training:  23%|██▎       | 10/43 [00:49<02:40,  4.87s/it]
Training:  26%|██▌       | 11/43 [00:54<02:35,  4.86s/it]
Training:  28%|██▊       | 12/43 [00:59<02:30,  4.86s/it]
Training:  30%|███       | 13/43 [01:03<02:25,  4.84s/it]
Training:  33%|███▎      | 14/43 [01:08<02:20,  4.84s/it]
Training:  35%|███▍      | 15/43 [01:13<02:15,  4.85s/it]
Training:  37%|███▋      | 16/43 [01:18<02:10,  4.84s/it]
Training:  40%|███▉      | 17/43 [01:23<02:05,  4.84s/it]
Training:  42%|████▏     | 18/43 [01:28<02:01,  4.85s/it]
Training:  44%|████▍     | 19/43 [01:32<01:56,  4.86s/it]
Training:  47%|████▋     | 20/43 [01:37<01:51,  4.85s/it]
Training:  49%|████▉     | 21/43 [01:42<01:46,  4.85s/it]
Training:  51%|█████     | 22/43 [01:47<01:42,  4.86s/it]
Training:  53%|█████▎    | 23/43 [01:52<01:37,  4.86s/it]
Training:  56%|█████▌    | 24/43 [01:57<01:32,  4.85s/it]
Training:  58%|█████▊    | 25/43 [02:02<01:27,  4.84s/it]
Training:  60%|██████    | 26/43 [02:06<01:22,  4.85s/it]
Training:  63%|██████▎   | 27/43 [02:11<01:17,  4.85s/it]
Training:  65%|██████▌   | 28/43 [02:16<01:12,  4.85s/it]
Training:  67%|██████▋   | 29/43 [02:21<01:07,  4.86s/it]
Training:  70%|██████▉   | 30/43 [02:26<01:04,  4.93s/it]
Training:  72%|███████▏  | 31/43 [02:31<00:58,  4.90s/it]
Training:  74%|███████▍  | 32/43 [02:36<00:53,  4.89s/it]
Training:  77%|███████▋  | 33/43 [02:41<00:48,  4.89s/it]
Training:  79%|███████▉  | 34/43 [02:46<00:43,  4.87s/it]
Training:  81%|████████▏ | 35/43 [02:50<00:38,  4.85s/it]
Training:  84%|████████▎ | 36/43 [02:55<00:33,  4.84s/it]
Training:  86%|████████▌ | 37/43 [03:00<00:29,  4.84s/it]
Training:  88%|████████▊ | 38/43 [03:05<00:24,  4.84s/it]
Training:  91%|█████████ | 39/43 [03:10<00:19,  4.82s/it]
Training:  93%|█████████▎| 40/43 [03:14<00:14,  4.82s/it]
Training:  95%|█████████▌| 41/43 [03:19<00:09,  4.82s/it]
Training:  98%|█████████▊| 42/43 [03:24<00:04,  4.81s/it]
Training: 100%|██████████| 43/43 [03:25<00:00,  3.79s/it]
Training: 100%|██████████| 43/43 [03:25<00:00,  4.79s/it]
2024-08-26 21:37:46.133 | INFO     | __main__:train:206 - Epoch 23 Average Loss: 2.70040
2024-08-26 21:37:47.734 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL23*****
2024-08-26 21:37:47.734 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:37:47.735 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.43s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.32s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.27s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.26s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.24s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.24s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.21s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.22s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.20s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.19s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.00s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.17s/it]
2024-08-26 21:38:00.696 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9706    1.0000    0.9851        33
audio-spectrogram-transformer     0.7143    1.0000    0.8333         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9412    0.9275    0.9343        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.6000    0.6000    0.6000         5
                    camembert     0.4000    0.4800    0.4364        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl        nan    0.0000       nan         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.5000    0.6667    0.5714         3
                   distilbert     0.8947    0.9714    0.9315        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.6667    0.9032    0.7671        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     0.7500    1.0000    0.8571         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    1.0000    1.0000         4
                        llama     0.5000    0.3333    0.4000         3
                   longformer     1.0000    0.9130    0.9545        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     0.8000    1.0000    0.8889         4
                       marian     1.0000    1.0000    1.0000         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     0.9444    1.0000    0.9714        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.8571    0.8571    0.8571         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    1.0000    1.0000         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.6667    0.6923    0.6792        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6667    1.0000    0.8000         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.8750    0.5000    0.6364        14
                    unispeech     1.0000    0.5000    0.6667         4
                unispeech-sat     0.3333    0.5000    0.4000         2
     vision-text-dual-encoder     0.0000    0.0000       nan         1
                          vit     0.7500    1.0000    0.8571         3
                     wav2vec2     0.7500    0.6429    0.6923        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.9333    0.4667    0.6222        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     0.7500    1.0000    0.8571         3

                     accuracy                         0.8517       681
                    macro avg     0.8652    0.8508    0.8773       681
                 weighted avg     0.8701    0.8517    0.8550       681

2024-08-26 21:38:00.696 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8517
2024-08-26 21:38:00.699 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:38:00.699 | INFO     | __main__:eval:453 - Validation loss: 3.479128837585449
2024-08-26 21:38:00.708 | INFO     | __main__:train:255 - epochs: 23
2024-08-26 21:38:00.709 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889, 2.659833694613257, 2.7924627481504927, 2.7004038234089696]
2024-08-26 21:38:00.709 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927, 3.407679232684049, 3.467908512462269, 3.479128837585449]
2024-08-26 21:38:00.710 | INFO     | __main__:train:157 - ====epoch #24====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:33,  5.09s/it]
Training:   5%|▍         | 2/43 [00:10<03:25,  5.02s/it]
Training:   7%|▋         | 3/43 [00:15<03:19,  4.99s/it]
Training:   9%|▉         | 4/43 [00:19<03:13,  4.97s/it]
Training:  12%|█▏        | 5/43 [00:24<03:08,  4.96s/it]
Training:  14%|█▍        | 6/43 [00:29<03:03,  4.95s/it]
Training:  16%|█▋        | 7/43 [00:34<02:57,  4.93s/it]
Training:  19%|█▊        | 8/43 [00:39<02:51,  4.91s/it]
Training:  21%|██        | 9/43 [00:44<02:46,  4.90s/it]
Training:  23%|██▎       | 10/43 [00:49<02:41,  4.89s/it]
Training:  26%|██▌       | 11/43 [00:54<02:35,  4.85s/it]
Training:  28%|██▊       | 12/43 [00:58<02:29,  4.82s/it]
Training:  30%|███       | 13/43 [01:03<02:24,  4.81s/it]
Training:  33%|███▎      | 14/43 [01:08<02:21,  4.88s/it]
Training:  35%|███▍      | 15/43 [01:13<02:15,  4.83s/it]
Training:  37%|███▋      | 16/43 [01:18<02:10,  4.82s/it]
Training:  40%|███▉      | 17/43 [01:22<02:04,  4.77s/it]
Training:  42%|████▏     | 18/43 [01:27<01:59,  4.77s/it]
Training:  44%|████▍     | 19/43 [01:32<01:54,  4.76s/it]
Training:  47%|████▋     | 20/43 [01:37<01:49,  4.77s/it]
Training:  49%|████▉     | 21/43 [01:41<01:44,  4.75s/it]
Training:  51%|█████     | 22/43 [01:46<01:39,  4.76s/it]
Training:  53%|█████▎    | 23/43 [01:51<01:35,  4.76s/it]
Training:  56%|█████▌    | 24/43 [01:56<01:30,  4.75s/it]
Training:  58%|█████▊    | 25/43 [02:00<01:25,  4.74s/it]
Training:  60%|██████    | 26/43 [02:05<01:20,  4.73s/it]
Training:  63%|██████▎   | 27/43 [02:10<01:15,  4.73s/it]
Training:  65%|██████▌   | 28/43 [02:14<01:10,  4.73s/it]
Training:  67%|██████▋   | 29/43 [02:19<01:06,  4.73s/it]
Training:  70%|██████▉   | 30/43 [02:24<01:01,  4.74s/it]
Training:  72%|███████▏  | 31/43 [02:29<00:56,  4.73s/it]
Training:  74%|███████▍  | 32/43 [02:33<00:52,  4.73s/it]
Training:  77%|███████▋  | 33/43 [02:38<00:47,  4.73s/it]
Training:  79%|███████▉  | 34/43 [02:43<00:42,  4.73s/it]
Training:  81%|████████▏ | 35/43 [02:48<00:37,  4.74s/it]
Training:  84%|████████▎ | 36/43 [02:52<00:33,  4.74s/it]
Training:  86%|████████▌ | 37/43 [02:57<00:28,  4.74s/it]
Training:  88%|████████▊ | 38/43 [03:02<00:23,  4.73s/it]
Training:  91%|█████████ | 39/43 [03:07<00:18,  4.73s/it]
Training:  93%|█████████▎| 40/43 [03:11<00:14,  4.73s/it]
Training:  95%|█████████▌| 41/43 [03:16<00:09,  4.76s/it]
Training:  98%|█████████▊| 42/43 [03:21<00:04,  4.75s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  3.74s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  4.72s/it]
2024-08-26 21:41:23.471 | INFO     | __main__:train:206 - Epoch 24 Average Loss: 2.88235
2024-08-26 21:41:25.082 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL24*****
2024-08-26 21:41:25.083 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:41:25.083 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.40s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.30s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.27s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.23s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.20s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.19s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.20s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.19s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.17s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.20s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.02s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.16s/it]
2024-08-26 21:41:37.857 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.9565    1.0000    0.9778        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9412    0.9275    0.9343        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.6000    0.6000    0.6000         5
                    camembert     0.5455    0.2400    0.3333        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     0.6667    1.0000    0.8000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.5000    1.0000    0.6667         3
                   distilbert     0.9714    0.9714    0.9714        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.6078    1.0000    0.7561        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.4000    1.0000    0.5714         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     1.0000    1.0000    1.0000         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    0.6667    0.8000         3
                     gpt_neox     0.3333    1.0000    0.5000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     0.5000    1.0000    0.6667         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     0.7143    1.0000    0.8333         5
                         lilt     1.0000    0.2500    0.4000         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    0.9130    0.9545        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     1.0000    0.7500    0.8571         4
                       marian     1.0000    0.8000    0.8889         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     0.8947    1.0000    0.9444        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.6000    0.8571    0.7059         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    1.0000    1.0000         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.5938    0.7308    0.6552        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6000    0.7500    0.6667         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.9000    0.6429    0.7500        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6190    0.9286    0.7429        28
                        wavlm     1.0000    0.0833    0.1538        12
                      whisper     1.0000    1.0000    1.0000         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.9333    0.4667    0.6222        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8429       681
                    macro avg     0.8830    0.8351    0.8495       681
                 weighted avg     0.8742    0.8429    0.8373       681

2024-08-26 21:41:37.857 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8429
2024-08-26 21:41:37.862 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:41:37.862 | INFO     | __main__:eval:453 - Validation loss: 3.4826982671564277
2024-08-26 21:41:37.872 | INFO     | __main__:train:255 - epochs: 24
2024-08-26 21:41:37.872 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889, 2.659833694613257, 2.7924627481504927, 2.7004038234089696, 2.8823549858359403]
2024-08-26 21:41:37.872 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927, 3.407679232684049, 3.467908512462269, 3.479128837585449, 3.4826982671564277]
2024-08-26 21:41:37.872 | INFO     | __main__:train:157 - ====epoch #25====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:33,  5.09s/it]
Training:   5%|▍         | 2/43 [00:10<03:24,  4.99s/it]
Training:   7%|▋         | 3/43 [00:14<03:19,  4.98s/it]
Training:   9%|▉         | 4/43 [00:19<03:13,  4.97s/it]
Training:  12%|█▏        | 5/43 [00:24<03:09,  4.98s/it]
Training:  14%|█▍        | 6/43 [00:29<03:04,  4.98s/it]
Training:  16%|█▋        | 7/43 [00:34<02:58,  4.95s/it]
Training:  19%|█▊        | 8/43 [00:39<02:51,  4.90s/it]
Training:  21%|██        | 9/43 [00:44<02:47,  4.94s/it]
Training:  23%|██▎       | 10/43 [00:49<02:44,  4.98s/it]
Training:  26%|██▌       | 11/43 [00:54<02:40,  5.01s/it]
Training:  28%|██▊       | 12/43 [00:59<02:33,  4.96s/it]
Training:  30%|███       | 13/43 [01:04<02:27,  4.91s/it]
Training:  33%|███▎      | 14/43 [01:09<02:21,  4.86s/it]
Training:  35%|███▍      | 15/43 [01:13<02:15,  4.83s/it]
Training:  37%|███▋      | 16/43 [01:18<02:12,  4.89s/it]
Training:  40%|███▉      | 17/43 [01:23<02:07,  4.92s/it]
Training:  42%|████▏     | 18/43 [01:28<02:03,  4.96s/it]
Training:  44%|████▍     | 19/43 [01:33<01:59,  4.96s/it]
Training:  47%|████▋     | 20/43 [01:38<01:53,  4.92s/it]
Training:  49%|████▉     | 21/43 [01:43<01:47,  4.88s/it]
Training:  51%|█████     | 22/43 [01:48<01:41,  4.85s/it]
Training:  53%|█████▎    | 23/43 [01:53<01:36,  4.81s/it]
Training:  56%|█████▌    | 24/43 [01:58<01:32,  4.88s/it]
Training:  58%|█████▊    | 25/43 [02:03<01:28,  4.92s/it]
Training:  60%|██████    | 26/43 [02:08<01:25,  5.02s/it]
Training:  63%|██████▎   | 27/43 [02:13<01:19,  4.98s/it]
Training:  65%|██████▌   | 28/43 [02:18<01:13,  4.92s/it]
Training:  67%|██████▋   | 29/43 [02:22<01:08,  4.88s/it]
Training:  70%|██████▉   | 30/43 [02:27<01:02,  4.83s/it]
Training:  72%|███████▏  | 31/43 [02:32<00:57,  4.80s/it]
Training:  74%|███████▍  | 32/43 [02:37<00:53,  4.87s/it]
Training:  77%|███████▋  | 33/43 [02:42<00:49,  4.93s/it]
Training:  79%|███████▉  | 34/43 [02:47<00:44,  4.96s/it]
Training:  81%|████████▏ | 35/43 [02:52<00:39,  4.93s/it]
Training:  84%|████████▎ | 36/43 [02:57<00:34,  4.95s/it]
Training:  86%|████████▌ | 37/43 [03:02<00:29,  4.94s/it]
Training:  88%|████████▊ | 38/43 [03:06<00:24,  4.90s/it]
Training:  91%|█████████ | 39/43 [03:11<00:19,  4.87s/it]
Training:  93%|█████████▎| 40/43 [03:16<00:14,  4.82s/it]
Training:  95%|█████████▌| 41/43 [03:21<00:09,  4.81s/it]
Training:  98%|█████████▊| 42/43 [03:26<00:04,  4.88s/it]
Training: 100%|██████████| 43/43 [03:27<00:00,  3.86s/it]
Training: 100%|██████████| 43/43 [03:27<00:00,  4.83s/it]
2024-08-26 21:45:05.707 | INFO     | __main__:train:206 - Epoch 25 Average Loss: 2.69326
2024-08-26 21:45:07.312 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL25*****
2024-08-26 21:45:07.312 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:45:07.312 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.43s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.32s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.28s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.24s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.21s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.23s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.23s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.24s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.23s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.22s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.02s/it]
Evaluating: 100%|██████████| 11/11 [00:13<00:00,  1.18s/it]
2024-08-26 21:45:20.339 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     0.9706    1.0000    0.9851        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     0.8800    1.0000    0.9362        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9697    0.9275    0.9481        69
                     big_bird     0.9545    0.9545    0.9545        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.4444    0.4800    0.4615        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     0.6667    1.0000    0.8000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.6667    0.6667    0.6667         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.6667    0.9677    0.7895        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     1.0000    1.0000    1.0000         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    0.6667    0.8000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.8333    0.9091         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    1.0000    1.0000         4
                        llama     0.5000    0.3333    0.4000         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     0.8000    1.0000    0.8889         4
                       marian     0.6667    0.4000    0.5000         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.8571    0.8571    0.8571         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    0.8333    0.9091         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.6600    0.6346    0.6471        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6667    1.0000    0.8000         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.8571    0.8571    0.8571        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.7407    0.7143    0.7273        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.7000    0.4667    0.5600        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8576       681
                    macro avg     0.8928    0.8555    0.8811       681
                 weighted avg     0.8693    0.8576    0.8588       681

2024-08-26 21:45:20.339 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8576
2024-08-26 21:45:20.355 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:45:20.355 | INFO     | __main__:eval:453 - Validation loss: 3.378306659785184
2024-08-26 21:45:20.362 | INFO     | __main__:train:255 - epochs: 25
2024-08-26 21:45:20.363 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889, 2.659833694613257, 2.7924627481504927, 2.7004038234089696, 2.8823549858359403, 2.6932603120803833]
2024-08-26 21:45:20.363 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927, 3.407679232684049, 3.467908512462269, 3.479128837585449, 3.4826982671564277, 3.378306659785184]
2024-08-26 21:45:20.363 | INFO     | __main__:train:157 - ====epoch #26====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:28,  4.96s/it]
Training:   5%|▍         | 2/43 [00:09<03:18,  4.85s/it]
Training:   7%|▋         | 3/43 [00:14<03:17,  4.93s/it]
Training:   9%|▉         | 4/43 [00:19<03:12,  4.95s/it]
Training:  12%|█▏        | 5/43 [00:24<03:08,  4.95s/it]
Training:  14%|█▍        | 6/43 [00:29<03:03,  4.95s/it]
Training:  16%|█▋        | 7/43 [00:34<02:57,  4.94s/it]
Training:  19%|█▊        | 8/43 [00:39<02:56,  5.05s/it]
Training:  21%|██        | 9/43 [00:44<02:50,  5.02s/it]
Training:  23%|██▎       | 10/43 [00:49<02:44,  4.99s/it]
Training:  26%|██▌       | 11/43 [00:54<02:38,  4.95s/it]
Training:  28%|██▊       | 12/43 [00:59<02:33,  4.96s/it]
Training:  30%|███       | 13/43 [01:04<02:29,  4.97s/it]
Training:  33%|███▎      | 14/43 [01:09<02:24,  4.98s/it]
Training:  35%|███▍      | 15/43 [01:14<02:19,  4.96s/it]
Training:  37%|███▋      | 16/43 [01:19<02:13,  4.95s/it]
Training:  40%|███▉      | 17/43 [01:24<02:07,  4.92s/it]
Training:  42%|████▏     | 18/43 [01:29<02:02,  4.91s/it]
Training:  44%|████▍     | 19/43 [01:33<01:57,  4.89s/it]
Training:  47%|████▋     | 20/43 [01:38<01:52,  4.89s/it]
Training:  49%|████▉     | 21/43 [01:43<01:47,  4.89s/it]
Training:  51%|█████     | 22/43 [01:48<01:43,  4.93s/it]
Training:  53%|█████▎    | 23/43 [01:53<01:38,  4.91s/it]
Training:  56%|█████▌    | 24/43 [01:58<01:33,  4.94s/it]
Training:  58%|█████▊    | 25/43 [02:03<01:28,  4.93s/it]
Training:  60%|██████    | 26/43 [02:08<01:23,  4.94s/it]
Training:  63%|██████▎   | 27/43 [02:13<01:19,  4.95s/it]
Training:  65%|██████▌   | 28/43 [02:18<01:13,  4.93s/it]
Training:  67%|██████▋   | 29/43 [02:23<01:08,  4.92s/it]
Training:  70%|██████▉   | 30/43 [02:28<01:03,  4.91s/it]
Training:  72%|███████▏  | 31/43 [02:33<00:58,  4.91s/it]
Training:  74%|███████▍  | 32/43 [02:38<00:54,  4.93s/it]
Training:  77%|███████▋  | 33/43 [02:43<00:49,  4.94s/it]
Training:  79%|███████▉  | 34/43 [02:47<00:44,  4.95s/it]
Training:  81%|████████▏ | 35/43 [02:52<00:39,  4.95s/it]
Training:  84%|████████▎ | 36/43 [02:57<00:34,  4.96s/it]
Training:  86%|████████▌ | 37/43 [03:03<00:30,  5.02s/it]
Training:  88%|████████▊ | 38/43 [03:08<00:25,  5.03s/it]
Training:  91%|█████████ | 39/43 [03:13<00:20,  5.01s/it]
Training:  93%|█████████▎| 40/43 [03:18<00:15,  5.02s/it]
Training:  95%|█████████▌| 41/43 [03:23<00:10,  5.02s/it]
Training:  98%|█████████▊| 42/43 [03:28<00:05,  5.01s/it]
Training: 100%|██████████| 43/43 [03:29<00:00,  3.97s/it]
Training: 100%|██████████| 43/43 [03:29<00:00,  4.88s/it]
2024-08-26 21:48:50.093 | INFO     | __main__:train:206 - Epoch 26 Average Loss: 2.64754
2024-08-26 21:48:51.699 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL26*****
2024-08-26 21:48:51.700 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:48:51.700 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:14,  1.44s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.32s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.28s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.27s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.26s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.25s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.22s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.20s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.18s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.21s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.01s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.18s/it]
2024-08-26 21:49:04.701 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     0.8333    1.0000    0.9091         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9846    0.9275    0.9552        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.4400    0.4400    0.4400        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     1.0000    1.0000    1.0000         4
                      codegen     0.8000    1.0000    0.8889         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.5000    0.6667    0.5714         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.7812    0.8065    0.7937        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.4000    1.0000    0.5714         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    0.6667    0.8000         3
                     gpt_neox     0.5000    1.0000    0.6667         2
                         gptj     1.0000    0.8333    0.9091         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    1.0000    1.0000         4
                        llama     0.6667    0.6667    0.6667         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     0.7500    1.0000    0.8571         3
                      m2m_100     1.0000    0.5000    0.6667         4
                       marian     0.8333    1.0000    0.9091         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     0.8571    1.0000    0.9231         6
                          mt5     1.0000    0.6667    0.8000         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.5833    1.0000    0.7368         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     1.0000    0.8333    0.9091         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.6182    0.6538    0.6355        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6000    0.7500    0.6667         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.8182    0.6429    0.7200        14
                    unispeech     0.6000    0.7500    0.6667         4
                unispeech-sat        nan    0.0000       nan         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.7692    0.7143    0.7407        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.6250    0.6667    0.6452        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     0.7500    1.0000    0.8571         3

                     accuracy                         0.8561       681
                    macro avg     0.8817    0.8411    0.8810       681
                 weighted avg     0.8721    0.8561    0.8635       681

2024-08-26 21:49:04.701 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8561
2024-08-26 21:49:04.705 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:49:04.705 | INFO     | __main__:eval:453 - Validation loss: 3.364569674838673
2024-08-26 21:49:04.713 | INFO     | __main__:train:255 - epochs: 26
2024-08-26 21:49:04.714 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889, 2.659833694613257, 2.7924627481504927, 2.7004038234089696, 2.8823549858359403, 2.6932603120803833, 2.6475369569867158]
2024-08-26 21:49:04.715 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927, 3.407679232684049, 3.467908512462269, 3.479128837585449, 3.4826982671564277, 3.378306659785184, 3.364569674838673]
2024-08-26 21:49:04.715 | INFO     | __main__:train:157 - ====epoch #27====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:28,  4.96s/it]
Training:   5%|▍         | 2/43 [00:09<03:23,  4.95s/it]
Training:   7%|▋         | 3/43 [00:14<03:17,  4.94s/it]
Training:   9%|▉         | 4/43 [00:19<03:12,  4.93s/it]
Training:  12%|█▏        | 5/43 [00:24<03:05,  4.87s/it]
Training:  14%|█▍        | 6/43 [00:29<02:59,  4.85s/it]
Training:  16%|█▋        | 7/43 [00:34<02:56,  4.91s/it]
Training:  19%|█▊        | 8/43 [00:39<02:51,  4.90s/it]
Training:  21%|██        | 9/43 [00:44<02:45,  4.88s/it]
Training:  23%|██▎       | 10/43 [00:48<02:40,  4.87s/it]
Training:  26%|██▌       | 11/43 [00:53<02:34,  4.84s/it]
Training:  28%|██▊       | 12/43 [00:58<02:30,  4.84s/it]
Training:  30%|███       | 13/43 [01:03<02:24,  4.82s/it]
Training:  33%|███▎      | 14/43 [01:08<02:19,  4.81s/it]
Training:  35%|███▍      | 15/43 [01:12<02:14,  4.81s/it]
Training:  37%|███▋      | 16/43 [01:17<02:09,  4.80s/it]
Training:  40%|███▉      | 17/43 [01:22<02:06,  4.85s/it]
Training:  42%|████▏     | 18/43 [01:27<02:01,  4.87s/it]
Training:  44%|████▍     | 19/43 [01:32<01:56,  4.86s/it]
Training:  47%|████▋     | 20/43 [01:37<01:51,  4.83s/it]
Training:  49%|████▉     | 21/43 [01:41<01:46,  4.82s/it]
Training:  51%|█████     | 22/43 [01:47<01:43,  4.92s/it]
Training:  53%|█████▎    | 23/43 [01:52<01:38,  4.93s/it]
Training:  56%|█████▌    | 24/43 [01:56<01:33,  4.92s/it]
Training:  58%|█████▊    | 25/43 [02:01<01:28,  4.90s/it]
Training:  60%|██████    | 26/43 [02:06<01:22,  4.87s/it]
Training:  63%|██████▎   | 27/43 [02:11<01:17,  4.84s/it]
Training:  65%|██████▌   | 28/43 [02:16<01:12,  4.82s/it]
Training:  67%|██████▋   | 29/43 [02:21<01:07,  4.85s/it]
Training:  70%|██████▉   | 30/43 [02:25<01:03,  4.86s/it]
Training:  72%|███████▏  | 31/43 [02:30<00:57,  4.82s/it]
Training:  74%|███████▍  | 32/43 [02:35<00:52,  4.79s/it]
Training:  77%|███████▋  | 33/43 [02:40<00:48,  4.86s/it]
Training:  79%|███████▉  | 34/43 [02:45<00:43,  4.87s/it]
Training:  81%|████████▏ | 35/43 [02:50<00:38,  4.84s/it]
Training:  84%|████████▎ | 36/43 [02:54<00:33,  4.82s/it]
Training:  86%|████████▌ | 37/43 [02:59<00:28,  4.78s/it]
Training:  88%|████████▊ | 38/43 [03:04<00:23,  4.75s/it]
Training:  91%|█████████ | 39/43 [03:08<00:18,  4.73s/it]
Training:  93%|█████████▎| 40/43 [03:13<00:14,  4.73s/it]
Training:  95%|█████████▌| 41/43 [03:18<00:09,  4.75s/it]
Training:  98%|█████████▊| 42/43 [03:23<00:04,  4.75s/it]
Training: 100%|██████████| 43/43 [03:24<00:00,  3.74s/it]
Training: 100%|██████████| 43/43 [03:24<00:00,  4.76s/it]
2024-08-26 21:52:29.314 | INFO     | __main__:train:206 - Epoch 27 Average Loss: 2.70613
2024-08-26 21:52:30.917 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL27*****
2024-08-26 21:52:30.917 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:52:30.917 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.32s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.28s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.26s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.26s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.25s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.22s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.20s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.22s/it]
Evaluating:  82%|████████▏ | 9/11 [00:11<00:02,  1.22s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.23s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.03s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.18s/it]
2024-08-26 21:52:43.883 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     0.8333    1.0000    0.9091         5
                         bart     0.9565    1.0000    0.9778        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9143    0.9275    0.9209        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.5000    0.3600    0.4186        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     1.0000    1.0000    1.0000         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.6000    1.0000    0.7500         3
                   distilbert     0.9714    0.9714    0.9714        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.8929    0.8065    0.8475        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    1.0000    1.0000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.8333    0.9091         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    1.0000    1.0000         4
                        llama     0.5000    0.3333    0.4000         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     0.7500    1.0000    0.8571         3
                      m2m_100     0.6667    0.5000    0.5714         4
                       marian     0.6667    0.8000    0.7273         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     0.6667    1.0000    0.8000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.8333    0.9091         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7000    1.0000    0.8235         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     1.0000    0.6667    0.8000         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.5513    0.8269    0.6615        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6000    0.7500    0.6667         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.9167    0.7857    0.8462        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder     0.0000    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.7692    0.7143    0.7407        28
                        wavlm     0.5556    0.8333    0.6667        12
                      whisper     1.0000    1.0000    1.0000         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.8750    0.4667    0.6087        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8590       681
                    macro avg     0.8727    0.8434    0.8752       681
                 weighted avg     0.8789    0.8590    0.8623       681

2024-08-26 21:52:43.883 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.859
2024-08-26 21:52:43.891 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:52:43.891 | INFO     | __main__:eval:453 - Validation loss: 3.421489802273837
2024-08-26 21:52:43.899 | INFO     | __main__:train:255 - epochs: 27
2024-08-26 21:52:43.899 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889, 2.659833694613257, 2.7924627481504927, 2.7004038234089696, 2.8823549858359403, 2.6932603120803833, 2.6475369569867158, 2.7061318186826484]
2024-08-26 21:52:43.900 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927, 3.407679232684049, 3.467908512462269, 3.479128837585449, 3.4826982671564277, 3.378306659785184, 3.364569674838673, 3.421489802273837]
2024-08-26 21:52:43.900 | INFO     | __main__:train:157 - ====epoch #28====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:32,  5.07s/it]
Training:   5%|▍         | 2/43 [00:10<03:25,  5.02s/it]
Training:   7%|▋         | 3/43 [00:14<03:18,  4.97s/it]
Training:   9%|▉         | 4/43 [00:19<03:14,  4.98s/it]
Training:  12%|█▏        | 5/43 [00:24<03:08,  4.97s/it]
Training:  14%|█▍        | 6/43 [00:29<03:03,  4.95s/it]
Training:  16%|█▋        | 7/43 [00:34<02:58,  4.95s/it]
Training:  19%|█▊        | 8/43 [00:39<02:54,  4.97s/it]
Training:  21%|██        | 9/43 [00:44<02:49,  4.97s/it]
Training:  23%|██▎       | 10/43 [00:50<02:47,  5.07s/it]
Training:  26%|██▌       | 11/43 [00:55<02:41,  5.05s/it]
Training:  28%|██▊       | 12/43 [01:00<02:36,  5.04s/it]
Training:  30%|███       | 13/43 [01:05<02:30,  5.03s/it]
Training:  33%|███▎      | 14/43 [01:10<02:25,  5.02s/it]
Training:  35%|███▍      | 15/43 [01:15<02:20,  5.02s/it]
Training:  37%|███▋      | 16/43 [01:20<02:15,  5.00s/it]
Training:  40%|███▉      | 17/43 [01:25<02:09,  4.98s/it]
Training:  42%|████▏     | 18/43 [01:30<02:04,  4.99s/it]
Training:  44%|████▍     | 19/43 [01:35<02:00,  5.01s/it]
Training:  47%|████▋     | 20/43 [01:40<01:55,  5.02s/it]
Training:  49%|████▉     | 21/43 [01:45<01:49,  4.98s/it]
Training:  51%|█████     | 22/43 [01:49<01:42,  4.89s/it]
Training:  53%|█████▎    | 23/43 [01:54<01:36,  4.84s/it]
Training:  56%|█████▌    | 24/43 [01:59<01:33,  4.90s/it]
Training:  58%|█████▊    | 25/43 [02:04<01:28,  4.94s/it]
Training:  60%|██████    | 26/43 [02:09<01:24,  4.98s/it]
Training:  63%|██████▎   | 27/43 [02:14<01:20,  5.01s/it]
Training:  65%|██████▌   | 28/43 [02:19<01:15,  5.02s/it]
Training:  67%|██████▋   | 29/43 [02:24<01:10,  5.04s/it]
Training:  70%|██████▉   | 30/43 [02:29<01:05,  5.05s/it]
Training:  72%|███████▏  | 31/43 [02:34<01:00,  5.06s/it]
Training:  74%|███████▍  | 32/43 [02:39<00:55,  5.06s/it]
Training:  77%|███████▋  | 33/43 [02:44<00:50,  5.04s/it]
Training:  79%|███████▉  | 34/43 [02:49<00:45,  5.02s/it]
Training:  81%|████████▏ | 35/43 [02:54<00:40,  5.01s/it]
Training:  84%|████████▎ | 36/43 [03:00<00:35,  5.08s/it]
Training:  86%|████████▌ | 37/43 [03:05<00:30,  5.09s/it]
Training:  88%|████████▊ | 38/43 [03:10<00:25,  5.06s/it]
Training:  91%|█████████ | 39/43 [03:15<00:20,  5.04s/it]
Training:  93%|█████████▎| 40/43 [03:20<00:14,  4.98s/it]
Training:  95%|█████████▌| 41/43 [03:24<00:09,  4.92s/it]
Training:  98%|█████████▊| 42/43 [03:29<00:04,  4.87s/it]
Training: 100%|██████████| 43/43 [03:31<00:00,  3.82s/it]
Training: 100%|██████████| 43/43 [03:31<00:00,  4.91s/it]
2024-08-26 21:56:14.981 | INFO     | __main__:train:206 - Epoch 28 Average Loss: 2.74151
2024-08-26 21:56:16.593 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL28*****
2024-08-26 21:56:16.593 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:56:16.593 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.32s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:10,  1.22s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.18s/it]
Evaluating:  36%|███▋      | 4/11 [00:04<00:08,  1.17s/it]
Evaluating:  45%|████▌     | 5/11 [00:05<00:06,  1.16s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:05,  1.16s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.17s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.17s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.19s/it]
Evaluating:  91%|█████████ | 10/11 [00:11<00:01,  1.19s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.00it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.13s/it]
2024-08-26 21:56:29.047 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.9412    0.9275    0.9343        69
                     big_bird     0.9545    0.9545    0.9545        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.4444    0.3200    0.3721        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     1.0000    1.0000    1.0000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.7500    1.0000    0.8571         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.8621    0.8065    0.8333        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    1.0000    1.0000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    1.0000    1.0000         4
                        llama     0.5000    0.6667    0.5714         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     1.0000    1.0000    1.0000         4
                       marian     0.8333    1.0000    0.9091         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.6667    0.8000         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7778    1.0000    0.8750         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    0.8333    0.9091         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.5658    0.8269    0.6719        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6667    1.0000    0.8000         4
                     speecht5     1.0000    0.5000    0.6667         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.8571    0.8571    0.8571        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6279    0.9643    0.7606        28
                        wavlm     1.0000    0.0833    0.1538        12
                      whisper     1.0000    1.0000    1.0000         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.7778    0.4667    0.5833        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8649       681
                    macro avg     0.9099    0.8733    0.8848       681
                 weighted avg     0.8862    0.8649    0.8600       681

2024-08-26 21:56:29.047 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8649
2024-08-26 21:56:29.053 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 21:56:29.053 | INFO     | __main__:eval:453 - Validation loss: 3.3691718036478218
2024-08-26 21:56:29.061 | INFO     | __main__:train:255 - epochs: 28
2024-08-26 21:56:29.061 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889, 2.659833694613257, 2.7924627481504927, 2.7004038234089696, 2.8823549858359403, 2.6932603120803833, 2.6475369569867158, 2.7061318186826484, 2.7415077159571095]
2024-08-26 21:56:29.062 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927, 3.407679232684049, 3.467908512462269, 3.479128837585449, 3.4826982671564277, 3.378306659785184, 3.364569674838673, 3.421489802273837, 3.3691718036478218]
2024-08-26 21:56:29.062 | INFO     | __main__:train:157 - ====epoch #29====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:05<03:33,  5.09s/it]
Training:   5%|▍         | 2/43 [00:09<03:23,  4.96s/it]
Training:   7%|▋         | 3/43 [00:14<03:16,  4.90s/it]
Training:   9%|▉         | 4/43 [00:19<03:10,  4.87s/it]
Training:  12%|█▏        | 5/43 [00:24<03:04,  4.86s/it]
Training:  14%|█▍        | 6/43 [00:29<02:59,  4.84s/it]
Training:  16%|█▋        | 7/43 [00:34<02:53,  4.82s/it]
Training:  19%|█▊        | 8/43 [00:38<02:47,  4.79s/it]
Training:  21%|██        | 9/43 [00:43<02:42,  4.78s/it]
Training:  23%|██▎       | 10/43 [00:48<02:37,  4.78s/it]
Training:  26%|██▌       | 11/43 [00:53<02:33,  4.78s/it]
Training:  28%|██▊       | 12/43 [00:57<02:28,  4.78s/it]
Training:  30%|███       | 13/43 [01:02<02:23,  4.78s/it]
Training:  33%|███▎      | 14/43 [01:07<02:20,  4.85s/it]
Training:  35%|███▍      | 15/43 [01:12<02:16,  4.89s/it]
Training:  37%|███▋      | 16/43 [01:17<02:11,  4.89s/it]
Training:  40%|███▉      | 17/43 [01:22<02:06,  4.88s/it]
Training:  42%|████▏     | 18/43 [01:27<02:01,  4.87s/it]
Training:  44%|████▍     | 19/43 [01:31<01:55,  4.83s/it]
Training:  47%|████▋     | 20/43 [01:36<01:50,  4.80s/it]
Training:  49%|████▉     | 21/43 [01:41<01:47,  4.88s/it]
Training:  51%|█████     | 22/43 [01:46<01:41,  4.84s/it]
Training:  53%|█████▎    | 23/43 [01:51<01:36,  4.83s/it]
Training:  56%|█████▌    | 24/43 [01:56<01:31,  4.81s/it]
Training:  58%|█████▊    | 25/43 [02:00<01:26,  4.80s/it]
Training:  60%|██████    | 26/43 [02:05<01:21,  4.78s/it]
Training:  63%|██████▎   | 27/43 [02:10<01:16,  4.77s/it]
Training:  65%|██████▌   | 28/43 [02:15<01:11,  4.77s/it]
Training:  67%|██████▋   | 29/43 [02:19<01:06,  4.76s/it]
Training:  70%|██████▉   | 30/43 [02:24<01:01,  4.75s/it]
Training:  72%|███████▏  | 31/43 [02:29<00:56,  4.75s/it]
Training:  74%|███████▍  | 32/43 [02:34<00:52,  4.75s/it]
Training:  77%|███████▋  | 33/43 [02:38<00:47,  4.75s/it]
Training:  79%|███████▉  | 34/43 [02:43<00:42,  4.74s/it]
Training:  81%|████████▏ | 35/43 [02:48<00:37,  4.74s/it]
Training:  84%|████████▎ | 36/43 [02:53<00:33,  4.75s/it]
Training:  86%|████████▌ | 37/43 [02:57<00:28,  4.73s/it]
Training:  88%|████████▊ | 38/43 [03:02<00:23,  4.75s/it]
Training:  91%|█████████ | 39/43 [03:07<00:18,  4.74s/it]
Training:  93%|█████████▎| 40/43 [03:12<00:14,  4.74s/it]
Training:  95%|█████████▌| 41/43 [03:16<00:09,  4.75s/it]
Training:  98%|█████████▊| 42/43 [03:21<00:04,  4.75s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  3.74s/it]
Training: 100%|██████████| 43/43 [03:22<00:00,  4.72s/it]
2024-08-26 21:59:52.047 | INFO     | __main__:train:206 - Epoch 29 Average Loss: 2.72639
2024-08-26 21:59:53.647 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL29*****
2024-08-26 21:59:53.647 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 21:59:53.647 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.31s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:10,  1.21s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:09,  1.18s/it]
Evaluating:  36%|███▋      | 4/11 [00:04<00:08,  1.21s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.22s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.22s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.23s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.21s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.19s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.18s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.01it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.15s/it]
2024-08-26 22:00:06.302 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.8667    0.9420    0.9028        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.3684    0.2800    0.3182        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     0.6667    1.0000    0.8000         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     0.9600    0.9231    0.9412        26
                         detr     0.6000    1.0000    0.7500         3
                   distilbert     1.0000    0.9714    0.9855        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.7879    0.8387    0.8125        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     1.0000    1.0000    1.0000         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    0.6667    0.8000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    1.0000    1.0000         4
                        llama     0.2500    0.3333    0.2857         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     1.0000    1.0000    1.0000         3
                      m2m_100     1.0000    1.0000    1.0000         4
                       marian     0.6250    1.0000    0.7692         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.6667    0.8000         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7000    1.0000    0.8235         7
                    pegasus_x     1.0000    1.0000    1.0000         1
                       plbart     1.0000    0.5000    0.6667         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.5918    0.5577    0.5743        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.7500    0.7500    0.7500         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.7857    0.7857    0.7857        14
                    unispeech     1.0000    0.2500    0.4000         4
                unispeech-sat     0.2500    0.5000    0.3333         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6279    0.9643    0.7606        28
                        wavlm     1.0000    0.0833    0.1538        12
                      whisper     0.8889    1.0000    0.9412         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.5758    0.6333    0.6032        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8429       681
                    macro avg     0.8939    0.8467    0.8698       681
                 weighted avg     0.8575    0.8429    0.8383       681

2024-08-26 22:00:06.302 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8429
2024-08-26 22:00:06.306 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 22:00:06.306 | INFO     | __main__:eval:453 - Validation loss: 3.367888255552812
2024-08-26 22:00:06.313 | INFO     | __main__:train:255 - epochs: 29
2024-08-26 22:00:06.314 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889, 2.659833694613257, 2.7924627481504927, 2.7004038234089696, 2.8823549858359403, 2.6932603120803833, 2.6475369569867158, 2.7061318186826484, 2.7415077159571095, 2.726391002189281]
2024-08-26 22:00:06.314 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927, 3.407679232684049, 3.467908512462269, 3.479128837585449, 3.4826982671564277, 3.378306659785184, 3.364569674838673, 3.421489802273837, 3.3691718036478218, 3.367888255552812]
2024-08-26 22:00:06.314 | INFO     | __main__:train:157 - ====epoch #30====

Training:   0%|          | 0/43 [00:00<?, ?it/s]
Training:   2%|▏         | 1/43 [00:04<03:24,  4.88s/it]
Training:   5%|▍         | 2/43 [00:09<03:17,  4.81s/it]
Training:   7%|▋         | 3/43 [00:14<03:11,  4.78s/it]
Training:   9%|▉         | 4/43 [00:19<03:05,  4.77s/it]
Training:  12%|█▏        | 5/43 [00:24<03:03,  4.82s/it]
Training:  14%|█▍        | 6/43 [00:28<02:57,  4.80s/it]
Training:  16%|█▋        | 7/43 [00:33<02:51,  4.77s/it]
Training:  19%|█▊        | 8/43 [00:38<02:46,  4.75s/it]
Training:  21%|██        | 9/43 [00:42<02:41,  4.75s/it]
Training:  23%|██▎       | 10/43 [00:47<02:36,  4.73s/it]
Training:  26%|██▌       | 11/43 [00:52<02:31,  4.72s/it]
Training:  28%|██▊       | 12/43 [00:57<02:26,  4.72s/it]
Training:  30%|███       | 13/43 [01:01<02:21,  4.72s/it]
Training:  33%|███▎      | 14/43 [01:06<02:17,  4.73s/it]
Training:  35%|███▍      | 15/43 [01:11<02:12,  4.73s/it]
Training:  37%|███▋      | 16/43 [01:16<02:07,  4.74s/it]
Training:  40%|███▉      | 17/43 [01:20<02:03,  4.74s/it]
Training:  42%|████▏     | 18/43 [01:25<01:58,  4.73s/it]
Training:  44%|████▍     | 19/43 [01:30<01:53,  4.73s/it]
Training:  47%|████▋     | 20/43 [01:34<01:48,  4.73s/it]
Training:  49%|████▉     | 21/43 [01:39<01:44,  4.74s/it]
Training:  51%|█████     | 22/43 [01:44<01:39,  4.73s/it]
Training:  53%|█████▎    | 23/43 [01:49<01:34,  4.73s/it]
Training:  56%|█████▌    | 24/43 [01:53<01:29,  4.72s/it]
Training:  58%|█████▊    | 25/43 [01:58<01:25,  4.73s/it]
Training:  60%|██████    | 26/43 [02:03<01:20,  4.73s/it]
Training:  63%|██████▎   | 27/43 [02:08<01:15,  4.73s/it]
Training:  65%|██████▌   | 28/43 [02:12<01:10,  4.72s/it]
Training:  67%|██████▋   | 29/43 [02:17<01:06,  4.73s/it]
Training:  70%|██████▉   | 30/43 [02:22<01:01,  4.74s/it]
Training:  72%|███████▏  | 31/43 [02:27<00:57,  4.78s/it]
Training:  74%|███████▍  | 32/43 [02:31<00:52,  4.76s/it]
Training:  77%|███████▋  | 33/43 [02:36<00:47,  4.74s/it]
Training:  79%|███████▉  | 34/43 [02:41<00:42,  4.75s/it]
Training:  81%|████████▏ | 35/43 [02:46<00:37,  4.74s/it]
Training:  84%|████████▎ | 36/43 [02:50<00:33,  4.74s/it]
Training:  86%|████████▌ | 37/43 [02:55<00:28,  4.72s/it]
Training:  88%|████████▊ | 38/43 [03:00<00:23,  4.73s/it]
Training:  91%|█████████ | 39/43 [03:05<00:18,  4.74s/it]
Training:  93%|█████████▎| 40/43 [03:09<00:14,  4.74s/it]
Training:  95%|█████████▌| 41/43 [03:14<00:09,  4.74s/it]
Training:  98%|█████████▊| 42/43 [03:19<00:04,  4.74s/it]
Training: 100%|██████████| 43/43 [03:20<00:00,  3.72s/it]
Training: 100%|██████████| 43/43 [03:20<00:00,  4.67s/it]
2024-08-26 22:03:26.930 | INFO     | __main__:train:206 - Epoch 30 Average Loss: 2.72286
2024-08-26 22:03:28.525 | INFO     | __main__:eval:311 - ***** Running evaluation roberta_CL30*****
2024-08-26 22:03:28.526 | INFO     | __main__:eval:312 -   Num examples = 681
2024-08-26 22:03:28.526 | INFO     | __main__:eval:313 -   Batch size = 64

Evaluating:   0%|          | 0/11 [00:00<?, ?it/s]
Evaluating:   9%|▉         | 1/11 [00:01<00:13,  1.39s/it]
Evaluating:  18%|█▊        | 2/11 [00:02<00:11,  1.31s/it]
Evaluating:  27%|██▋       | 3/11 [00:03<00:10,  1.28s/it]
Evaluating:  36%|███▋      | 4/11 [00:05<00:08,  1.27s/it]
Evaluating:  45%|████▌     | 5/11 [00:06<00:07,  1.26s/it]
Evaluating:  55%|█████▍    | 6/11 [00:07<00:06,  1.23s/it]
Evaluating:  64%|██████▎   | 7/11 [00:08<00:04,  1.21s/it]
Evaluating:  73%|███████▎  | 8/11 [00:09<00:03,  1.19s/it]
Evaluating:  82%|████████▏ | 9/11 [00:10<00:02,  1.17s/it]
Evaluating:  91%|█████████ | 10/11 [00:12<00:01,  1.17s/it]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.01it/s]
Evaluating: 100%|██████████| 11/11 [00:12<00:00,  1.16s/it]
2024-08-26 22:03:41.311 | INFO     | __main__:eval:421 - Classification Report for model_type
                               precision    recall  f1-score   support

                       albert     1.0000    1.0000    1.0000        33
audio-spectrogram-transformer     1.0000    1.0000    1.0000         5
                         bart     1.0000    1.0000    1.0000        22
                         beit     1.0000    0.6667    0.8000         6
                         bert     0.8421    0.9275    0.8828        69
                     big_bird     1.0000    0.9545    0.9767        22
              bigbird_pegasus     1.0000    0.6250    0.7692         8
                   blenderbot     0.7500    0.6000    0.6667         5
                    camembert     0.3947    0.6000    0.4762        25
                       canine     1.0000    1.0000    1.0000         2
                         clip     0.8000    1.0000    0.8889         4
                      codegen     0.8000    1.0000    0.8889         4
                     convnext     1.0000    1.0000    1.0000         7
                   convnextv2     0.6667    1.0000    0.8000         2
                         ctrl     1.0000    1.0000    1.0000         1
                      deberta     1.0000    1.0000    1.0000        15
                   deberta-v2     1.0000    0.9231    0.9600        26
                         detr     0.6000    1.0000    0.7500         3
                   distilbert     0.9714    0.9714    0.9714        35
                          dpr     1.0000    1.0000    1.0000         9
                      electra     0.9200    0.7419    0.8214        31
                          esm     1.0000    1.0000    1.0000         7
                         fnet     0.5000    1.0000    0.6667         2
                         glpn     1.0000    1.0000    1.0000         1
                         gpt2     0.5000    1.0000    0.6667         1
                  gpt_bigcode        nan    0.0000       nan         1
                      gpt_neo     1.0000    0.6667    0.8000         3
                     gpt_neox     0.6667    1.0000    0.8000         2
                         gptj     1.0000    0.6667    0.8000         6
                     layoutlm     1.0000    1.0000    1.0000         1
                   layoutlmv3     1.0000    1.0000    1.0000         2
                          led     1.0000    1.0000    1.0000         5
                         lilt     1.0000    1.0000    1.0000         4
                        llama     0.3333    0.3333    0.3333         3
                   longformer     1.0000    1.0000    1.0000        23
                       longt5     0.7500    1.0000    0.8571         3
                      m2m_100     1.0000    0.7500    0.8571         4
                       marian     0.8333    1.0000    0.9091         5
                  mask2former     1.0000    1.0000    1.0000         8
                   maskformer     1.0000    1.0000    1.0000         5
                        mbart     1.0000    1.0000    1.0000         2
                   mobilebert     1.0000    0.8000    0.8889         5
                 mobilenet_v2     1.0000    1.0000    1.0000         4
                    mobilevit     1.0000    1.0000    1.0000         1
                        mpnet     1.0000    1.0000    1.0000        17
                          mpt     1.0000    1.0000    1.0000         6
                          mt5     1.0000    0.6667    0.8000         6
                          opt     1.0000    1.0000    1.0000         5
                      pegasus     0.7000    1.0000    0.8235         7
                    pegasus_x        nan    0.0000       nan         1
                       plbart     1.0000    0.8333    0.9091         6
                       regnet     1.0000    1.0000    1.0000         3
                       resnet     1.0000    1.0000    1.0000         4
                      roberta     0.5957    0.5385    0.5657        52
                         rwkv     1.0000    1.0000    1.0000         5
                          sam     1.0000    1.0000    1.0000         2
                    segformer     1.0000    1.0000    1.0000         3
               speech_to_text     0.6667    1.0000    0.8000         4
                     speecht5        nan    0.0000       nan         2
                         swin     1.0000    1.0000    1.0000         6
                           t5     0.7857    0.7857    0.7857        14
                    unispeech     0.5000    0.5000    0.5000         4
                unispeech-sat     0.0000    0.0000       nan         2
     vision-text-dual-encoder        nan    0.0000       nan         1
                          vit     1.0000    1.0000    1.0000         3
                     wav2vec2     0.6279    0.9643    0.7606        28
                        wavlm     1.0000    0.0833    0.1538        12
                      whisper     1.0000    1.0000    1.0000         8
                         xglm     1.0000    1.0000    1.0000         4
                  xlm-roberta     0.6250    0.5000    0.5556        30
                        xlnet     1.0000    1.0000    1.0000        11
                        yolos     1.0000    1.0000    1.0000         3

                     accuracy                         0.8429       681
                    macro avg     0.8798    0.8347    0.8789       681
                 weighted avg     0.8623    0.8429    0.8445       681

2024-08-26 22:03:41.312 | INFO     | __main__:eval:422 - Validation Accuracy for model_type: 0.8429
2024-08-26 22:03:41.316 | INFO     | __main__:eval:425 - Classification report saved to ./eval/roberta_model_type_CLCE_64_5e-05_report_21
2024-08-26 22:03:41.316 | INFO     | __main__:eval:453 - Validation loss: 3.4489860101179644
2024-08-26 22:03:41.326 | INFO     | __main__:train:255 - epochs: 30
2024-08-26 22:03:41.326 | INFO     | __main__:train:256 - train_loss: [5.608091232388518, 4.298767699751743, 3.8190557014110476, 3.4874428649281346, 3.44284456829692, 3.3403199107147925, 3.1626151134801463, 3.0863009663515313, 2.98361658495526, 2.927120569140412, 3.007123062776965, 2.925357413846393, 2.910828618116157, 2.818215153938116, 2.816087295842725, 2.907810638117236, 3.0059732614561567, 2.7764069812242376, 2.729142743487691, 2.877095327820889, 2.659833694613257, 2.7924627481504927, 2.7004038234089696, 2.8823549858359403, 2.6932603120803833, 2.6475369569867158, 2.7061318186826484, 2.7415077159571095, 2.726391002189281, 2.722860025805096]
2024-08-26 22:03:41.327 | INFO     | __main__:train:257 - test_loss: [5.390675176273692, 4.626526594161987, 4.27214258367365, 4.099447033622048, 3.8981552340767602, 3.767307259819724, 3.715103344483809, 3.6822332468899814, 3.558532303029841, 3.5820599902759898, 3.5776077183810147, 3.516594409942627, 3.4841770258816807, 3.4368900169025767, 3.509556835347956, 3.486647194082087, 3.451884161342274, 3.4414474097165195, 3.399580326947299, 3.4127013466574927, 3.407679232684049, 3.467908512462269, 3.479128837585449, 3.4826982671564277, 3.378306659785184, 3.364569674838673, 3.421489802273837, 3.3691718036478218, 3.367888255552812, 3.4489860101179644]
