[{"repo_name": "Kevincp560/bigbird-pegasus-large-bigpatent-finetuned-pubMed", "arch": "BigBirdPegasusForConditionalGeneration", "model_type": "bigbird_pegasus", "task": "text2text-generation", "layers": "input to to view bigbirdpegasusscaledwordembedding add dropout to add add isinf any to add add isinf any to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any to add add isinf any to add add isinf any to add add isinf any to add add isinf any to add add isinf any to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add isinf any isnan any to add add isinf any to add add isinf any to add add isinf any to add add isinf any isnan any to add add isinf any to add add isinf any isnan any to add add layernorm to output"}, {"repo_name": "rubentito/longformer-base-mpdocvqa", "arch": "LongformerForQuestionAnswering", "model_type": "longformer", "task": "question-answering", "layers": "input to pad to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "beomi/KoAlpaca-KoRWKV-1.5B", "arch": "RwkvForCausalLM", "model_type": "rwkv", "task": "text-generation", "layers": "input to embedding to layernorm add add to add add to layernorm to __getitem__ mul add linear4bit __getitem__ float maximum maximum sub exp mul add div to mul add sub exp mul sub exp mul add mul add mul add mul add add maximum sub exp mul sub exp sub exp mul add add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float add add zeros_like zeropad2d mul mul add linear4bit sigmoid mul linear4bit add add to add add to add add to add add div to add add to layernorm to __getitem__ zeropad2d mul add linear4bit sigmoid mul linear4bit add add to add add to layernorm to __getitem__ mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add mul mul mul add div to mul add linear4bit sigmoid mul linear4bit add add to add add to layernorm to __getitem__ zeropad2d mul add linear4bit __getitem__ float maximum maximum sub exp mul add div to mul add sub exp mul sub exp mul add mul add mul add mul add add maximum sub exp mul sub exp sub exp mul add add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float add add zeros_like mul add linear4bit __getitem__ __getitem__ mul add linear4bit sigmoid mul linear4bit add add div to layernorm to __getitem__ mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add mul mul mul add div to mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ mul add linear4bit sigmoid mul add to add add to add layernorm to __getitem__ mul add linear4bit relu square linear4bit mul add to layernorm to __getitem__ mul add linear4bit sigmoid mul linear4bit add add to layernorm to __getitem__ mul add linear4bit __getitem__ float maximum maximum sub exp mul add div to mul add sub exp mul sub exp mul add mul add mul add mul add add maximum sub exp mul sub exp sub exp mul add add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float add add zeros_like mul add linear4bit __getitem__ __getitem__ zeropad2d mul mul mul add linear4bit sigmoid mul linear4bit add add to add add div to add layernorm to __getitem__ mul add linear4bit sigmoid mul add to add add to layernorm to __getitem__ mul add linear4bit sigmoid mul linear4bit add add to layernorm to __getitem__ zeropad2d mul add linear4bit __getitem__ float maximum maximum sub exp mul add div to mul add sub exp mul sub exp mul add mul add mul add mul add add maximum sub exp mul sub exp sub exp mul add add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float add add zeros_like mul add linear4bit sigmoid mul linear4bit add add to layernorm to __getitem__ zeropad2d mul add linear4bit __getitem__ float maximum maximum sub exp mul add div to mul add sub exp mul sub exp mul add mul add mul add mul add add maximum sub exp mul sub exp sub exp mul add add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float add add zeros_like mul add linear4bit __getitem__ __getitem__ mul add linear4bit sigmoid mul linear4bit add add to layernorm to __getitem__ mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add mul mul mul add div to mul add linear4bit __getitem__ float maximum maximum sub exp mul add sub exp sub exp mul mul add mul add add maximum sub exp sub exp sub exp add add maximum sub exp mul mul add sub exp add __getitem__ float add add zeros_like zeropad2d mul mul mul add linear4bit sigmoid mul linear4bit add add div layernorm to output"}, {"repo_name": "t5-3b", "arch": "T5WithLMHeadModel", "model_type": "t5", "task": "translation", "layers": "input to to embedding to add mistralrmsnorm to linear4bit mul linear4bit add to add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose output"}, {"repo_name": "IDEA-CCNL/Erlangshen-UniMC-DeBERTa-v2-110M-Chinese", "arch": "DebertaV2ForMaskedLM", "model_type": "deberta-v2", "task": "fill-mask", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to permute contiguous conv1d permute contiguous masked_fill stabledropout geluactivation add layernorm to mul to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "pykeio/lite-toxic-comment-classification", "arch": "AlbertForSequenceClassification", "model_type": "albert", "task": "text-classification", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm output"}, {"repo_name": "SEBIS/code_trans_t5_small_source_code_summarization_csharp_multitask", "arch": "T5Model", "model_type": "t5", "task": "summarization", "layers": "input to to embedding to mistralrmsnorm to linear4bit view transpose __getitem__ cat mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose __getitem__ cat mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose output"}, {"repo_name": "bjubert/6_epochs_camembert", "arch": "CamembertForTokenClassification", "model_type": "camembert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "nielsr/maskformer-swin-tiny-ade", "arch": "MaskFormerForInstanceSegmentation", "model_type": "maskformer", "task": "unknown", "layers": "input conv2d flatten transpose layernorm dropout layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view maskformerswindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view maskformerswindroppath add layernorm linear geluactivation linear dropout add layernorm permute view contiguous conv2d groupnorm add conv2d groupnorm relu conv2d output"}, {"repo_name": "LLukas22/all-mpnet-base-v2-embedding-all", "arch": "MPNetModel", "model_type": "mpnet", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "Forturne/AGC_KoBigBird", "arch": "BigBirdForQuestionAnswering", "model_type": "big_bird", "task": "question-answering", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "raphgg/dummy-model", "arch": "CamembertForMaskedLM", "model_type": "camembert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "KarelDO/roberta-base.CEBaB_confounding.price_food_ambiance_negative.sa.5-class.seed_43", "arch": "RobertaForNonlinearSequenceClassification", "model_type": "roberta", "task": "unknown", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "chau/autotrain-swot-small-3266191126", "arch": "DebertaV2ForQuestionAnswering", "model_type": "deberta-v2", "task": "question-answering", "layers": "input to to embedding layernorm mul stabledropout to to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "HasinMDG/SDeberta-base-v0", "arch": "DebertaV2Model", "model_type": "deberta-v2", "task": "text-classification", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "stevemobs/deberta-base-combined-squad1-aqa-and-newsqa-1epoch", "arch": "DebertaForQuestionAnswering", "model_type": "deberta", "task": "question-answering", "layers": "input embedding debertalayernorm mul stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm output"}, {"repo_name": "GCopoulos/deberta-finetuned-answer-polarity-7e5", "arch": "DebertaForSequenceClassification", "model_type": "deberta", "task": "text-classification", "layers": "input embedding zeros_like debertalayernorm mul stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm linear view permute chunk transpose matmul add masked_fill softmax masked_fill stabledropout matmul permute contiguous view linear stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm output"}, {"repo_name": "kundank/dspt-electra-small-hellaswag", "arch": "ElectraModel", "model_type": "electra", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "lixiqi/beit-base-patch16-224-pt22k-ft22k-finetuned-FER2013-6e-05", "arch": "BeitForImageClassification", "model_type": "beit", "task": "image-classification", "layers": "input conv2d flatten transpose cat dropout add add add add add add add add add add add add add add add add add add add add add add add add output"}, {"repo_name": "seyonec/PubChem10M_SMILES_BPE_240k", "arch": "RobertaForMaskedLM", "model_type": "roberta", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "Damith/AraELECTRA-discriminator-QuranQA", "arch": "ElectraForQuestionAnswering", "model_type": "electra", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "Palak/microsoft_deberta-base_squad", "arch": "DebertaForQuestionAnswering", "model_type": "deberta", "task": "question-answering", "layers": "input embedding debertalayernorm mul stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm output"}, {"repo_name": "oliverguhr/wav2vec2-large-xlsr-53-german-cv9", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "wietsedv/xlm-roberta-base-ft-udpos28-pt", "arch": "XLMRobertaForTokenClassification", "model_type": "xlm-roberta", "task": "token-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "textattack/xlnet-base-cased-WNLI", "arch": "XLNetLMHeadModel", "model_type": "xlnet", "task": "text-generation", "layers": "input transpose contiguous embedding dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "unicamp-dl/ptt5-base-portuguese-vocab", "arch": "T5WithLMHeadModel", "model_type": "t5", "task": "text2text-generation", "layers": "input to to embedding to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to add mistralrmsnorm to linear4bit silu mul linear4bit add mistralrmsnorm linear float to output"}, {"repo_name": "DSI/ar_emotion_6", "arch": "BertForMultiLabelSequenceClassification", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "maastrichtlawtech/legal-camembert", "arch": "CamembertForMaskedLM", "model_type": "camembert", "task": "fill-mask", "layers": "input to to ne int mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "SI2M-Lab/DarijaBERT", "arch": "BertForMaskedLM", "model_type": "bert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "r10521708/albert-base-chinese-finetuned-qqp-TM-5x", "arch": "AlbertForSequenceClassification", "model_type": "albert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm __getitem__ linear tanh to output"}, {"repo_name": "oliverguhr/fullstop-punctuation-multilingual-sonar-base", "arch": "XLMRobertaForTokenClassification", "model_type": "xlm-roberta", "task": "token-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "Taige/xlnet-int", "arch": "XLNetForSequenceClassification", "model_type": "xlnet", "task": "text-classification", "layers": "input transpose contiguous embedding dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "Dr-BERT/DrBERT-4GB", "arch": "CamembertForMaskedLM", "model_type": "camembert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "osiria/deberta-italian-question-answering", "arch": "DebertaV2ForQuestionAnswering", "model_type": "deberta-v2", "task": "question-answering", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "deepset/gelectra-large-germanquad", "arch": "ElectraForQuestionAnswering", "model_type": "electra", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "RogerB/afro-xlmr-large-finetuned-kintweetsD", "arch": "XLMRobertaForMaskedLM", "model_type": "xlm-roberta", "task": "fill-mask", "layers": "input to to ne int mul long add embedding add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "shahrukhx01/muv2x-simcse-smole-bert", "arch": "BertForCL", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "facebook/maskformer-resnet101-ade", "arch": "MaskFormerForInstanceSegmentation", "model_type": "maskformer", "task": "unknown", "layers": "input conv2d batchnorm2d relu maxpool2d conv2d batchnorm2d add relu add relu add relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu conv2d groupnorm add conv2d groupnorm relu interpolate add conv2d groupnorm relu conv2d output"}, {"repo_name": "naver/splade_v2_max", "arch": "DistilBertForMaskedLM", "model_type": "distilbert", "task": "fill-mask", "layers": "input embedding add layernorm dropout linear view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "joheras/longformer-base-4096-bne-es-finetuned-v2", "arch": "LongformerForTokenClassification", "model_type": "longformer", "task": "token-classification", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "huggingface/CodeBERTa-small-v1", "arch": "RobertaForMaskedLM", "model_type": "roberta", "task": "fill-mask", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "sentence-transformers/gtr-t5-base", "arch": "T5EncoderModel", "model_type": "t5", "task": "sentence-similarity", "layers": "input view embedding dropout add add add add add add add add add add add add add add t5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add add add add add add add add add add t5layernorm dropout output"}, {"repo_name": "versae/bertin-roberta-base-spanish-finetuned-recores", "arch": "RobertaForMultipleChoice", "model_type": "roberta", "task": "multiple-choice", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "approach0/dpr-cotbert-120", "arch": "DprEncoder", "model_type": "bert", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "vocab-transformers/distilbert-word2vec_256k-MLM_best", "arch": "DistilBertForMaskedLM", "model_type": "distilbert", "task": "fill-mask", "layers": "input embedding add layernorm dropout linear view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "hf-internal-testing/tiny-detr-mobilenetsv3", "arch": "DetrForObjectDetection", "model_type": "detr", "task": "object-detection", "layers": "input __getitem__ float interpolate to __getitem__ detrsinepositionembedding to __getitem__ float interpolate to __getitem__ detrsinepositionembedding to __getitem__ float interpolate to __getitem__ flatten __getitem__ expand to __rsub__ masked_fill add view softmax dropout bmm view transpose reshape linear dropout add layernorm add layernorm layernorm output"}, {"repo_name": "aboMesalam/my_awesome_swag_model", "arch": "BertForMultipleChoice", "model_type": "bert", "task": "multiple-choice", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "acappella/dummy-model", "arch": "CamembertModel", "model_type": "camembert", "task": "feature-extraction", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm __getitem__ linear tanh output"}, {"repo_name": "wbbbbb/wav2vec2-large-chinese-zh-cn", "arch": "Wav2Vec2ForPreTraining", "model_type": "wav2vec2", "task": "pretraining", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "patrickvonplaten/wav2vec2-base-timit-demo-colab", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to to to to view speecht5sinusoidalpositionalembedding add dropout to to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to linear to output"}, {"repo_name": "joheras/longformer-base-4096-bne-es-finetuned-augmented1", "arch": "LongformerForTokenClassification", "model_type": "longformer", "task": "token-classification", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "amandakonet/climatebert-fact-checking", "arch": "RobertaForSequenceClassification", "model_type": "roberta", "task": "text-classification", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "ZeyadAhmed/AraElectra-Arabic-SQuADv2-QA", "arch": "ElectraForQuestionAnswering", "model_type": "electra", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "hf-tiny-model-private/tiny-random-DistilBertForQuestionAnswering", "arch": "DistilBertForQuestionAnswering", "model_type": "distilbert", "task": "question-answering", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to to to to linear4bit view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm add layernorm to add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to output"}, {"repo_name": "jonatasgrosman/exp_w2v2t_de_unispeech-sat_s968", "arch": "UniSpeechSatForCTC", "model_type": "unispeech-sat", "task": "automatic-speech-recognition", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "transformersbook/distilbert-base-uncased-distilled-clinc", "arch": "DistilBertForSequenceClassification", "model_type": "distilbert", "task": "text-classification", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "tubui7121/td-ner-bert", "arch": "DebertaV2ForTokenClassification", "model_type": "deberta-v2", "task": "token-classification", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to output"}, {"repo_name": "kundank/dspt-electra-small-mnli", "arch": "ElectraModel", "model_type": "electra", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "hts98/gender_classification", "arch": "Wav2Vec2ForSequenceClassification", "model_type": "wav2vec2", "task": "audio-classification", "layers": "input __getitem__ conv1d groupnorm geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation transpose layernorm linear dropout transpose conv1d wav2vec2samepadlayer geluactivation transpose add layernorm dropout linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm output"}, {"repo_name": "GCopoulos/opt-125m-finetuned-answerpol-01", "arch": "OPTForSequenceClassification", "model_type": "opt", "task": "text-classification", "layers": "input to to view embedding add to add reshape add view to add reshape add view to layernorm to linear4bit view transpose contiguous view transpose bmm view add max view softmax to dropout bmm view transpose reshape linear4bit dropout add reshape add view to layernorm to linear4bit view transpose contiguous view transpose bmm view add max view softmax to dropout bmm view transpose reshape linear4bit dropout add reshape add view to layernorm to linear4bit view transpose contiguous to output"}, {"repo_name": "pucpr/clinicalnerpt-pharmacologic", "arch": "BertForTokenClassification", "model_type": "bert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "cross-encoder/ms-marco-electra-base", "arch": "ElectraForSequenceClassification", "model_type": "electra", "task": "text-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "textattack/xlnet-base-cased-MRPC", "arch": "XLNetLMHeadModel", "model_type": "xlnet", "task": "text-generation", "layers": "input transpose contiguous embedding dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "jplu/tf-camembert-base", "arch": "CamembertForMaskedLM", "model_type": "camembert", "task": "fill-mask", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "alexziweiwang/retrain_epoch2to5", "arch": "Wav2Vec2ForCTCnCLS", "model_type": "wav2vec2", "task": "unknown", "layers": "input __getitem__ conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation transpose layernorm linear dropout transpose conv1d wav2vec2samepadlayer geluactivation transpose add dropout layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm output"}, {"repo_name": "llange/xlm-roberta-large-spanish", "arch": "XLMRobertaForMaskedLM", "model_type": "xlm-roberta", "task": "fill-mask", "layers": "input to to ne int mul long add embedding add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "voidful/albert_chinese_tiny", "arch": "AlbertForMaskedLM", "model_type": "albert", "task": "fill-mask", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm linear geluactivation linear add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm linear geluactivation linear add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm linear geluactivation linear add layernorm __getitem__ linear tanh output"}, {"repo_name": "hf-internal-testing/tiny-random-wavlm", "arch": "WavLMForSequenceClassification", "model_type": "wavlm", "task": "audio-classification", "layers": "input __getitem__ conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation transpose layernorm output"}, {"repo_name": "SEBIS/legal_t5_small_summ_en", "arch": "T5WithLMHeadModel", "model_type": "t5", "task": "text2text-generation", "layers": "input to to embedding to mistralrmsnorm to linear4bit view transpose output"}, {"repo_name": "tgsc/sentence-transformers_paraphrase-multilingual-mpnet-base-v2", "arch": "XLMRobertaModel", "model_type": "xlm-roberta", "task": "sentence-similarity", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "dkleczek/bert-base-polish-uncased-v1", "arch": "BertForMaskedLM", "model_type": "bert", "task": "fill-mask", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "AyoubMDL/plbart_assert_finetuned", "arch": "PLBartForConditionalGeneration", "model_type": "plbart", "task": "text2text-generation", "layers": "input to to plbartlearnedpositionalembedding to add layernorm dropout to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm add layernorm isnan any to add layernorm add layernorm isnan any to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm add layernorm isnan any to add layernorm add layernorm isnan any to add layernorm add layernorm isnan any to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any to add layernorm add layernorm to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm isnan any to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm add layernorm isnan any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any to to to linear4bit view transpose contiguous to output"}, {"repo_name": "jlondonobo/whisper-large-v2-pt-v3", "arch": "WhisperForConditionalGeneration", "model_type": "whisper", "task": "automatic-speech-recognition", "layers": "input view whisperpositionalembedding embedding add dropout layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add add add layernorm linear geluactivation dropout linear dropout add add add add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm output"}, {"repo_name": "facebook/regnet-x-320", "arch": "RegNetForImageClassification", "model_type": "regnet", "task": "image-classification", "layers": "input conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d add relu adaptiveavgpool2d output"}, {"repo_name": "ChrisPCz/lilt-roberta-base-wechsel-german", "arch": "LiltModel", "model_type": "lilt", "task": "feature-extraction", "layers": "input to to ne int type_as mul long add to embedding add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "nlp-en-es/bertin-large-finetuned-sqac", "arch": "RobertaForQuestionAnswering", "model_type": "roberta", "task": "question-answering", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "facebook/sam-vit-large", "arch": "SamModel", "model_type": "sam", "task": "mask-generation", "layers": "input conv2d permute add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add permute conv2d samlayernorm conv2d samlayernorm add repeat_interleave flatten permute unsqueeze add linear reshape transpose matmul div softmax matmul transpose reshape linear add layernorm linear reshape transpose matmul transpose reshape linear add layernorm linear relu linear add layernorm add linear reshape transpose matmul div softmax matmul transpose reshape linear add layernorm __getitem__ __getitem__ linear relu linear relu linear stack matmul reshape __getitem__ output"}, {"repo_name": "QuanjieHan/resume_ner_1", "arch": "BertForTokenClassification", "model_type": "bert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "LLukas22/deberta-v3-base-qa-en", "arch": "DebertaV2ForQuestionAnswering", "model_type": "deberta-v2", "task": "question-answering", "layers": "input to to embedding layernorm mul stabledropout to to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "jjonhwa/BIG-29000", "arch": "BertForV2QuestionAnswering", "model_type": "big_bird", "task": "unknown", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "sschet/biomedical-ner-all", "arch": "DistilBertForTokenClassification", "model_type": "distilbert", "task": "token-classification", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "MysticShadow427/wav2vec2-speech-emotion-recognition-crema", "arch": "Wav2Vec2ForSpeechClassification", "model_type": "wav2vec2", "task": "unknown", "layers": "input __getitem__ conv1d groupnorm geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation transpose layernorm linear dropout add layernorm dropout linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm output"}, {"repo_name": "MilaNLProc/feel-it-italian-sentiment", "arch": "CamembertForSequenceClassification", "model_type": "camembert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "hoskinson-center/proofGPT-v0.1", "arch": "GPTNeoXForCausalLM", "model_type": "gpt_neox", "task": "text-generation", "layers": "input to embedding dropout to add to add to layernorm to linear4bit view __getitem__ permute gptneoxrotaryembedding output"}, {"repo_name": "ken11/albert-base-japanese-v1", "arch": "AlbertForMaskedLM", "model_type": "albert", "task": "fill-mask", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "shailja/fine-tuned-codegen-2B-Verilog", "arch": "CodeGenForCausalLM", "model_type": "codegen", "task": "text-generation", "layers": "input view embedding dropout add layernorm linear reshape split reshape reshape permute output"}, {"repo_name": "jjonhwa/3750", "arch": "RobertaForV2QuestionAnswering", "model_type": "roberta", "task": "unknown", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "Mizuiro-sakura/deberta-v2-base-japanese-finetuned-QAe", "arch": "DebertaV2ForQuestionAnswering", "model_type": "deberta-v2", "task": "question-answering", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to add layernorm to mul to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "Helsinki-NLP/opus-mt-ko-fr", "arch": "MarianMTModel", "model_type": "marian", "task": "translation", "layers": "input to to view embedding mul add dropout to add layernorm linear4bit silu dropout linear4bit dropout add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to add layernorm linear4bit silu dropout linear4bit dropout add layernorm isinf any isnan any to add layernorm linear4bit silu dropout linear4bit dropout add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to to to linear4bit view transpose contiguous to output"}, {"repo_name": "alramalho/distilbart-mnli-12-1-extended-labels", "arch": "BartForSequenceClassification", "model_type": "bart", "task": "text-classification", "layers": "input to to view bartscaledwordembedding add layernorm dropout to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isinf any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm isinf any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm isinf any to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm isinf any to to to linear4bit view transpose contiguous output"}, {"repo_name": "hf-internal-testing/tiny-random-LongT5ForConditionalGeneration", "arch": "LongT5ForConditionalGeneration", "model_type": "longt5", "task": "text2text-generation", "layers": "input view embedding dropout longt5layernorm linear view pad reshape pad __getitem__ cat einsum contiguous view __getitem__ linear dropout add longt5layernorm linear relu dropout linear dropout add add longt5layernorm linear relu dropout linear dropout add add longt5layernorm linear relu dropout linear dropout add longt5layernorm linear view pad reshape pad __getitem__ cat einsum add float softmax type_as dropout type einsum contiguous view __getitem__ linear dropout add longt5layernorm linear relu dropout linear dropout add add longt5layernorm linear relu dropout linear dropout add longt5layernorm dropout output"}, {"repo_name": "csebuetnlp/mT5_multilingual_XLSum", "arch": "MT5ForConditionalGeneration", "model_type": "mt5", "task": "text2text-generation", "layers": "input view embedding dropout add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add add mt5layernorm linear view transpose matmul transpose contiguous view linear dropout add add mt5layernorm linear view transpose matmul transpose contiguous view linear dropout add add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add add add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add mt5layernorm linear mul dropout linear dropout add add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add mt5layernorm linear mul dropout linear dropout add add mt5layernorm linear mul dropout linear dropout add mt5layernorm dropout output"}, {"repo_name": "dbmdz/electra-base-turkish-mc4-uncased-discriminator", "arch": "ElectraForPreTraining", "model_type": "electra", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "hieule/deberta-vie-conv", "arch": "DebertaV2ForMaskedLM", "model_type": "deberta-v2", "task": "fill-mask", "layers": "input to to embedding add layernorm mul stabledropout to to to to linear4bit view permute contiguous view bmm view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "typeform/mobilebert-uncased-mnli", "arch": "MobileBertForSequenceClassification", "model_type": "mobilebert", "task": "zero-shot-classification", "layers": "input to to embedding cat linear4bit add add nonorm dropout to to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to to add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to to add nonorm to output"}, {"repo_name": "sentence-transformers/paraphrase-distilroberta-base-v1", "arch": "RobertaModel", "model_type": "roberta", "task": "sentence-similarity", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "shreeramchandra/ser_wav2vec_indianEnglish_greek_pretrained_balanced_data", "arch": "Wav2Vec2ForSpeechClassification", "model_type": "wav2vec2", "task": "audio-classification", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "philschmid/clip-zero-shot-image-classification", "arch": "CLIPModel", "model_type": "clip", "task": "zero-shot-image-classification", "layers": "input to to view to argmax __getitem__ linear norm div matmul mul t to output"}, {"repo_name": "approach0/dpr-cotbert-220", "arch": "DprEncoder", "model_type": "bert", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "lemon234071/t5-base-Chinese", "arch": "MT5ForConditionalGeneration", "model_type": "mt5", "task": "text2text-generation", "layers": "input view embedding dropout add add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add mt5layernorm linear newgeluactivation mul dropout linear dropout add add add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add add mt5layernorm linear view transpose matmul add float softmax type_as dropout matmul transpose contiguous view linear dropout add add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add mt5layernorm linear newgeluactivation mul dropout linear dropout add mt5layernorm linear view transpose matmul transpose contiguous view linear dropout add mt5layernorm linear mul dropout linear dropout add add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul transpose contiguous view linear dropout add add add add add add add add mt5layernorm dropout output"}, {"repo_name": "nagupv/deberta-v3-large-hf-weights_v2_f3", "arch": "DebertaV2ForMultipleChoice", "model_type": "deberta-v2", "task": "multiple-choice", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "sentence-transformers/quora-distilbert-multilingual", "arch": "DistilBertModel", "model_type": "distilbert", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "AnReu/math_albert", "arch": "AlbertModel", "model_type": "albert", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to output"}, {"repo_name": "ibombonato/swin-age-classifier", "arch": "SwinForImageClassification", "model_type": "swin", "task": "image-classification", "layers": "input conv2d flatten transpose layernorm dropout layernorm view pad view permute contiguous view view linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add add add layernorm linear geluactivation linear dropout add add add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "KarelDO/bert-base-uncased.CEBaB_confounding.food_service_positive.sa.5-class.seed_44", "arch": "BertForNonlinearSequenceClassification", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "jjonhwa/4250", "arch": "RobertaForV2QuestionAnswering", "model_type": "roberta", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "abertsch/unlimiformer-bart-summscreen-retrieval", "arch": "BartModel", "model_type": "bart", "task": "text2text-generation", "layers": "input to to bartlearnedpositionalembedding to add layernorm dropout to add layernorm add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isinf any isnan any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "ALM-AHME/beit-large-patch16-224-finetuned-BreastCancer-Classification-BreakHis-AH-60-20-20", "arch": "BeitForImageClassification", "model_type": "beit", "task": "image-classification", "layers": "input conv2d flatten transpose cat dropout add layernorm linear geluactivation linear dropout mul add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add add add add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add add add add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add add add layernorm linear geluactivation linear dropout mul beitdroppath add __getitem__ mean layernorm output"}, {"repo_name": "anuragshas/whisper-large-v2-bn", "arch": "WhisperForConditionalGeneration", "model_type": "whisper", "task": "automatic-speech-recognition", "layers": "input view whisperpositionalembedding embedding add dropout layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add add add layernorm linear geluactivation dropout linear dropout add add add add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm output"}, {"repo_name": "ybelkada/japanese-roberta-question-answering", "arch": "RobertaForQuestionAnswering", "model_type": "roberta", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "facebook/roscoe-512-roberta-base", "arch": "RobertaForCL", "model_type": "roberta", "task": "unknown", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "Kirili4ik/mbart_ruDialogSum", "arch": "MBartForConditionalGeneration", "model_type": "mbart", "task": "text2text-generation", "layers": "input mbartlearnedpositionalembedding to add layernorm dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm output"}, {"repo_name": "Alireza1044/albert-base-v2-stsb", "arch": "AlbertForSequenceClassification", "model_type": "albert", "task": "text-classification", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm output"}, {"repo_name": "sultan/BioM-ALBERT-xxlarge", "arch": "AlbertForMaskedLM", "model_type": "albert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm __getitem__ linear tanh to output"}, {"repo_name": "sshleifer/distill-pegasus-cnn-16-4", "arch": "PegasusForConditionalGeneration", "model_type": "pegasus", "task": "text2text-generation", "layers": "input __getitem__ view embedding mul add dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm output"}, {"repo_name": "facebook/m2m100-12B-avg-5-ckpt", "arch": "M2M100ForConditionalGeneration", "model_type": "m2m_100", "task": "text2text-generation", "layers": "input __getitem__ view embedding mul m2m100sinusoidalpositionalembedding to add dropout layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add add add add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add add add add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add add add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add add add layernorm linear view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear dropout add add add add add add add add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add add add layernorm linear view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear dropout add add add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add add add add add add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add add add add add add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add add add layernorm output"}, {"repo_name": "ffgcc/InfoCSE-bert-large", "arch": "BertForCL", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "Soulaimen/convnext-large-224-22k-1k-convnext_short_sleeve_data_cleaned", "arch": "ConvNextForImageClassification", "model_type": "convnext", "task": "image-classification", "layers": "input conv2d convnextlayernorm conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add convnextlayernorm conv2d conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add output"}, {"repo_name": "Serjssv/ast-finetuned-audioset-10-10-0.4593-finetuned-gtzan", "arch": "ASTForAudioClassification", "model_type": "audio-spectrogram-transformer", "task": "audio-classification", "layers": "input unsqueeze transpose conv2d flatten transpose cat add dropout layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "hf-internal-testing/tiny-random-DebertaForQuestionAnswering", "arch": "DebertaForQuestionAnswering", "model_type": "deberta", "task": "question-answering", "layers": "input embedding add add debertalayernorm mul stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm output"}, {"repo_name": "reralle/wavlm-basic_s-f-o_8batch_5sec_0.0001lr_unfrozen", "arch": "WavLMForSequenceClassification", "model_type": "wavlm", "task": "audio-classification", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "jjonhwa/BIG-2000", "arch": "BertForV2QuestionAnswering", "model_type": "big_bird", "task": "unknown", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm __getitem__ linear tanh to output"}, {"repo_name": "facebook/s2t-small-mustc-en-it-st", "arch": "Speech2TextForConditionalGeneration", "model_type": "speech_to_text", "task": "automatic-speech-recognition", "layers": "input to to view embedding mul add dropout to add add layernorm linear4bit relu dropout linear4bit dropout add to add add add to add add add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add add layernorm linear4bit relu dropout linear4bit dropout add to add add add to layernorm to linear4bit view transpose contiguous to output"}, {"repo_name": "nateraw/planes-trains-automobiles", "arch": "ViTForImageClassification", "model_type": "vit", "task": "image-classification", "layers": "input to to to to conv2d flatten transpose cat add dropout to to add layernorm to linear4bit geluactivation to linear4bit dropout add to add to add to layernorm to to to linear4bit view permute scaled_dot_product_attention permute contiguous view to linear4bit dropout add to add to add layernorm to linear4bit geluactivation to linear4bit dropout add to add layernorm to linear4bit geluactivation to linear4bit dropout add to add layernorm to linear4bit geluactivation to linear4bit dropout add to add layernorm to linear4bit geluactivation to linear4bit dropout add to add layernorm to linear4bit geluactivation to linear4bit dropout add to add layernorm to linear4bit geluactivation to linear4bit dropout add to add to add to add to add to layernorm to to to linear4bit view permute scaled_dot_product_attention permute contiguous view to linear4bit dropout add to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "gcyzsl/unsup-VisualCSE-roberta-base", "arch": "RobertaForCL", "model_type": "roberta", "task": "unknown", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "mrm8488/convnext-tiny-finetuned-beans", "arch": "ConvNextForImageClassification", "model_type": "convnext", "task": "image-classification", "layers": "input conv2d convnextlayernorm conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d add add add convnextlayernorm conv2d conv2d permute convnextlayernorm linear geluactivation linear mul permute add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add output"}, {"repo_name": "Akhil06042002/model_model_super_model", "arch": "BertForMultipleChoice", "model_type": "bert", "task": "multiple-choice", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "abhijithneilabraham/longformer_covid_qa", "arch": "LongformerForQuestionAnswering", "model_type": "longformer", "task": "question-answering", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "PolarNight/deberta-base-ner-hw", "arch": "DebertaForTokenClassification", "model_type": "deberta", "task": "token-classification", "layers": "input embedding debertalayernorm mul stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm output"}, {"repo_name": "SEBIS/legal_t5_small_summ_it", "arch": "T5WithLMHeadModel", "model_type": "t5", "task": "text2text-generation", "layers": "input to to embedding to add add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add output"}, {"repo_name": "vumichien/emo-mobilebert", "arch": "MobileBertForSequenceClassification", "model_type": "mobilebert", "task": "text-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear add nonorm add nonorm linear relu linear add nonorm add nonorm add nonorm linear dropout add nonorm add nonorm add nonorm linear view permute matmul permute contiguous view linear add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm linear nonorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm linear nonorm add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm linear nonorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm add nonorm linear nonorm add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm linear nonorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm add nonorm linear nonorm add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm linear view permute matmul permute contiguous view linear add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm linear view permute matmul permute contiguous view linear add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm linear view permute matmul permute contiguous view linear add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm linear view permute matmul permute contiguous view linear add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm linear view permute matmul permute contiguous view linear add nonorm add nonorm add nonorm add nonorm add nonorm linear dropout add nonorm __getitem__ linear tanh output"}, {"repo_name": "bgk/lodosalberttr", "arch": "AlbertForTokenClassification", "model_type": "albert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to output"}, {"repo_name": "facebook/mms-lid-1024", "arch": "Wav2Vec2ForSequenceClassification", "model_type": "wav2vec2", "task": "audio-classification", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "hfl/chinese-legal-electra-small-generator", "arch": "ElectraForMaskedLM", "model_type": "electra", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "cl-tohoku/bert-large-japanese-char-v2", "arch": "BertForPreTraining", "model_type": "bert", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "dev-senolys/camembert_base_finetunned_one_thema_balanced_8_epochs", "arch": "CamembertForSequenceClassification", "model_type": "camembert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "ZeyadAhmed/AraElectra-Arabic-SQuADv2-CLS", "arch": "ElectraForSequenceClassification", "model_type": "electra", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "anas-awadalla/bart-base-few-shot-k-512-finetuned-squad-seed-0", "arch": "BartForQuestionAnswering", "model_type": "bart", "task": "question-answering", "layers": "input to to bartlearnedpositionalembedding to add layernorm dropout to add layernorm add layernorm isinf any isnan any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "alexziweiwang/retrain_first1epoch", "arch": "Wav2Vec2ForCTCnCLS", "model_type": "wav2vec2", "task": "unknown", "layers": "input __getitem__ conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation transpose layernorm linear dropout transpose conv1d wav2vec2samepadlayer geluactivation transpose add dropout layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm output"}, {"repo_name": "kundank/dspt-electra-small-yahooanswertopics", "arch": "ElectraModel", "model_type": "electra", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "mobashgr/BC4CHEMD-WLT-128-BioELECTRA-Pubmed-WELT-30", "arch": "ElectraForTokenClassification", "model_type": "electra", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "facebook/convnext-base-224-22k-1k", "arch": "ConvNextForImageClassification", "model_type": "convnext", "task": "image-classification", "layers": "input conv2d convnextlayernorm add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add convnextlayernorm conv2d conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d add add add mean layernorm output"}, {"repo_name": "KarelDO/bert-base-uncased.CEBaB_confounding.food_service_positive.absa.5-class.seed_42", "arch": "BertForNonlinearSequenceClassification", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "ITG/whisper-base-gl", "arch": "WhisperForConditionalGeneration", "model_type": "whisper", "task": "automatic-speech-recognition", "layers": "input to to view embedding add dropout to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to add add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "castorini/ance-dpr-context-multi", "arch": "DPRContextEncoder", "model_type": "dpr", "task": "unknown", "layers": "input to to to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm __getitem__ to output"}, {"repo_name": "PlanTL-GOB-ES/roberta-base-bne-capitel-ner", "arch": "RobertaForTokenClassification", "model_type": "roberta", "task": "token-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "Mihaj/wav2vec2-large-golos-robot-for-airi", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "sentence-transformers/nli-bert-large", "arch": "BertModel", "model_type": "bert", "task": "sentence-similarity", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "JanSt/albert-base-v2_mbti-classification", "arch": "AlbertForSequenceClassification", "model_type": "albert", "task": "text-classification", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm output"}, {"repo_name": "SauravMaheshkar/clr-finetuned-albert-base", "arch": "AlbertForMaskedLM", "model_type": "albert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to output"}, {"repo_name": "Ebtihal/AraDiaBERT_V3", "arch": "BertLMHeadModel", "model_type": "bert", "task": "text-generation", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute to output"}, {"repo_name": "radned/ast-finetuned-audioset-10-10-0.4593-AST-fp16-finetuned-gtzan", "arch": "ASTForAudioClassification", "model_type": "audio-spectrogram-transformer", "task": "audio-classification", "layers": "input unsqueeze transpose conv2d flatten transpose cat add dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add add add add layernorm linear geluactivation linear dropout add add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add add add add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add layernorm output"}, {"repo_name": "pszemraj/bart-large-code-instructiongen", "arch": "BartForConditionalGeneration", "model_type": "bart", "task": "text2text-generation", "layers": "input to to view bartscaledwordembedding add layernorm dropout to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isinf any to add layernorm add layernorm to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm isinf any to add layernorm add layernorm to add layernorm add layernorm to to to linear4bit view transpose contiguous output"}, {"repo_name": "DunnBC22/mpnet-base-apple_iphone_se_reviews", "arch": "MPNetForSequenceClassification", "model_type": "mpnet", "task": "text-classification", "layers": "input to __getitem__ to ne int type_as mul long add embedding add layernorm dropout to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "gchhablani/fnet-base-finetuned-sst2", "arch": "FNetForSequenceClassification", "model_type": "fnet", "task": "text-classification", "layers": "input embedding add add layernorm linear dropout add layernorm linear newgeluactivation linear dropout add layernorm fnetbasicfouriertransform add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm fnetbasicfouriertransform add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm output"}, {"repo_name": "VoVanPhuc/unsup-SimCSE-VietNamese-phobert-base", "arch": "RobertaForCL", "model_type": "roberta", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "roneneldan/TinyStories-3M", "arch": "GPTNeoForCausalLM", "model_type": "gpt_neo", "task": "text-generation", "layers": "input view embedding add dropout add add add add add add add add add add add add add add add add layernorm view output"}, {"repo_name": "DDSC/roberta-base-danish", "arch": "RobertaForMaskedLM", "model_type": "roberta", "task": "fill-mask", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "hyperonym/xlm-roberta-longformer-base-16384", "arch": "LongformerModel", "model_type": "longformer", "task": "feature-extraction", "layers": "input pad embedding add add layernorm dropout transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm add layernorm transpose linear div view transpose type_as masked_fill new_ones transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm __getitem__ __getitem__ linear tanh output"}, {"repo_name": "ppsingh/action-policy-plans-classifier", "arch": "MPNetForSequenceClassification", "model_type": "mpnet", "task": "text-classification", "layers": "input to to ne int type_as mul long add embedding add layernorm dropout to to to add layernorm to add layernorm to to add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "morenolq/bart-it-fanpage", "arch": "BartForConditionalGeneration", "model_type": "bart", "task": "text2text-generation", "layers": "input to to bartlearnedpositionalembedding to add layernorm dropout to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any to add layernorm add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any to add layernorm add layernorm isnan any to add layernorm add layernorm to output"}, {"repo_name": "jjonhwa/BIG-1125", "arch": "BertForV2QuestionAnswering", "model_type": "big_bird", "task": "unknown", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "Ibrahim-Alam/finetuning-xlnet-base-cased-on-imdb", "arch": "XLNetForSequenceClassification", "model_type": "xlnet", "task": "text-classification", "layers": "input transpose contiguous embedding dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "bofenghuang/whisper-large-v2-french", "arch": "WhisperForConditionalGeneration", "model_type": "whisper", "task": "automatic-speech-recognition", "layers": "input view whisperpositionalembedding embedding add dropout layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add add add layernorm linear geluactivation dropout linear dropout add add add add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm output"}, {"repo_name": "ireneli1024/bigbird-pegasus-large-pubmed-plos-finetuned", "arch": "BigBirdPegasusForConditionalGeneration", "model_type": "bigbird_pegasus", "task": "text2text-generation", "layers": "input to to view bigbirdpegasusscaledwordembedding add dropout to add add isinf any to add add isinf any to add add isinf any to add add isinf any to add add isinf any to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any to add add isinf any to add add isinf any to add add isinf any to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any to add add isinf any to add add isinf any isnan any to add add isinf any to add add isinf any to add add isinf any to layernorm to linear4bit view permute matmul div softmax dropout matmul permute contiguous view linear4bit dropout add add isinf any isnan any layernorm to output"}, {"repo_name": "eleldar/language-detection", "arch": "XLMRobertaForSequenceClassification", "model_type": "xlm-roberta", "task": "text-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "bongsoo/albert-small-kor-sbert-v1.1", "arch": "AlbertModel", "model_type": "albert", "task": "sentence-similarity", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to output"}, {"repo_name": "klue/roberta-large", "arch": "RobertaForMaskedLM", "model_type": "roberta", "task": "fill-mask", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "pszemraj/led-large-book-summary", "arch": "LEDForConditionalGeneration", "model_type": "led", "task": "text2text-generation", "layers": "input pad view embedding new_full embedding cat add layernorm dropout transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm add layernorm transpose linear view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm add layernorm transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm add layernorm transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm add layernorm transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm __getitem__ linear view transpose contiguous output"}, {"repo_name": "Bingsu/clip-vit-large-patch14-ko", "arch": "CLIPModel", "model_type": "clip", "task": "zero-shot-image-classification", "layers": "input to to to to conv2d flatten transpose cat add layernorm to to layernorm to linear4bit view transpose contiguous view bmm view transpose reshape linear4bit add add to add add to layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit add add to add add to add layernorm to linear4bit quickgeluactivation linear4bit add to add layernorm to linear4bit quickgeluactivation linear4bit add to add add to layernorm to linear4bit view transpose contiguous view transpose bmm softmax dropout bmm view transpose reshape linear4bit add add to add add to add add to add add to add add to layernorm to linear4bit view transpose contiguous view bmm view transpose reshape linear4bit add layernorm to linear4bit quickgeluactivation linear4bit add to add layernorm to linear4bit quickgeluactivation linear4bit add to layernorm to linear4bit view transpose contiguous view transpose bmm softmax dropout bmm view transpose reshape linear4bit add layernorm to linear4bit quickgeluactivation linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit add add to layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit add add to layernorm to linear4bit view transpose contiguous view transpose bmm softmax dropout bmm view transpose reshape linear4bit add add to add layernorm to linear4bit quickgeluactivation linear4bit add to add add to add add to add add to add add to add add to output"}, {"repo_name": "sofa566/my_awesome_swag_model", "arch": "BertForMultipleChoice", "model_type": "bert", "task": "multiple-choice", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "acmc/summarizer_google_bigbird-pegasus-large-pubmed_base_faceted", "arch": "BigBirdPegasusForConditionalGeneration", "model_type": "bigbird_pegasus", "task": "text2text-generation", "layers": "input to to view bigbirdpegasusscaledwordembedding add dropout to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any isnan any to add add isinf any to add add isinf any isnan any to add add isinf any to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any to add add isinf any isnan any to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any isnan any to add add isinf any to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add isinf any to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add isinf any to add add isinf any isnan any to add add isinf any to add add isinf any to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any isnan any to layernorm to linear4bit view permute matmul div softmax dropout matmul permute contiguous view linear4bit dropout add add isinf any isnan any to add add isinf any isnan any layernorm to output"}, {"repo_name": "SEBIS/code_trans_t5_large_code_documentation_generation_python_transfer_learning_finetune", "arch": "T5Model", "model_type": "t5", "task": "summarization", "layers": "input to to embedding to mistralrmsnorm to linear4bit view transpose __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose output"}, {"repo_name": "sentence-transformers/msmarco-distilbert-base-v2", "arch": "DistilBertModel", "model_type": "distilbert", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout linear view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "bhavikardeshna/xlm-roberta-base-arabic", "arch": "XLMRobertaForQuestionAnswering", "model_type": "xlm-roberta", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "IT-community/BART_cnn_news_text_classification", "arch": "BartForSequenceClassification", "model_type": "bart", "task": "text-classification", "layers": "input to to view bartscaledwordembedding add layernorm dropout to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "zohaib99k/Bert_Arabic-SQuADv2-QA", "arch": "ElectraForQuestionAnswering", "model_type": "electra", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "textattack/albert-base-v2-rotten_tomatoes", "arch": "AlbertForMaskedLM", "model_type": "albert", "task": "fill-mask", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm output"}, {"repo_name": "vuiseng9/bert-base-uncased-squadv1-65.1-sparse", "arch": "NNCFNetwork", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "DarrenLo/blenderbot_pal", "arch": "BlenderbotForConditionalGeneration", "model_type": "blenderbot", "task": "text2text-generation", "layers": "input to to __getitem__ view blenderbotscaledwordembedding add dropout to layernorm to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add add isnan any layernorm to output"}, {"repo_name": "pszemraj/long-t5-tglobal-base-sci-simplify", "arch": "LongT5ForConditionalGeneration", "model_type": "longt5", "task": "text2text-generation", "layers": "input view embedding dropout add add longt5layernorm linear view pad reshape pad __getitem__ cat cat einsum contiguous view __getitem__ linear dropout add add add add add add add add add add add add longt5layernorm linear view pad reshape pad __getitem__ cat cat einsum contiguous view __getitem__ linear dropout add add add add longt5layernorm linear view pad reshape pad __getitem__ cat cat einsum contiguous view __getitem__ linear dropout add add longt5layernorm linear view pad reshape pad __getitem__ cat cat einsum contiguous view __getitem__ linear dropout add add add add longt5layernorm dropout output"}, {"repo_name": "llm-book/bert-base-japanese-v3-bpr-passage-aio", "arch": "BertModel", "model_type": "bert", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "nagupv/deberta-v3-large-hf-weights_v4_f0", "arch": "DebertaV2ForMultipleChoice", "model_type": "deberta-v2", "task": "multiple-choice", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "Gladiator/microsoft-deberta-v3-large_ner_conll2003", "arch": "DebertaV2ForTokenClassification", "model_type": "deberta-v2", "task": "token-classification", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "ku-nlp/deberta-v2-base-japanese", "arch": "DebertaV2ForMaskedLM", "model_type": "deberta-v2", "task": "fill-mask", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to permute contiguous conv1d permute contiguous masked_fill stabledropout geluactivation add to mul to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to output"}, {"repo_name": "Tverous/sft-trl", "arch": "GPTJForCausalLM", "model_type": "gptj", "task": "text-generation", "layers": "input to view embedding dropout to add to layernorm to linear4bit view __getitem__ cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit view __getitem__ cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add layernorm view to output"}, {"repo_name": "KarelDO/roberta-base.CEBaB_confounding.uniform.absa.5-class.seed_43", "arch": "RobertaForNonlinearSequenceClassification", "model_type": "roberta", "task": "unknown", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "leonweber/electra_small", "arch": "ElectraModel", "model_type": "electra", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "joeddav/bart-large-mnli-yahoo-answers", "arch": "BartForSequenceClassification", "model_type": "bart", "task": "zero-shot-classification", "layers": "input bartlearnedpositionalembedding to add layernorm dropout linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous output"}, {"repo_name": "gokuls/mobilebert_add_GLUE_Experiment_logit_kd_mnli_256", "arch": "MobileBertForSequenceClassification", "model_type": "mobilebert", "task": "text-classification", "layers": "input to to embedding cat linear4bit add add nonorm dropout to to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm mobilebertpooler to output"}, {"repo_name": "facebook/xglm-4.5B", "arch": "XGLMForCausalLM", "model_type": "xglm", "task": "text-generation", "layers": "input view embedding mul add dropout layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add add add add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add add add add add layernorm linear relu dropout linear dropout add add add add add add add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add layernorm linear relu dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add add add add add add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add layernorm linear relu dropout linear dropout add add add add layernorm linear relu dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add add add layernorm output"}, {"repo_name": "mahaswec/setfit_ostrom", "arch": "MPNetModel", "model_type": "mpnet", "task": "text-classification", "layers": "input to to embedding add layernorm dropout to to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to add layernorm to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "Rostlab/prot_t5_xl_bfd", "arch": "T5WithLMHeadModel", "model_type": "t5", "task": "text2text-generation", "layers": "input to to embedding to mistralrmsnorm to linear4bit view transpose __getitem__ cat mul add output"}, {"repo_name": "sentence-transformers/distilroberta-base-msmarco-v2", "arch": "RobertaModel", "model_type": "roberta", "task": "sentence-similarity", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "CAMeL-Lab/bert-base-arabic-camelbert-da", "arch": "BertForMaskedLM", "model_type": "bert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "sentence-transformers/multi-qa-distilbert-cos-v1", "arch": "DistilBertForMaskedLM", "model_type": "distilbert", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout linear view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "dhananjay2912/lsg-bart-base-4096-mediqa-chat-taskb", "arch": "LSGBartForConditionalGeneration", "model_type": "bart", "task": "text2text-generation", "layers": "input to to bartlearnedpositionalembedding to add layernorm dropout to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any isinf any to to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to output"}, {"repo_name": "nateraw/convnext-tiny-224-finetuned-eurosat-albumentations", "arch": "ConvNextForImageClassification", "model_type": "convnext", "task": "image-classification", "layers": "input conv2d convnextlayernorm add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d add add add add add add add add add convnextlayernorm conv2d add add add mean layernorm output"}, {"repo_name": "sentence-transformers/msmarco-roberta-base-v3", "arch": "RobertaModel", "model_type": "roberta", "task": "sentence-similarity", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "roneneldan/TinyStories-28M", "arch": "GPTNeoForCausalLM", "model_type": "gpt_neo", "task": "text-generation", "layers": "input view embedding add dropout layernorm linear view permute matmul permute contiguous view linear dropout add add add add add add add add add add add add add add layernorm linear view permute matmul permute contiguous view linear dropout add add layernorm view output"}, {"repo_name": "Helsinki-NLP/opus-mt-iir-en", "arch": "MarianMTModel", "model_type": "marian", "task": "translation", "layers": "input to to view embedding mul add dropout to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to to to linear4bit view transpose contiguous to output"}, {"repo_name": "bigcode/gpt_bigcode-santacoder", "arch": "GPTBigCodeForCausalLM", "model_type": "gpt_bigcode", "task": "text-generation", "layers": "input to view embedding add dropout to layernorm to linear4bit split output"}, {"repo_name": "KoboldAI/fairseq-dense-6.7B-Janeway", "arch": "XGLMForCausalLM", "model_type": "xglm", "task": "text-generation", "layers": "input view embedding mul add dropout layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous view transpose bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm output"}, {"repo_name": "voidful/albert_chinese_base", "arch": "AlbertForMaskedLM", "model_type": "albert", "task": "fill-mask", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm linear relu linear add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm linear relu linear add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm linear relu linear add layernorm add layernorm add layernorm add layernorm add layernorm output"}, {"repo_name": "RWKV/rwkv-4-430m-pile", "arch": "RwkvForCausalLM", "model_type": "rwkv", "task": "text-generation", "layers": "input to embedding to layernorm add layernorm to __getitem__ mul add linear4bit sigmoid mul add to layernorm to __getitem__ mul add linear4bit __getitem__ mul add mul add mul add div to mul add div to __getitem__ mul mul mul add linear4bit __getitem__ float maximum sub exp add sub exp mul add sub exp add add maximum sub exp mul __getitem__ float maximum sub exp mul mul add add add sub exp add sub exp add add maximum sub exp mul mul zeros_like mul add linear4bit sigmoid mul linear4bit add add to layernorm to __getitem__ mul add linear4bit sigmoid mul linear4bit add add to add layernorm to __getitem__ zeropad2d mul add linear4bit relu square linear4bit mul add to add add to layernorm to __getitem__ mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ zeropad2d mul add linear4bit relu square linear4bit mul add div to layernorm to __getitem__ mul add linear4bit sigmoid mul linear4bit add add to layernorm to __getitem__ mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ zeropad2d mul add linear4bit relu square linear4bit mul add to add add to add add to layernorm to __getitem__ mul add linear4bit sigmoid mul linear4bit add add to layernorm to __getitem__ zeropad2d mul add linear4bit __getitem__ mul add mul add mul add div to mul add div to __getitem__ mul mul mul add linear4bit sigmoid mul linear4bit add add div to add add to layernorm to __getitem__ mul add linear4bit __getitem__ mul add mul add mul add div to mul add div to __getitem__ mul mul mul add linear4bit __getitem__ float maximum sub exp add sub exp mul add sub exp add add maximum sub exp mul __getitem__ float maximum sub exp mul mul add add add sub exp add sub exp add add maximum sub exp mul mul zeros_like zeropad2d mul mul mul add linear4bit sigmoid mul linear4bit add add to add layernorm to __getitem__ zeropad2d mul add linear4bit sigmoid mul add to layernorm to __getitem__ mul add linear4bit sigmoid mul linear4bit add add to add add to add add div to add add to add add to add add to add layernorm to __getitem__ mul add linear4bit relu square linear4bit mul add to add add to add add div layernorm to output"}, {"repo_name": "facebook/regnet-y-004", "arch": "RegNetForImageClassification", "model_type": "regnet", "task": "image-classification", "layers": "input conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu adaptiveavgpool2d conv2d relu conv2d sigmoid mul conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu adaptiveavgpool2d conv2d relu conv2d sigmoid mul conv2d batchnorm2d add relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu adaptiveavgpool2d conv2d relu conv2d sigmoid mul conv2d batchnorm2d add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu adaptiveavgpool2d conv2d relu conv2d sigmoid mul conv2d batchnorm2d add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu mul conv2d batchnorm2d add relu add relu add relu add relu add relu add relu adaptiveavgpool2d output"}, {"repo_name": "mazkooleg/0-9up-wavlm-base-plus-ft", "arch": "WavLMForSequenceClassification", "model_type": "wavlm", "task": "audio-classification", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "HJOK/task2_deberta_spamMLM_v1", "arch": "DebertaV2Model", "model_type": "deberta-v2", "task": "feature-extraction", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to add to mul to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "hku-nlp/instructor-large", "arch": "T5EncoderModel", "model_type": "t5", "task": "sentence-similarity", "layers": "input to to embedding to mistralrmsnorm to linear4bit view transpose mul add output"}, {"repo_name": "3gg/all-mpnet-base-v2", "arch": "MPNetForMaskedLM", "model_type": "mpnet", "task": "sentence-similarity", "layers": "input to __getitem__ to embedding add layernorm dropout to to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "voidful/dpr-ctx_encoder-bert-base-multilingual", "arch": "DPRContextEncoder", "model_type": "dpr", "task": "unknown", "layers": "input to to to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to output"}, {"repo_name": "Sjdan/cls_asr_5", "arch": "Wav2Vec2ForSpeechClassification", "model_type": "wav2vec2", "task": "unknown", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "YuJungSoo/kobigbird-base26-46196128", "arch": "BigBirdForQuestionAnswering", "model_type": "big_bird", "task": "question-answering", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "uclanlp/plbart-python-en_XX", "arch": "PLBartForConditionalGeneration", "model_type": "plbart", "task": "text2text-generation", "layers": "input to to view plbartscaledwordembedding add layernorm dropout to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm add layernorm isnan any to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "vinid/plip", "arch": "CLIPModel", "model_type": "clip", "task": "zero-shot-image-classification", "layers": "input to to view to embedding add to to layernorm to linear4bit view transpose contiguous view bmm view transpose reshape linear4bit add layernorm to linear4bit quickgeluactivation linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit add add to layernorm to linear4bit view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit add layernorm to linear4bit quickgeluactivation linear4bit add to layernorm to linear4bit view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit add add to layernorm to linear4bit view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit add add to layernorm to linear4bit view transpose contiguous view bmm view transpose reshape linear4bit add layernorm to linear4bit quickgeluactivation linear4bit add to add layernorm to linear4bit quickgeluactivation linear4bit add to layernorm to linear4bit view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit add add to layernorm to linear4bit view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit add layernorm to linear4bit quickgeluactivation linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit add layernorm to linear4bit quickgeluactivation linear4bit add to layernorm to linear4bit view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit add layernorm to linear4bit quickgeluactivation linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit add layernorm to linear4bit quickgeluactivation linear4bit add layernorm to output"}, {"repo_name": "907508196l/wavlm-libri-clean-100h-base-plus-finetuned-ks", "arch": "WavLMForSequenceClassification", "model_type": "wavlm", "task": "audio-classification", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition", "arch": "Wav2Vec2ForSequenceClassification", "model_type": "wav2vec2", "task": "audio-classification", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "vatsalinfodesk/pegasus-samsum", "arch": "BigBirdPegasusForConditionalGeneration", "model_type": "bigbird_pegasus", "task": "text2text-generation", "layers": "input to to view bigbirdpegasusscaledwordembedding add dropout to add add isinf any isnan any to add add isinf any to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add isinf any to add add isinf any to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any isnan any to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add isinf any isnan any to add add isinf any isnan any to add add isinf any isnan any to add add isinf any to layernorm to linear4bit view permute transpose matmul div softmax dropout matmul permute contiguous view linear4bit dropout add add isinf any to add add isinf any to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any isnan any to add add isinf any to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add isinf any layernorm to output"}, {"repo_name": "hf-internal-testing/tiny-random-XGLMForCausalLM", "arch": "XGLMForCausalLM", "model_type": "xglm", "task": "text-generation", "layers": "input view embedding mul add dropout layernorm linear view transpose contiguous output"}, {"repo_name": "NtDNlp/sentence-embedding-vietnamese", "arch": "XLMRobertaModel", "model_type": "xlm-roberta", "task": "feature-extraction", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "ynie/bart-large-snli_mnli_fever_anli_R1_R2_R3-nli", "arch": "BartForSequenceClassification", "model_type": "bart", "task": "text-classification", "layers": "input bartlearnedpositionalembedding to add layernorm dropout linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous output"}, {"repo_name": "sjdata/speecht5_finetuned_common_voice_11_de", "arch": "SpeechT5ForTextToSpeech", "model_type": "speecht5", "task": "text-to-speech", "layers": "input view speecht5sinusoidalpositionalembedding add dropout linear view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear output"}, {"repo_name": "hf-internal-testing/tiny-random-DebertaV2ForMaskedLM", "arch": "DebertaV2ForMaskedLM", "model_type": "deberta-v2", "task": "fill-mask", "layers": "input to to embedding add add layernorm mul stabledropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to output"}, {"repo_name": "breadlicker45/music-rwkv2-v4", "arch": "RwkvForCausalLM", "model_type": "rwkv", "task": "text-generation", "layers": "input to embedding to layernorm layernorm to zeropad2d mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit sigmoid mul add to layernorm to zeropad2d mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add div to mul add mul mul mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit sigmoid mul add to layernorm to zeropad2d mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit relu square linear4bit mul add to layernorm to zeropad2d mul add linear4bit zeros_like __getitem__ float sub exp mul add add maximum sub exp mul mul add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float maximum sub exp mul add mul add sub exp mul add add add maximum sub exp mul add div to mul add sub exp mul add mul add linear4bit __getitem__ __getitem__ mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit relu square linear4bit mul add to add layernorm to zeropad2d mul add linear4bit sigmoid mul add to layernorm to zeropad2d mul add linear4bit zeros_like __getitem__ float sub exp mul add add maximum sub exp mul mul add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float maximum sub exp mul add mul add sub exp mul add add add maximum sub exp mul add div to mul add sub exp mul add mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit sigmoid mul add div to add add to layernorm to zeropad2d mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit relu square linear4bit mul add to layernorm to zeropad2d mul add linear4bit zeros_like __getitem__ float sub exp mul add add maximum sub exp mul mul add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float maximum sub exp mul add mul add sub exp mul add add add maximum sub exp mul add div to mul add sub exp mul add mul add linear4bit __getitem__ __getitem__ mul add linear4bit sigmoid mul linear4bit add add to layernorm to zeropad2d mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add div to mul add mul mul mul add linear4bit sigmoid mul linear4bit add add to layernorm to zeropad2d mul add linear4bit zeros_like __getitem__ float sub exp mul add add maximum sub exp mul mul add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float maximum sub exp mul add mul add sub exp mul add add add maximum sub exp mul add div to mul add sub exp mul add mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit sigmoid mul add to layernorm to zeropad2d mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit sigmoid mul add div to layernorm to zeropad2d mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add div to mul add mul mul mul add linear4bit zeros_like __getitem__ float sub exp add maximum sub exp mul add maximum sub exp mul add sub exp add __getitem__ float maximum sub exp mul mul add sub exp add add add maximum sub exp mul mul add sub exp add mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit sigmoid mul add to layernorm to zeropad2d mul add linear4bit zeros_like __getitem__ float sub exp mul add add maximum sub exp mul mul add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float maximum sub exp mul add mul add sub exp mul add add add maximum sub exp mul add div to mul add sub exp mul add mul add linear4bit __getitem__ __getitem__ mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit relu square linear4bit mul add to layernorm to zeropad2d mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit relu square linear4bit mul add to layernorm to zeropad2d mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit sigmoid mul add to add layernorm to zeropad2d mul add linear4bit relu square linear4bit mul add to layernorm to zeropad2d mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add div to mul add mul mul mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit relu square linear4bit mul add div to add layernorm to zeropad2d mul add linear4bit relu square linear4bit mul add to layernorm to zeropad2d mul add linear4bit zeros_like __getitem__ float sub exp mul add add maximum sub exp mul mul add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float maximum sub exp mul add mul add sub exp mul add add add maximum sub exp mul add div to mul add sub exp mul add mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit sigmoid mul add to layernorm to zeropad2d mul add linear4bit zeros_like __getitem__ float sub exp mul add add maximum sub exp mul mul add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float maximum sub exp mul add mul add sub exp mul add add add maximum sub exp mul add div to mul add sub exp mul add mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit relu square linear4bit mul add to layernorm to zeropad2d mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit sigmoid mul add to layernorm to zeropad2d mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit sigmoid mul add to layernorm to zeropad2d mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add div to mul add mul mul mul add linear4bit zeros_like __getitem__ float sub exp add maximum sub exp mul add maximum sub exp mul add sub exp add __getitem__ float maximum sub exp mul mul add sub exp add add add maximum sub exp mul mul add sub exp add mul add linear4bit sigmoid mul linear4bit add layernorm to zeropad2d mul add linear4bit relu square linear4bit mul add div layernorm to output"}, {"repo_name": "joefox/mbart-large-ru-zh-ru-many-to-many-mmt", "arch": "MBartForConditionalGeneration", "model_type": "mbart", "task": "translation", "layers": "input mbartlearnedpositionalembedding to add layernorm dropout layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add add layernorm linear relu dropout linear dropout add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add add layernorm linear relu dropout linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm output"}, {"repo_name": "HuggingFaceH4/tiny-random-LlamaForSeqClass", "arch": "LlamaForSequenceClassification", "model_type": "llama", "task": "text-classification", "layers": "input to embedding to add llamarmsnorm to linear4bit mul linear4bit add to add llamarmsnorm to linear4bit mul linear4bit add llamarmsnorm to output"}, {"repo_name": "hf-internal-testing/tiny-random-MobileNetV2ForImageClassification", "arch": "MobileNetV2ForImageClassification", "model_type": "mobilenet_v2", "task": "image-classification", "layers": "input pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add add add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 adaptiveavgpool2d flatten output"}, {"repo_name": "joheras/clinico-bsc-bio-ehr-es-longformer", "arch": "LongformerForTokenClassification", "model_type": "longformer", "task": "token-classification", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "jjonhwa/BIG-2125", "arch": "BertForV2QuestionAnswering", "model_type": "big_bird", "task": "unknown", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "ydshieh/roberta-large-ner-english", "arch": "RobertaForTokenClassification", "model_type": "roberta", "task": "token-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "debarshi-kundu-94/esm2_t12_35M_UR50D-finetuned-secondary-structure", "arch": "EsmForTokenClassification", "model_type": "esm", "task": "token-classification", "layers": "input to to embedding masked_fill mul div to mul to to to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add to add to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add to add to to to add to add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add to add to to layernorm to linear4bit view permute mul rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add to add to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add to add to to to add to add to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute mul rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add to add to to layernorm to linear4bit view permute rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add layernorm to output"}, {"repo_name": "uisikdag/weed_resnet_imbalanced", "arch": "ResNetForImageClassification", "model_type": "resnet", "task": "image-classification", "layers": "input conv2d batchnorm2d relu maxpool2d conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d add relu add relu add relu add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu add relu add relu add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu output"}, {"repo_name": "albert-xlarge-v2", "arch": "AlbertForMaskedLM", "model_type": "albert", "task": "fill-mask", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm linear newgeluactivation linear add layernorm linear view permute matmul transpose flatten linear dropout add layernorm linear newgeluactivation linear add layernorm add layernorm linear newgeluactivation linear add layernorm add layernorm linear newgeluactivation linear add layernorm linear view permute matmul transpose flatten linear dropout add layernorm linear newgeluactivation linear add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm linear newgeluactivation linear add layernorm linear view permute matmul transpose flatten linear dropout add layernorm linear newgeluactivation linear add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm linear newgeluactivation linear add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm linear newgeluactivation linear add layernorm linear view permute matmul transpose flatten linear dropout add layernorm linear newgeluactivation linear add layernorm output"}, {"repo_name": "izumi-lab/electra-small-paper-japanese-fin-generator", "arch": "ElectraForMaskedLM", "model_type": "electra", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "agkphysics/wav2vec2-large-xlsr-53-amharic", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "efederici/sentence-BERTino", "arch": "DistilBertModel", "model_type": "distilbert", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout output"}, {"repo_name": "gokuls/mobilebert_sa_GLUE_Experiment_logit_kd_data_aug_mnli_256", "arch": "MobileBertForSequenceClassification", "model_type": "mobilebert", "task": "text-classification", "layers": "input to to embedding __getitem__ pad cat linear4bit add add nonorm dropout to to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm mobilebertpooler to output"}, {"repo_name": "emrecicekyurt/esm2_t12_35M_UR50D-finetuned-secondary-structure", "arch": "EsmForTokenClassification", "model_type": "esm", "task": "token-classification", "layers": "input to to ne int cumsum type_as add mul long add eq unsqueeze masked_fill mul div to mul to to to to layernorm to linear4bit view permute mul rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add to add to to layernorm to linear4bit view permute rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add to add to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute mul rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute mul rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add to add layernorm to output"}, {"repo_name": "google/multiberts-seed_3-step_200k", "arch": "BertForPreTraining", "model_type": "bert", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "ilos-vigil/bigbird-small-indonesian", "arch": "BigBirdForMaskedLM", "model_type": "big_bird", "task": "fill-mask", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm __getitem__ linear tanh to output"}, {"repo_name": "microsoft/deberta-xlarge-mnli", "arch": "DebertaForSequenceClassification", "model_type": "deberta", "task": "text-classification", "layers": "input embedding zeros_like debertalayernorm mul stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm linear view permute chunk transpose matmul add masked_fill softmax masked_fill stabledropout matmul permute contiguous view linear stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm linear view permute chunk transpose matmul add masked_fill softmax masked_fill stabledropout matmul permute contiguous view linear stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm output"}, {"repo_name": "eclat12450/fine-tuned-NSPbert-14", "arch": "BertForNextSentencePrediction", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "PrimeQA/XOR-TyDi_monolingual_DPR_ctx_encoder", "arch": "DPRContextEncoder", "model_type": "dpr", "task": "unknown", "layers": "input to to to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to output"}, {"repo_name": "cartesinus/iva_mt_wslot-m2m100_418M-en-de", "arch": "M2M100ForConditionalGeneration", "model_type": "m2m_100", "task": "translation", "layers": "input to to __getitem__ view m2m100scaledwordembedding m2m100sinusoidalpositionalembedding to add dropout to layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add isinf any isnan any to layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add isinf any isnan any to add layernorm linear4bit relu dropout linear4bit dropout add isinf any isnan any to layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add isinf any isnan any to add layernorm linear4bit relu dropout linear4bit dropout add isinf any to layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add isinf any isnan any to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add isinf any to layernorm to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add isinf any isnan any to layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add isinf any isnan any to layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add isinf any to layernorm to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add isinf any isnan any to add add layernorm to to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add to add layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add to add layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add to add add layernorm linear4bit relu dropout linear4bit dropout add to add add layernorm linear4bit relu dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add add layernorm linear4bit relu dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add add to layernorm to linear4bit view transpose contiguous to output"}, {"repo_name": "TitanML/Electra-Large-SQUADV2", "arch": "ElectraForQuestionAnswering", "model_type": "electra", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "jonatasgrosman/exp_w2v2t_nl_unispeech_s683", "arch": "UniSpeechForCTC", "model_type": "unispeech", "task": "automatic-speech-recognition", "layers": "input to cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "sasha/dog-food-convnext-tiny-224", "arch": "ConvNextForImageClassification", "model_type": "convnext", "task": "image-classification", "layers": "input conv2d convnextlayernorm add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d add add add add add add add add add convnextlayernorm conv2d add add add mean layernorm output"}, {"repo_name": "beomi/beep-KcELECTRA-base-hate", "arch": "ElectraForSequenceClassification", "model_type": "electra", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "KarelDO/lstm.CEBaB_confounding.uniform.absa.5-class.seed_43", "arch": "LSTMForSequenceClassification", "model_type": "bert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "facebook/s2t-small-mustc-en-nl-st", "arch": "Speech2TextForConditionalGeneration", "model_type": "speech_to_text", "task": "automatic-speech-recognition", "layers": "input to to view speech2textsinusoidalpositionalembedding add dropout to add add layernorm linear4bit relu dropout linear4bit dropout add to add add add to add add layernorm linear4bit relu dropout linear4bit dropout add to add add add to layernorm to linear4bit view transpose contiguous to output"}, {"repo_name": "facebook/mask2former-swin-large-cityscapes-semantic", "arch": "Mask2FormerForUniversalSegmentation", "model_type": "mask2former", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm dropout add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add view permute permute contiguous view layernorm view permute contiguous conv2d groupnorm add conv2d groupnorm relu conv2d output"}, {"repo_name": "fxmarty/tiny-random-longformer-onnxtrue", "arch": "LongformerModel", "model_type": "longformer", "task": "feature-extraction", "layers": "input to pad to ne int type_as mul long add to embedding add add layernorm dropout to to to to transpose linear4bit view transpose transpose reshape __getitem__ linear4bit div view transpose transpose reshape __getitem__ type_as masked_fill new_ones transpose reshape __getitem__ transpose reshape __getitem__ linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape __getitem__ linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "Neruoy/swin-finetuned-food101-e3", "arch": "SwinForImageClassification", "model_type": "swin", "task": "image-classification", "layers": "input conv2d flatten transpose layernorm dropout add add add add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add add add add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add add layernorm view pad view permute contiguous view view linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add add add layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add add add add add add add add layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add add add layernorm output"}, {"repo_name": "agemagician/mlong-t5-tglobal-base", "arch": "LongT5ForConditionalGeneration", "model_type": "longt5", "task": "text2text-generation", "layers": "input view embedding dropout add add add add add add add add add add add add add add add add add add add add add add add add longt5layernorm dropout output"}, {"repo_name": "monologg/koelectra-small-finetuned-naver-ner", "arch": "ElectraForTokenClassification", "model_type": "electra", "task": "token-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "Onegafer/glpn-nyu-finetuned-diode-230603-090701", "arch": "GLPNForDepthEstimation", "model_type": "glpn", "task": "depth-estimation", "layers": "input conv2d flatten transpose layernorm add add add add add add layernorm reshape permute contiguous conv2d flatten transpose layernorm layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add add add add add add add add add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm reshape permute contiguous conv2d flatten transpose layernorm add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm permute reshape conv2d reshape permute layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add add layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add add add add add layernorm permute reshape conv2d reshape permute layernorm linear view permute matmul permute contiguous view linear dropout glpndroppath add add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add add add layernorm permute reshape conv2d reshape permute layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add add add add add add add layernorm permute reshape conv2d reshape permute layernorm linear view permute matmul permute contiguous view linear dropout glpndroppath add add layernorm permute reshape conv2d reshape permute layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm permute reshape conv2d reshape permute layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add add layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout glpndroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm reshape permute contiguous conv2d flatten transpose layernorm add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add add add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout glpndroppath add layernorm reshape permute contiguous output"}, {"repo_name": "tkuye/job-description-classifier", "arch": "XLNetForSequenceClassification", "model_type": "xlnet", "task": "text-classification", "layers": "input transpose contiguous embedding dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "nadiaqutaiba/bert-base-uncased-finetuned-swag", "arch": "BertForMultipleChoice", "model_type": "bert", "task": "multiple-choice", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "GItaf/roberta-base-roberta-base-TF-weight2-epoch5", "arch": "RobertaForCausalLM", "model_type": "roberta", "task": "text-generation", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "Davlan/distilbert-base-multilingual-cased-ner-hrl", "arch": "DistilBertForTokenClassification", "model_type": "distilbert", "task": "token-classification", "layers": "input embedding add layernorm dropout linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "Methmani/ImageClassification_fashion-mnist", "arch": "MobileNetV2ForImageClassification", "model_type": "mobilenet_v2", "task": "image-classification", "layers": "input pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add add add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 output"}, {"repo_name": "ajtamayoh/Disease_Identification_RoBERTa_fine_tuned_Testing", "arch": "RobertaForTokenClassification", "model_type": "roberta", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "ffgcc/esimcse-bert-base-uncased", "arch": "BertForCL", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "pmpc/xlm-roberta-base2longformer-8192", "arch": "LongformerModel", "model_type": "longformer", "task": "feature-extraction", "layers": "input pad ne int cumsum type_as mul long add to embedding add add layernorm dropout transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm add layernorm transpose linear div view transpose type_as masked_fill new_ones transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm __getitem__ __getitem__ linear tanh output"}, {"repo_name": "fxmarty/resnet-tiny-beans", "arch": "ResNetForImageClassification", "model_type": "resnet", "task": "image-classification", "layers": "input conv2d batchnorm2d relu maxpool2d conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d add relu adaptiveavgpool2d output"}, {"repo_name": "mbeukman/xlm-roberta-base-finetuned-ner-luo", "arch": "XLMRobertaForTokenClassification", "model_type": "xlm-roberta", "task": "token-classification", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "akdeniz27/mDeBERTa-v3-base-turkish-ner", "arch": "DebertaV2ForTokenClassification", "model_type": "deberta-v2", "task": "token-classification", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "veronica320/TE-for-Event-Extraction", "arch": "RobertaForSequenceClassification", "model_type": "roberta", "task": "text-classification", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "spacemanidol/esci-us-base-mpnet-crossencoder", "arch": "MPNetForSequenceClassification", "model_type": "mpnet", "task": "text-classification", "layers": "input to to ne int type_as mul long add embedding add layernorm dropout to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "ehdwns1516/bert-base-uncased_SWAG", "arch": "BertForMultipleChoice", "model_type": "bert", "task": "multiple-choice", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "bookbot/distil-ast-audioset", "arch": "ASTForAudioClassification", "model_type": "audio-spectrogram-transformer", "task": "audio-classification", "layers": "input unsqueeze transpose conv2d flatten transpose cat add dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add add add add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add layernorm output"}, {"repo_name": "sshleifer/tiny-distilbert-base-cased-distilled-squad", "arch": "DistilBertForQuestionAnswering", "model_type": "distilbert", "task": "question-answering", "layers": "input to to embedding add layernorm dropout to to to to to linear4bit view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to output"}, {"repo_name": "Alireza1044/albert-base-v2-rte", "arch": "AlbertForSequenceClassification", "model_type": "albert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to add layernorm add layernorm to to to add layernorm add layernorm to to to add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to add layernorm add layernorm to to to add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to output"}, {"repo_name": "m3hrdadfi/wav2vec2-base-100k-eating-sound-collection", "arch": "Wav2Vec2ForSpeechClassification", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input __getitem__ conv1d groupnorm geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation transpose layernorm output"}, {"repo_name": "OpenAssistant/reward-model-electra-large-discriminator", "arch": "ElectraForSequenceClassification", "model_type": "electra", "task": "text-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "jcr987/camembert-finetuned-ner", "arch": "CamembertForTokenClassification", "model_type": "camembert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "CleverShovel/rubert-tiny2-tnved-v3", "arch": "BertForPreTraining", "model_type": "bert", "task": "pretraining", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm __getitem__ linear tanh output"}, {"repo_name": "boronbrown48/wangchanberta-topic-classification", "arch": "CamembertForSequenceClassification", "model_type": "camembert", "task": "text-classification", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "dlicari/Italian-Legal-BERT-SC", "arch": "CamembertForMaskedLM", "model_type": "camembert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "Wimflorijn/t5-text2text", "arch": "T5Model", "model_type": "t5", "task": "feature-extraction", "layers": "input to to embedding to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to add mistralrmsnorm to linear4bit silu mul linear4bit add to add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to add mistralrmsnorm to linear4bit silu mul linear4bit add to add add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to add add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to add add to mistralrmsnorm to linear4bit view transpose mistralrotaryembedding __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose mul add __getitem__ expand reshape scaled_dot_product_attention transpose contiguous view linear4bit add mistralrmsnorm to linear4bit silu mul linear4bit add mistralrmsnorm linear float to output"}, {"repo_name": "Aniemore/wavlm-emotion-russian-resd", "arch": "WavLMForSequenceClassification", "model_type": "wavlm", "task": "audio-classification", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "jayanthspratap/resnet-50-drfx-surgery-classifier", "arch": "ResNetForImageClassification", "model_type": "resnet", "task": "image-classification", "layers": "input conv2d batchnorm2d relu maxpool2d conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d add relu add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu output"}, {"repo_name": "Jeevesh8/bert_ft_qqp_6ep-94", "arch": "BertForSequenceClassification", "model_type": "bert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "DI-Huy/hustvl-yolos-tiny-benetech", "arch": "YolosForObjectDetection", "model_type": "yolos", "task": "object-detection", "layers": "input conv2d flatten transpose cat add dropout add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "brad1141/baseline_longformerv1", "arch": "LongformerForTokenClassification", "model_type": "longformer", "task": "token-classification", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "shivarama23/DiT_image_quality", "arch": "BeitForImageClassification", "model_type": "beit", "task": "image-classification", "layers": "input conv2d add flatten transpose cat dropout add add add add add add add add add add add add add add add add add add add add add add add add output"}, {"repo_name": "mohsenfayyaz/albert-fa-base-v2_pquad_and_persian_qa", "arch": "AlbertForQuestionAnswering", "model_type": "albert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to add layernorm add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm __getitem__ linear tanh to output"}, {"repo_name": "hf-tiny-model-private/tiny-random-BertForMultipleChoice", "arch": "BertForMultipleChoice", "model_type": "bert", "task": "multiple-choice", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "sudong97/kobigbird-base24-6832218", "arch": "BigBirdForQuestionAnswering", "model_type": "big_bird", "task": "question-answering", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "dariowsz/speecht5-base-finetuned-lj-speech", "arch": "SpeechT5ForTextToSpeech", "model_type": "speecht5", "task": "text-to-speech", "layers": "input __getitem__ conv1d groupnorm geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation transpose layernorm linear dropout transpose conv1d speecht5samepadlayer geluactivation transpose add add layernorm dropout linear view transpose contiguous view transpose bmm add softmax dropout bmm view transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous view transpose bmm add softmax dropout bmm view transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm add softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm add softmax dropout bmm view transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous view transpose bmm add softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view contiguous view transpose matmul transpose view add softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view contiguous view transpose matmul transpose view add softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous view transpose bmm add softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm add softmax dropout bmm view transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous view transpose bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm linear output"}, {"repo_name": "sureshs/distilbert-large-sms-spam", "arch": "DistilBertForSequenceClassification", "model_type": "distilbert", "task": "text-classification", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "baptiste-pasquier/distilcamembert-allocine", "arch": "CamembertForSequenceClassification", "model_type": "camembert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "mrizalf7/xlm-r-qa-small-squad-2.0", "arch": "XLMRobertaForQuestionAnswering", "model_type": "xlm-roberta", "task": "question-answering", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "facebook/regnet-y-006", "arch": "RegNetForImageClassification", "model_type": "regnet", "task": "image-classification", "layers": "input conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu mul conv2d batchnorm2d add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu mul conv2d batchnorm2d add relu add relu add relu add relu add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu adaptiveavgpool2d conv2d relu conv2d sigmoid mul conv2d batchnorm2d add relu add relu add relu add relu adaptiveavgpool2d output"}, {"repo_name": "cardiffnlp/camembert-base-tweet-sentiment-fr", "arch": "CamembertForSequenceClassification", "model_type": "camembert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "microsoft/swin-tiny-patch4-window7-224", "arch": "SwinForImageClassification", "model_type": "swin", "task": "image-classification", "layers": "input conv2d flatten transpose layernorm dropout layernorm view pad view permute contiguous view view linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add add add layernorm linear geluactivation linear dropout add add add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "sentence-transformers/stsb-mpnet-base-v2", "arch": "MPNetModel", "model_type": "mpnet", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "hf-internal-testing/tiny-random-CTRLLMHeadModel", "arch": "CTRLLMHeadModel", "model_type": "ctrl", "task": "text-generation", "layers": "input view embedding mul add add dropout add layernorm linear relu linear dropout add add layernorm linear relu linear dropout add layernorm linear reshape permute matmul div add add softmax matmul permute reshape linear dropout add layernorm linear relu linear dropout add layernorm linear reshape permute matmul div add add softmax matmul permute reshape linear dropout add layernorm linear relu linear dropout add layernorm linear reshape permute matmul div add add softmax matmul permute reshape linear dropout add layernorm linear relu linear dropout add layernorm output"}, {"repo_name": "Darna/detr-5000-400-finetuned-table-detector", "arch": "DetrForObjectDetection", "model_type": "detr", "task": "object-detection", "layers": "input __getitem__ float interpolate to __getitem__ detrsinepositionembedding to __getitem__ float interpolate to __getitem__ detrsinepositionembedding to __getitem__ float interpolate to __getitem__ flatten __getitem__ expand to __rsub__ masked_fill add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm add linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm add linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm add linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm add linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm add linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm add linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm add linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm layernorm output"}, {"repo_name": "M4ycon/distilbert-base-uncased-finetuned-squad", "arch": "DistilBertForQuestionAnswering", "model_type": "distilbert", "task": "question-answering", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "facebook/s2t-medium-mustc-multilingual-st", "arch": "Speech2TextForConditionalGeneration", "model_type": "speech_to_text", "task": "automatic-speech-recognition", "layers": "input to to view speech2textsinusoidalpositionalembedding add dropout to add add add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add add add to add layernorm to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add add to add add add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add add add to add add add layernorm to output"}, {"repo_name": "osiria/distilbert-base-italian-cased", "arch": "DistilBertForMaskedLM", "model_type": "distilbert", "task": "fill-mask", "layers": "input embedding add layernorm dropout linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "nvidia/segformer-b1-finetuned-cityscapes-1024-1024", "arch": "SegformerForSemanticSegmentation", "model_type": "segformer", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm layernorm permute reshape conv2d reshape permute layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout add layernorm permute reshape conv2d reshape permute layernorm linear view permute matmul permute contiguous view linear dropout segformerdroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add layernorm reshape permute contiguous conv2d flatten transpose layernorm add add add add layernorm reshape permute contiguous conv2d flatten transpose layernorm add add add add layernorm reshape permute contiguous conv2d flatten transpose layernorm layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout segformerdroppath add add layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout segformerdroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add layernorm reshape permute contiguous output"}, {"repo_name": "jjonhwa/250", "arch": "RobertaForV2QuestionAnswering", "model_type": "roberta", "task": "unknown", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "osiria/flare-it", "arch": "XLMRobertaForMaskedLM", "model_type": "xlm-roberta", "task": "fill-mask", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "ibm/mpt-7b-instruct2", "arch": "MPTForCausalLM", "model_type": "mpt", "task": "text-generation", "layers": "input embedding add add layernorm linear chunk reshape transpose transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add layernorm linear gelu linear dropout add add add add add add add add add add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add layernorm linear chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add layernorm linear chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add add add add add add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add add add add layernorm linear gelu linear dropout add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add layernorm linear gelu linear dropout add add add add add add add add add layernorm output"}, {"repo_name": "hugger111/BlenderBot_400M_distill_falcon_redefined_first10krows", "arch": "BlenderbotForConditionalGeneration", "model_type": "blenderbot", "task": "text2text-generation", "layers": "input to to view blenderbotscaledwordembedding add dropout to layernorm to linear8bitlt mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear8bitlt dropout add layernorm linear8bitlt geluactivation dropout linear8bitlt dropout add to add add to add add to add layernorm linear8bitlt geluactivation dropout linear8bitlt dropout add to add layernorm linear8bitlt geluactivation dropout linear8bitlt dropout add to add add to layernorm to linear8bitlt view transpose contiguous to output"}, {"repo_name": "gilf/french-camembert-postag-model", "arch": "CamembertForTokenClassification", "model_type": "camembert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "microsoft/mpnet-base", "arch": "MPNetForMaskedLM", "model_type": "mpnet", "task": "fill-mask", "layers": "input to to ne int type_as mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "twigs/bigbird-pegasus-large-4096-pubmed", "arch": "BigBirdPegasusForConditionalGeneration", "model_type": "bigbird_pegasus", "task": "text2text-generation", "layers": "input to to view bigbirdpegasusscaledwordembedding add dropout to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any to add add isinf any isnan any to add add isinf any to layernorm to linear4bit view permute transpose matmul div softmax dropout matmul permute contiguous view linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any to layernorm to linear4bit view permute matmul div softmax dropout matmul permute contiguous view linear4bit dropout add add isinf any to add add isinf any to add add isinf any to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any isnan any to add layernorm linear4bit newgeluactivation linear4bit dropout add clamp to layernorm to linear4bit view permute matmul div softmax dropout matmul permute contiguous view linear4bit dropout add add clamp to add add clamp to add add clamp to add add clamp to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit dropout add clamp to add layernorm linear4bit newgeluactivation linear4bit dropout add clamp layernorm to output"}, {"repo_name": "ArBert/albert-base-v2-finetuned-ner", "arch": "AlbertForTokenClassification", "model_type": "albert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to output"}, {"repo_name": "anjulRajendraSharma/wavlm-base-libri-clean-100", "arch": "WavLMForCTC", "model_type": "wavlm", "task": "automatic-speech-recognition", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "KarelDO/roberta-base.CEBaB_confounding.food_service_positive.absa.5-class.seed_42", "arch": "RobertaForNonlinearSequenceClassification", "model_type": "roberta", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "Deopusi/model2_5000_0.5_0_40_0", "arch": "DistilBertForMultilabelSequenceClassification", "model_type": "distilbert", "task": "unknown", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "KushalRamaiya/deberta-base-finetuned-news", "arch": "DebertaForSequenceClassification", "model_type": "deberta", "task": "text-classification", "layers": "input embedding debertalayernorm mul stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm output"}, {"repo_name": "MU-Kindai/Japanese-SimCSE-BERT-base-sup", "arch": "BertModel", "model_type": "bert", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "KoichiYasuoka/deberta-large-chinese-erlangshen-ud-goeswith", "arch": "DebertaV2ForTokenClassification", "model_type": "deberta-v2", "task": "token-classification", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to permute contiguous conv1d permute contiguous masked_fill stabledropout geluactivation add to mul to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "ccdv/lsg-bart-base-16384-pubmed", "arch": "LSGBartForConditionalGeneration", "model_type": "bart", "task": "text2text-generation", "layers": "input to to view bartscaledwordembedding add layernorm dropout to add layernorm add layernorm isnan any isinf any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any isinf any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to output"}, {"repo_name": "jjonhwa/BIG-28500", "arch": "BertForV2QuestionAnswering", "model_type": "big_bird", "task": "unknown", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "crystina-z/monoELECTRA_LCE_nneg31", "arch": "ElectraForSequenceClassification", "model_type": "electra", "task": "text-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "vesteinn/DanskBERT", "arch": "XLMRobertaForMaskedLM", "model_type": "xlm-roberta", "task": "fill-mask", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "pythainlp/wangchanglm-7.5B-sft-enth", "arch": "XGLMForCausalLM", "model_type": "xglm", "task": "text-generation", "layers": "input view embedding mul add dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add max view softmax dropout bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous output"}, {"repo_name": "KarelDO/roberta-base.CEBaB_confounding.price_food_ambiance_negative.sa.5-class.seed_42", "arch": "RobertaForNonlinearSequenceClassification", "model_type": "roberta", "task": "unknown", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "TingChenChang/make-multilingual-en-zh-tw-20220825062338", "arch": "XLMRobertaModel", "model_type": "xlm-roberta", "task": "sentence-similarity", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "NAACL2022/spider-nq-question-encoder", "arch": "DPRQuestionEncoder", "model_type": "dpr", "task": "feature-extraction", "layers": "input to to to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to output"}, {"repo_name": "facebook/detr-resnet-101", "arch": "DetrForObjectDetection", "model_type": "detr", "task": "object-detection", "layers": "input __getitem__ float interpolate to __getitem__ detrsinepositionembedding to __getitem__ float interpolate to __getitem__ detrsinepositionembedding to __getitem__ float interpolate to __getitem__ flatten __getitem__ expand to __rsub__ masked_fill add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm add linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm add linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm add linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm add linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm add linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm add linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm add linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm layernorm output"}, {"repo_name": "lothritz/LuxemBERT", "arch": "BertForMaskedLM", "model_type": "bert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "schen/longformer-chinese-base-4096", "arch": "BertForPreTraining", "model_type": "bert", "task": "pretraining", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "Nattapong/ISL-wangchanberta-NER-LST20-fineTune", "arch": "CamembertForTokenClassification", "model_type": "camembert", "task": "token-classification", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "facebook/mask2former-swin-small-cityscapes-instance", "arch": "Mask2FormerForUniversalSegmentation", "model_type": "mask2former", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm dropout add add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add add add layernorm linear geluactivation linear dropout add add add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add view permute permute contiguous view layernorm view permute contiguous conv2d groupnorm flatten transpose cat linear masked_fill view split flatten transpose reshape grid_sample stack flatten mul sum view transpose contiguous linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear masked_fill view split flatten transpose reshape grid_sample stack flatten mul sum view transpose contiguous linear dropout add layernorm add layernorm linear masked_fill view split flatten transpose reshape grid_sample stack flatten mul sum view transpose contiguous linear dropout add layernorm add layernorm linear masked_fill view split flatten transpose reshape grid_sample stack flatten mul sum view transpose contiguous linear dropout add layernorm linear relu dropout linear dropout add layernorm split transpose view mask2formersinepositionembedding flatten permute add multi_head_attention_forward dropout add layernorm permute add linear mul view transpose contiguous view bmm softmax view view dropout bmm view transpose reshape linear permute dropout add layernorm add layernorm layernorm transpose linear relu linear relu linear einsum output"}, {"repo_name": "Minata/plbart-base-finetuned-src_fm_fc_ms_ff-to-test", "arch": "PLBartForConditionalGeneration", "model_type": "plbart", "task": "text2text-generation", "layers": "input to to plbartlearnedpositionalembedding to add layernorm dropout to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "Iiro/bert_reviews", "arch": "DistilBertForSequenceClassification", "model_type": "distilbert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "intanm/20230430-002-baseline-mdeberta-qa-ft-clickbait-spoiling", "arch": "DebertaV2ForQuestionAnswering", "model_type": "deberta-v2", "task": "question-answering", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to output"}, {"repo_name": "nlpaueb/sec-bert-num", "arch": "BertForPreTraining", "model_type": "bert", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "Stevenhpy/unsup-simcse-bert-large-uncased-focal", "arch": "BertForCL", "model_type": "bert", "task": "sentence-similarity", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "navteca/all-mpnet-base-v2", "arch": "MPNetForMaskedLM", "model_type": "mpnet", "task": "sentence-similarity", "layers": "input to to ne int type_as mul long add embedding add layernorm dropout to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "valhalla/longformer-base-4096-finetuned-squadv1", "arch": "LongformerForQuestionAnswering", "model_type": "longformer", "task": "question-answering", "layers": "input to pad to ne int cumsum type_as mul long add to embedding add add layernorm dropout to to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "navteca/bart-large-mnli", "arch": "BartForSequenceClassification", "model_type": "bart", "task": "zero-shot-classification", "layers": "input bartlearnedpositionalembedding to add layernorm dropout linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm add layernorm linear view transpose contiguous output"}, {"repo_name": "sumet/ast-finetuned-audioset-10-10-0.4593-finetuned-gtzan", "arch": "ASTForAudioClassification", "model_type": "audio-spectrogram-transformer", "task": "audio-classification", "layers": "input unsqueeze transpose conv2d flatten transpose cat add dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add add add add layernorm linear geluactivation linear dropout add add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add add add add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add layernorm output"}, {"repo_name": "jonatasgrosman/exp_w2v2t_pl_wavlm_s859", "arch": "WavLMForCTC", "model_type": "wavlm", "task": "automatic-speech-recognition", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "firqaaa/indo-dpr-question_encoder-multiset-base", "arch": "DPRQuestionEncoder", "model_type": "bert", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "keras-io/transformers-qa", "arch": "DistilBertForQuestionAnswering", "model_type": "distilbert", "task": "question-answering", "layers": "input embedding add layernorm dropout linear view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm linear geluactivation linear dropout add layernorm linear view transpose matmul transpose contiguous view linear add layernorm linear geluactivation linear dropout add layernorm linear view transpose matmul transpose contiguous view linear add layernorm linear geluactivation linear dropout add layernorm linear view transpose matmul transpose contiguous view linear add layernorm linear geluactivation linear dropout add layernorm linear view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm linear geluactivation linear dropout add layernorm linear view transpose matmul transpose contiguous view linear add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "benjamin/roberta-base-wechsel-swahili", "arch": "RobertaForMaskedLM", "model_type": "roberta", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "arbml/whisper-small-ar", "arch": "WhisperForConditionalGeneration", "model_type": "whisper", "task": "automatic-speech-recognition", "layers": "input to to conv1d gelu conv1d gelu permute add dropout to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add isinf any to add add isinf any to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add isinf any to add add isinf any to add add isinf any to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add isinf any to add add isinf any to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add isinf any to add add isinf any to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add isinf any to add add isinf any to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "vuiseng9/bert-base-uncased-squadv1-85.4-sparse", "arch": "NNCFNetwork", "model_type": "bert", "task": "unknown", "layers": "input eq all"}, {"repo_name": "thonyyy/pegasus_indonesian_base-finetune", "arch": "PegasusForConditionalGeneration", "model_type": "pegasus", "task": "text2text-generation", "layers": "input view embedding mul add dropout layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add add add add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add add add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add add add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add add add layernorm output"}, {"repo_name": "hf-tiny-model-private/tiny-random-BertLMHeadModel", "arch": "BertLMHeadModel", "model_type": "bert", "task": "text-generation", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute to output"}, {"repo_name": "Davlan/naija-twitter-sentiment-afriberta-large", "arch": "XLMRobertaForSequenceClassification", "model_type": "xlm-roberta", "task": "text-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "gmihaila/wav2vec2-large-xlsr-53-romanian", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to to __getitem__ to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation transpose to layernorm linear4bit dropout to add dropout to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "florentgbelidji/all-mpnet-base-v2__tweet_eval_emotion__classifier", "arch": "MPNetForSequenceClassification", "model_type": "mpnet", "task": "text-classification", "layers": "input to to embedding add layernorm dropout to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "facebook/nllb-200-3.3B", "arch": "M2M100ForConditionalGeneration", "model_type": "m2m_100", "task": "translation", "layers": "input __getitem__ view embedding mul add dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add add add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous output"}, {"repo_name": "sshleifer/distill-pegasus-xsum-16-4", "arch": "PegasusForConditionalGeneration", "model_type": "pegasus", "task": "text2text-generation", "layers": "input __getitem__ view embedding mul add dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm output"}, {"repo_name": "flax-sentence-embeddings/stackoverflow_mpnet-base", "arch": "MPNetForMaskedLM", "model_type": "mpnet", "task": "sentence-similarity", "layers": "input to __getitem__ to ne int type_as mul long add embedding add layernorm dropout to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "hf-internal-testing/tiny-random-RwkvForCausalLM", "arch": "RwkvForCausalLM", "model_type": "rwkv", "task": "text-generation", "layers": "input to embedding to layernorm layernorm to __getitem__ zeropad2d mul add linear4bit zeros_like __getitem__ float add sub exp mul add div to add add maximum sub exp mul mul sub exp add mul add mul add mul add div to mul add mul add div to mul add div to mul add mul add mul add mul add mul add mul add mul add maximum add sub exp sub exp mul mul add maximum add sub exp sub exp mul maximum sub exp mul sub exp add maximum add maximum sub exp mul sub exp maximum sub exp mul sub exp add sub exp mul maximum sub exp mul sub exp __getitem__ float add sub exp mul add div to add add maximum sub exp mul mul maximum add add sub exp mul add add sub exp mul mul __getitem__ float add add __getitem__ float add add __getitem__ float add add mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ zeropad2d mul add linear4bit relu square linear4bit mul add to layernorm to __getitem__ zeropad2d mul add linear4bit zeros_like __getitem__ float add sub exp mul add div to add add maximum sub exp mul mul sub exp add mul add mul add mul add div to mul add mul add div to mul add div to mul add mul add mul add mul add mul add mul add mul add maximum add sub exp sub exp mul mul add maximum add sub exp sub exp mul maximum sub exp mul sub exp add maximum add maximum sub exp mul sub exp maximum sub exp mul sub exp add sub exp mul maximum sub exp mul sub exp __getitem__ float add sub exp mul add div to add add maximum sub exp mul mul maximum add add sub exp mul add add sub exp mul mul __getitem__ float add add __getitem__ float add add __getitem__ float add add mul add linear4bit __getitem__ __getitem__ __getitem__ __getitem__ __getitem__ mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ zeropad2d mul add linear4bit relu square linear4bit mul add to layernorm to __getitem__ zeropad2d mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ zeropad2d mul add linear4bit relu square linear4bit mul add to layernorm to __getitem__ zeropad2d mul add linear4bit zeros_like __getitem__ float add sub exp mul add div to add add maximum sub exp mul mul sub exp add mul add mul add mul add div to mul add mul add div to mul add div to mul add mul add mul add mul add mul add mul add mul add maximum add sub exp sub exp mul mul add maximum add sub exp sub exp mul maximum sub exp mul sub exp add maximum add maximum sub exp mul sub exp maximum sub exp mul sub exp add sub exp mul maximum sub exp mul sub exp __getitem__ float add sub exp mul add div to add add maximum sub exp mul mul maximum add add sub exp mul add add sub exp mul mul __getitem__ float add add __getitem__ float add add __getitem__ float add add mul add linear4bit __getitem__ __getitem__ __getitem__ __getitem__ __getitem__ mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ zeropad2d mul add linear4bit sigmoid mul add to layernorm to __getitem__ zeropad2d mul add linear4bit zeros_like __getitem__ float add sub exp mul add div to add add maximum sub exp mul mul sub exp add mul add mul add mul add div to mul add mul add div to mul add div to mul add mul add mul add mul add mul add mul add mul add maximum add sub exp sub exp mul mul add maximum add sub exp sub exp mul maximum sub exp mul sub exp add maximum add maximum sub exp mul sub exp maximum sub exp mul sub exp add sub exp mul maximum sub exp mul sub exp __getitem__ float add sub exp mul add div to add add maximum sub exp mul mul maximum add add sub exp mul add add sub exp mul mul __getitem__ float add add __getitem__ float add add __getitem__ float add add mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ zeropad2d mul add linear4bit sigmoid mul add layernorm to output"}, {"repo_name": "PlanTL-GOB-ES/longformer-base-4096-bne-es", "arch": "LongformerForMaskedLM", "model_type": "longformer", "task": "fill-mask", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "ismgar01/vit-base-cats-vs-dogs", "arch": "ViTForImageClassification", "model_type": "vit", "task": "image-classification", "layers": "input to to to to conv2d flatten transpose cat add dropout to to layernorm to to to linear4bit view permute scaled_dot_product_attention permute contiguous view to linear4bit dropout add to add to add to add to add to add to add to add to layernorm to to to linear4bit view permute scaled_dot_product_attention permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add to add to add to add to add to layernorm to to to linear4bit view permute scaled_dot_product_attention permute contiguous view to linear4bit dropout add to add to layernorm to to to linear4bit view permute scaled_dot_product_attention permute contiguous view to linear4bit dropout add to add to add to add to add layernorm to linear4bit geluactivation to linear4bit dropout add to layernorm to to to linear4bit view permute scaled_dot_product_attention permute contiguous view to linear4bit dropout add to add layernorm to output"}, {"repo_name": "ydshieh/tiny-random-BertForNextSentencePrediction", "arch": "BertForNextSentencePrediction", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "uclanlp/plbart-single_task-weak-summarization", "arch": "PLBartForConditionalGeneration", "model_type": "plbart", "task": "text2text-generation", "layers": "input to to view plbartscaledwordembedding add layernorm dropout to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm add layernorm isnan any to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "new5558/simcse-model-wangchanberta-base-att-spm-uncased", "arch": "CamembertModel", "model_type": "camembert", "task": "sentence-similarity", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "omarhkh/CutLER-n-omary02", "arch": "YolosForObjectDetection", "model_type": "yolos", "task": "object-detection", "layers": "input conv2d flatten transpose cat add dropout add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "viktoroo/sberbank-rubert-base-collection3", "arch": "BertForTokenClassification", "model_type": "bert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "danielsaggau/bregman_1.5", "arch": "LongformerModel", "model_type": "longformer", "task": "sentence-similarity", "layers": "input to pad to ne int mul long add to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill new_ones transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm __getitem__ to output"}, {"repo_name": "AlekseyKorshuk/gpt4all-j-groovy", "arch": "GPTJForCausalLM", "model_type": "gptj", "task": "text-generation", "layers": "input view embedding dropout add layernorm linear view permute matmul permute contiguous view linear dropout add add layernorm linear view permute matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add add layernorm linear view __getitem__ mul add cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add add add add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view permute matmul permute contiguous view linear dropout add add add layernorm linear view __getitem__ mul add cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear view __getitem__ mul add cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm view output"}, {"repo_name": "pickapic-anonymous/PickScore_v1", "arch": "CLIPModel", "model_type": "clip", "task": "zero-shot-image-classification", "layers": "input to to view to embedding add to to add layernorm to linear4bit geluactivation linear4bit add to add layernorm to linear4bit geluactivation linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit add layernorm to linear4bit geluactivation linear4bit add to add add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit add add to add add to add add to add add to add add to add add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit add add to add add to add add to layernorm to linear4bit view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit add add to add add to add layernorm to linear4bit geluactivation linear4bit add to add layernorm to linear4bit geluactivation linear4bit add to add add to add add to layernorm to linear4bit view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit add add to add add to add add to layernorm to linear4bit view transpose contiguous view transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit add layernorm to linear4bit geluactivation linear4bit add to add add layernorm to output"}, {"repo_name": "worachot-n/WangchanBERTa_LimeSoda_FakeNews", "arch": "CamembertForSequenceClassification", "model_type": "camembert", "task": "text-classification", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "hf-internal-testing/tiny-random-OPTForSequenceClassification", "arch": "OPTForSequenceClassification", "model_type": "opt", "task": "text-classification", "layers": "input to to view embedding add to add reshape add view to add reshape add view to add reshape add view to add reshape add view to add reshape layernorm linear4bit relu linear4bit dropout add view layernorm to output"}, {"repo_name": "Eike/lilt-xlm-roberta-base", "arch": "LiltModel", "model_type": "lilt", "task": "feature-extraction", "layers": "input to to ne int type_as mul long add to to embedding add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to linear4bit geluactivation to linear4bit dropout linear4bit view permute matmul div add add softmax dropout add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "deepset/gelectra-base-germanquad", "arch": "ElectraForQuestionAnswering", "model_type": "electra", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm output"}, {"repo_name": "l-yohai/bigbird-roberta-base-mnli", "arch": "BigBirdForSequenceClassification", "model_type": "big_bird", "task": "text-classification", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "facebook/mask2former-swin-small-ade-semantic", "arch": "Mask2FormerForUniversalSegmentation", "model_type": "mask2former", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm dropout layernorm view pad view permute contiguous view view linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add add add add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add view permute permute contiguous view layernorm view permute contiguous conv2d groupnorm flatten transpose cat add layernorm linear relu dropout linear dropout add layernorm add linear view softmax view transpose reshape mul sum view transpose contiguous linear dropout add layernorm add layernorm add layernorm linear relu dropout linear dropout add layernorm add linear view softmax view transpose reshape mul sum view transpose contiguous linear dropout add layernorm add layernorm add layernorm add layernorm linear masked_fill view split flatten transpose reshape grid_sample stack flatten mul sum view transpose contiguous linear dropout add layernorm add layernorm split transpose view mask2formersinepositionembedding flatten permute add multi_head_attention_forward dropout add layernorm permute add linear mul view transpose contiguous view bmm softmax view view dropout bmm view transpose reshape linear permute dropout add layernorm linear relu dropout linear dropout add layernorm add multi_head_attention_forward dropout add layernorm permute linear view transpose contiguous view bmm view transpose reshape linear permute dropout add layernorm linear relu dropout linear dropout add layernorm add multi_head_attention_forward dropout add layernorm add layernorm add layernorm add multi_head_attention_forward dropout add layernorm permute add linear mul view transpose contiguous view bmm softmax view view dropout bmm view transpose reshape linear permute dropout add layernorm add layernorm layernorm transpose linear relu linear relu linear einsum output"}, {"repo_name": "theojolliffe/bigbird-pegasus-large-arxiv-finetuned-roundup-280922", "arch": "BigBirdPegasusForConditionalGeneration", "model_type": "bigbird_pegasus", "task": "text2text-generation", "layers": "input to to view bigbirdpegasusscaledwordembedding add dropout to layernorm to linear4bit view permute matmul div softmax dropout matmul permute contiguous view linear4bit dropout add add isinf any to add add isinf any to add add isinf any isnan any to add add isinf any to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any to add add isinf any isnan any to add add isinf any isnan any to add add to add add isinf any to layernorm to linear4bit view permute matmul div softmax dropout matmul permute contiguous view linear4bit dropout add add isinf any isnan any to add add isinf any to add add isinf any to add add isinf any to layernorm to linear4bit view permute transpose matmul div softmax dropout matmul permute contiguous view linear4bit dropout add add isinf any to add add isinf any isnan any to add layernorm linear4bit newgeluactivation linear4bit dropout add isinf any layernorm to output"}, {"repo_name": "susooo/kobigbird-base28-47999573", "arch": "BigBirdForQuestionAnswering", "model_type": "big_bird", "task": "question-answering", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "bongsoo/albert-small-kor-sbert-v1", "arch": "AlbertModel", "model_type": "albert", "task": "sentence-similarity", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to output"}, {"repo_name": "sivasankalpp/dpr-multidoc2dial-token-ctx-encoder", "arch": "DPRContextEncoder", "model_type": "dpr", "task": "unknown", "layers": "input to to to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to output"}, {"repo_name": "cross-encoder/nli-deberta-v3-large", "arch": "DebertaV2ForSequenceClassification", "model_type": "deberta-v2", "task": "zero-shot-classification", "layers": "input embedding zeros_like layernorm mul stabledropout linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm output"}, {"repo_name": "ptah23/ast-finetuned-audioset-10-10-0.4593-finetuned-gtzan", "arch": "ASTForAudioClassification", "model_type": "audio-spectrogram-transformer", "task": "audio-classification", "layers": "input unsqueeze transpose conv2d flatten transpose cat add dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add add add add layernorm linear geluactivation linear dropout add add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add add add add layernorm linear geluactivation linear dropout add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add layernorm output"}, {"repo_name": "ArtifactAI/led_base_16384_arxiv_summarization", "arch": "LEDForConditionalGeneration", "model_type": "led", "task": "text2text-generation", "layers": "input pad view embedding cat add layernorm dropout transpose linear div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm __getitem__ output"}, {"repo_name": "Helsinki-NLP/opus-mt-eo-de", "arch": "MarianMTModel", "model_type": "marian", "task": "translation", "layers": "input to to __getitem__ view embedding mul add dropout to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit silu dropout linear4bit dropout add layernorm isinf any isnan any to add layernorm linear4bit silu dropout linear4bit dropout add layernorm isinf any isnan any to to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit silu dropout linear4bit dropout add layernorm isinf any isnan any to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit silu dropout linear4bit dropout add layernorm isinf any isnan any to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm add layernorm isinf any isnan any to add layernorm linear4bit silu dropout linear4bit dropout add layernorm isinf any isnan any to output"}, {"repo_name": "austin/adr-ner", "arch": "DebertaForTokenClassification", "model_type": "deberta", "task": "token-classification", "layers": "input embedding debertalayernorm mul stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm output"}, {"repo_name": "firqaaa/indo-dpr-ctx_encoder-multiset-base", "arch": "DPRContextEncoder", "model_type": "bert", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "gcyzsl/unsup-AudioCSE-roberta-large", "arch": "RobertaForCL", "model_type": "roberta", "task": "unknown", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "hafidikhsan/wav2vec2-large-xlsr-53-english-pronunciation-evaluation-aod-real-balance", "arch": "Wav2Vec2ForSequenceClassification", "model_type": "wav2vec2", "task": "audio-classification", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "sentence-transformers/sentence-t5-large", "arch": "T5EncoderModel", "model_type": "t5", "task": "sentence-similarity", "layers": "input view embedding dropout add add add t5layernorm linear relu dropout linear dropout add t5layernorm linear view transpose matmul transpose contiguous view linear dropout add add add t5layernorm linear relu dropout linear dropout add t5layernorm linear view transpose matmul transpose contiguous view linear dropout add add add t5layernorm linear relu dropout linear dropout add t5layernorm linear view transpose matmul transpose contiguous view linear dropout add add t5layernorm linear view transpose transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add t5layernorm linear relu dropout linear dropout add t5layernorm linear view transpose matmul transpose contiguous view linear dropout add t5layernorm linear relu dropout linear dropout add t5layernorm linear view transpose matmul transpose contiguous view linear dropout add t5layernorm linear relu dropout linear dropout add add t5layernorm linear relu dropout linear dropout add t5layernorm linear view transpose matmul transpose contiguous view linear dropout add t5layernorm linear relu dropout linear dropout add add t5layernorm linear relu dropout linear dropout add t5layernorm linear view transpose transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add t5layernorm linear relu dropout linear dropout add add t5layernorm linear relu dropout linear dropout add t5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add add t5layernorm linear view transpose transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add t5layernorm linear relu dropout linear dropout add add add add add t5layernorm linear view transpose matmul transpose contiguous view linear dropout add t5layernorm linear relu dropout linear dropout add add t5layernorm linear relu dropout linear dropout add add t5layernorm linear relu dropout linear dropout add add t5layernorm linear relu dropout linear dropout add add t5layernorm linear relu dropout linear dropout add t5layernorm dropout linear view transpose output"}, {"repo_name": "stefan-it/electra-base-gc4-64k-100000-cased-generator", "arch": "ElectraForMaskedLM", "model_type": "electra", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "Davlan/bert-base-multilingual-cased-finetuned-swahili", "arch": "BertForMaskedLM", "model_type": "bert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "gokceuludogan/ChemBERTaLM", "arch": "RobertaForCausalLM", "model_type": "roberta", "task": "text-generation", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute to output"}, {"repo_name": "nguyenvulebinh/wav2vec2-base-vi", "arch": "Wav2Vec2ForPreTraining", "model_type": "wav2vec2", "task": "pretraining", "layers": "input __getitem__ conv1d groupnorm geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation transpose layernorm output"}, {"repo_name": "sentence-transformers/clip-ViT-B-32-multilingual-v1", "arch": "DistilBertModel", "model_type": "distilbert", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "hf-internal-testing/tiny-random-LongformerModel", "arch": "LongformerModel", "model_type": "longformer", "task": "feature-extraction", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view new_zeros view transpose new_ones tril flip __getitem__ flip expand bool where expand bool where __getitem__ full_like __getitem__ full_like add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ full_like where __getitem__ full_like where new_ones tril flip __getitem__ flip expand bool expand bool add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to output"}, {"repo_name": "ethanyt/guwen-ner", "arch": "RobertaForTokenClassification", "model_type": "roberta", "task": "token-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "flaviagiammarino/medsam-vit-base", "arch": "SamModel", "model_type": "sam", "task": "mask-generation", "layers": "input conv2d permute add layernorm pad reshape permute contiguous reshape linear reshape permute reshape unbind reshape einsum __getitem__ add add reshape softmax to dropout matmul reshape permute reshape linear reshape permute contiguous reshape __getitem__ contiguous add layernorm linear geluactivation linear add add add layernorm linear reshape permute reshape unbind transpose matmul reshape add add reshape softmax to dropout matmul reshape permute reshape linear add add add add layernorm pad reshape permute contiguous reshape linear reshape permute reshape unbind reshape einsum __getitem__ add add reshape softmax to dropout matmul reshape permute reshape linear reshape permute contiguous reshape __getitem__ contiguous add add add add add add layernorm pad reshape permute contiguous reshape linear reshape permute reshape unbind reshape einsum __getitem__ add add reshape softmax to dropout matmul reshape permute reshape linear reshape permute contiguous reshape __getitem__ contiguous add add layernorm linear reshape permute reshape unbind transpose matmul reshape add add reshape softmax to dropout matmul reshape permute reshape linear add add add add add add add add permute conv2d samlayernorm conv2d samlayernorm add repeat_interleave flatten permute unsqueeze add layernorm add linear reshape transpose matmul div softmax matmul transpose reshape linear add layernorm transpose reshape convtranspose2d samlayernorm gelu convtranspose2d gelu reshape matmul reshape __getitem__ output"}, {"repo_name": "pmpc/TSo-cross-en-de-roberta-sentence-transformer-longformer-base-16384", "arch": "LongformerModel", "model_type": "longformer", "task": "feature-extraction", "layers": "input pad embedding add add layernorm dropout transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm add layernorm transpose linear div view transpose type_as masked_fill new_ones transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm __getitem__ __getitem__ linear tanh output"}, {"repo_name": "LLukas22/all-MiniLM-L12-v2-qa-en", "arch": "BertForQuestionAnswering", "model_type": "bert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "xlm-roberta-large-finetuned-conll02-spanish", "arch": "XLMRobertaForMaskedLM", "model_type": "xlm-roberta", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "Udit191/autotrain-summarization_bart_longformer-54164127153", "arch": "LEDForConditionalGeneration", "model_type": "led", "task": "text2text-generation", "layers": "input to to pad view embedding cat add layernorm dropout to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear4bit dropout add layernorm add layernorm isinf any to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear4bit dropout add layernorm add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm isinf any isnan any to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear4bit dropout add layernorm add layernorm isinf any isnan any to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isinf any to add layernorm add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm __getitem__ to output"}, {"repo_name": "tum-nlp/IDMGSP-Galactica-TRAIN-CG", "arch": "OPTForSequenceClassification", "model_type": "opt", "task": "text-classification", "layers": "input to to view embedding add to add reshape add view to add reshape layernorm linear4bit geluactivation linear4bit dropout add view to add reshape layernorm linear4bit geluactivation linear4bit dropout add view to add reshape layernorm linear4bit geluactivation linear4bit dropout add view to add reshape layernorm linear4bit geluactivation linear4bit dropout add view to add reshape layernorm linear4bit geluactivation linear4bit dropout add view to layernorm to linear4bit mul view transpose contiguous view bmm view add max view softmax to dropout bmm view transpose reshape linear4bit dropout add reshape layernorm linear4bit geluactivation linear4bit dropout add view to add reshape layernorm linear4bit geluactivation linear4bit dropout add view to add reshape add view to layernorm to linear4bit mul view transpose contiguous view bmm view add max view softmax to dropout bmm view transpose reshape linear4bit dropout add reshape layernorm linear4bit geluactivation linear4bit dropout add view to add reshape layernorm linear4bit geluactivation linear4bit dropout add view to add reshape layernorm linear4bit geluactivation linear4bit dropout add view layernorm to output"}, {"repo_name": "reemalyami/AraRoBERTa-LB", "arch": "RobertaForMaskedLM", "model_type": "roberta", "task": "fill-mask", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "ckiplab/albert-base-chinese-pos", "arch": "AlbertForTokenClassification", "model_type": "albert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to to to add layernorm linear4bit relu linear4bit add layernorm to output"}, {"repo_name": "SuryaaSeran/bert-base-uncased-finetuned-swag", "arch": "BertForMultipleChoice", "model_type": "bert", "task": "multiple-choice", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "gyuri2020/kw-classification-setfithead-model", "arch": "MPNetModel", "model_type": "mpnet", "task": "text-classification", "layers": "input to __getitem__ to embedding add layernorm dropout to to to add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "facebook/convnextv2-large-22k-224", "arch": "ConvNextV2ForImageClassification", "model_type": "convnextv2", "task": "image-classification", "layers": "input conv2d convnextv2layernorm conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add convnextv2layernorm conv2d add add add convnextv2layernorm conv2d conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add convnextv2layernorm conv2d add add add mean layernorm output"}, {"repo_name": "johannes-garstenauer/distilbert-heaps-masked", "arch": "DistilBertForMaskedLM", "model_type": "distilbert", "task": "fill-mask", "layers": "input to to embedding add layernorm dropout to to output"}, {"repo_name": "sivasankalpp/dpr-multidoc2dial-structure-question-encoder", "arch": "DPRQuestionEncoder", "model_type": "dpr", "task": "feature-extraction", "layers": "input to to to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to output"}, {"repo_name": "tuner007/pegasus_qa", "arch": "PegasusForConditionalGeneration", "model_type": "pegasus", "task": "text2text-generation", "layers": "input __getitem__ view embedding mul add dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm output"}, {"repo_name": "arampacha/wav2vec2-large-xlsr-czech", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "vymn/Brain", "arch": "BlenderbotForConditionalGeneration", "model_type": "blenderbot", "task": "text2text-generation", "layers": "input to to __getitem__ view blenderbotscaledwordembedding add dropout to add add to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "textattack/xlnet-base-cased-rotten-tomatoes", "arch": "XLNetLMHeadModel", "model_type": "xlnet", "task": "text-generation", "layers": "input transpose contiguous embedding dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "Davlan/afro-xlmr-large-61L", "arch": "XLMRobertaForMaskedLM", "model_type": "xlm-roberta", "task": "fill-mask", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm __getitem__ linear tanh output"}, {"repo_name": "Hanwoon/pz-bert-kr", "arch": "BertForMaskedLM", "model_type": "bert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "sentence-transformers/distiluse-base-multilingual-cased", "arch": "DistilBertModel", "model_type": "distilbert", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout linear view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose div matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "calbert/indic-bert", "arch": "AlbertModel", "model_type": "albert", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to output"}, {"repo_name": "kaisar-barlybay-sse/kaz_legal_distilbert_full_corpus_10.0_16", "arch": "DistilBertForMultipleChoice", "model_type": "distilbert", "task": "multiple-choice", "layers": "input to to embedding add layernorm dropout to to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to output"}, {"repo_name": "userGagan/segformer-b0-finetuned-segments-sidewalk-2", "arch": "SegformerForSemanticSegmentation", "model_type": "segformer", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm add add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add layernorm reshape permute contiguous conv2d flatten transpose layernorm add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add layernorm permute reshape conv2d reshape permute layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout segformerdroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add layernorm reshape permute contiguous conv2d flatten transpose layernorm add add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add layernorm reshape permute contiguous conv2d flatten transpose layernorm layernorm linear view permute matmul permute contiguous view linear dropout segformerdroppath add add layernorm linear view permute matmul permute contiguous view linear dropout segformerdroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add layernorm reshape permute contiguous output"}, {"repo_name": "jjonhwa/BIG-24000", "arch": "BertForV2QuestionAnswering", "model_type": "big_bird", "task": "unknown", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "LarkAI/bart_large_nl2sql", "arch": "BartModel", "model_type": "bart", "task": "text2text-generation", "layers": "input to to bartlearnedpositionalembedding to add layernorm dropout to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm to to to linear4bit view transpose contiguous output"}, {"repo_name": "Celal11/beit-base-patch16-224-pt22k-ft22k-finetuned-FER2013CKPlus", "arch": "BeitForImageClassification", "model_type": "beit", "task": "image-classification", "layers": "input conv2d flatten transpose cat dropout layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul add layernorm linear geluactivation linear dropout mul add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add add __getitem__ mean layernorm output"}, {"repo_name": "jacinthes/cross-encoder-sloberta-si-nli-snli-mnli", "arch": "CamembertForSequenceClassification", "model_type": "camembert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "purna419/layoutlmv3-finetuned-cord_100", "arch": "LayoutLMv3ForTokenClassification", "model_type": "layoutlmv3", "task": "token-classification", "layers": "input conv2d flatten transpose cat add dropout layernorm layernorm dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute transpose matmul add add div amax unsqueeze sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "hf-internal-testing/tiny-random-DebertaForSequenceClassification", "arch": "DebertaForSequenceClassification", "model_type": "deberta", "task": "text-classification", "layers": "input embedding add add debertalayernorm mul stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm output"}, {"repo_name": "amittimalsina/lilt-roberta-base-bne", "arch": "LiltModel", "model_type": "lilt", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "jonatasgrosman/exp_w2v2t_et_wavlm_s887", "arch": "WavLMForCTC", "model_type": "wavlm", "task": "automatic-speech-recognition", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "ilionesso/esm2_t12_35M_UR50D-finetuned-localization", "arch": "EsmForSequenceClassification", "model_type": "esm", "task": "text-classification", "layers": "input to to eq unsqueeze masked_fill mul div to mul to to to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute mul rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute mul rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute mul rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add to add to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add to add layernorm to output"}, {"repo_name": "parsi-ai-nlpclass/NLP_Spring23_HW4_Document_Classifier_G13", "arch": "BigBirdForSequenceClassification", "model_type": "big_bird", "task": "text-classification", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm __getitem__ linear tanh to output"}, {"repo_name": "Linseypass/scielectra", "arch": "ElectraModel", "model_type": "electra", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "RUI525/ClinicalBERT-finetune-MedMCQA-w-context", "arch": "DistilBertForMultipleChoice", "model_type": "distilbert", "task": "multiple-choice", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "ALM-AHME/beit-large-patch16-224-finetuned-Lesion-Classification-HAM10000-AH-60-20-20", "arch": "BeitForImageClassification", "model_type": "beit", "task": "image-classification", "layers": "input conv2d flatten transpose cat dropout add layernorm linear geluactivation linear dropout mul add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add add add add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add add add add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add add add layernorm linear geluactivation linear dropout mul beitdroppath add __getitem__ mean layernorm output"}, {"repo_name": "shahrukhx01/simcse-smole-bert-muv", "arch": "BertForCL", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "pedramyazdipoor/persian_xlm_roberta_large", "arch": "XLMRobertaForQuestionAnswering", "model_type": "xlm-roberta", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm __getitem__ linear tanh output"}, {"repo_name": "mdraw/german-news-sentiment-bert", "arch": "BertForSequenceClassification", "model_type": "bert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "vivekraina/Llama-2-7b-hf-8bit", "arch": "LlamaForCausalLM", "model_type": "llama", "task": "text-generation", "layers": "input to to embedding to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose llamarotaryembedding unsqueeze mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add add to add llamarmsnorm to linear8bitlt silu mul linear8bitlt add to add llamarmsnorm to linear8bitlt silu mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose llamarotaryembedding unsqueeze mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt silu mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose llamarotaryembedding unsqueeze mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to add llamarmsnorm to linear8bitlt silu mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ neg cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add add to add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt silu mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt silu mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add add to llamarmsnorm to linear8bitlt view transpose llamarotaryembedding unsqueeze mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to add llamarmsnorm to linear8bitlt silu mul linear8bitlt add to add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt silu mul linear8bitlt add to add add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt silu mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose llamarotaryembedding unsqueeze mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt mul linear8bitlt add to add llamarmsnorm to linear8bitlt mul linear8bitlt add to llamarmsnorm to linear8bitlt view transpose llamarotaryembedding unsqueeze mul add scaled_dot_product_attention transpose contiguous view linear8bitlt add llamarmsnorm to linear8bitlt silu mul linear8bitlt add llamarmsnorm linear float to output"}, {"repo_name": "TomPWM/ouvrage-classif", "arch": "MPNetModel", "model_type": "mpnet", "task": "text-classification", "layers": "input to to ne int type_as mul long add embedding add layernorm dropout to to to add layernorm to add layernorm to to add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to add layernorm to to add layernorm to add layernorm to to add layernorm to add layernorm to to add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "KoichiYasuoka/deberta-base-japanese-aozora-ud-head", "arch": "DebertaV2ForQuestionAnswering", "model_type": "deberta-v2", "task": "question-answering", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "taspips/bigbird_finetuned_model", "arch": "BigBirdForQuestionAnswering", "model_type": "big_bird", "task": "question-answering", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "Ramos-Ramos/xlm-roberta-base-en-tl-4-end", "arch": "XLMRobertaModel", "model_type": "xlm-roberta", "task": "sentence-similarity", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "cointegrated/rut5-small", "arch": "MT5ForConditionalGeneration", "model_type": "mt5", "task": "text2text-generation", "layers": "input view embedding dropout add mt5layernorm linear newgeluactivation mul dropout linear dropout add add mt5layernorm linear newgeluactivation mul dropout linear dropout add add add add mt5layernorm linear newgeluactivation mul dropout linear dropout add add mt5layernorm linear newgeluactivation mul dropout linear dropout add add mt5layernorm linear newgeluactivation mul dropout linear dropout add add add add mt5layernorm linear newgeluactivation mul dropout linear dropout add mt5layernorm dropout output"}, {"repo_name": "roneneldan/TinyStories-8M", "arch": "GPTNeoForCausalLM", "model_type": "gpt_neo", "task": "text-generation", "layers": "input view embedding add dropout layernorm linear view permute to output"}, {"repo_name": "jjonhwa/BIG-6000", "arch": "BertForV2QuestionAnswering", "model_type": "big_bird", "task": "unknown", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "nielsr/lilt-xlm-roberta-base", "arch": "LiltModel", "model_type": "lilt", "task": "feature-extraction", "layers": "input to to ne int type_as mul long add to embedding add layernorm dropout to to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "jonatasgrosman/exp_w2v2t_ru_unispeech-sat_s418", "arch": "UniSpeechSatForCTC", "model_type": "unispeech-sat", "task": "automatic-speech-recognition", "layers": "input to to __getitem__ to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation transpose to layernorm to output"}, {"repo_name": "IDEA-CCNL/Erlangshen-UniMC-RoBERTa-330M-Chinese", "arch": "BertForMaskedLM", "model_type": "bert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "bilbo991/clip-chuck", "arch": "VisionTextDualEncoderModel", "model_type": "vision-text-dual-encoder", "task": "feature-extraction", "layers": "input to to embedding to add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose __getitem__ cat mul add scaled_dot_product_attention transpose contiguous view linear4bit add add to mistralrmsnorm to linear4bit view transpose mul add output"}, {"repo_name": "yemen2016/memo_final", "arch": "XLMRobertaForMaskedLM", "model_type": "xlm-roberta", "task": "fill-mask", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "nbroad/bigbird-base-health-fact", "arch": "BigBirdForSequenceClassification", "model_type": "big_bird", "task": "text-classification", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "patrickvonplaten/wavlm-libri-clean-100h-large", "arch": "WavLMForCTC", "model_type": "wavlm", "task": "automatic-speech-recognition", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "gaussalgo/xlm-roberta-large_extractive-QA_en-cs", "arch": "XLMRobertaForQuestionAnswering", "model_type": "xlm-roberta", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm __getitem__ linear tanh output"}, {"repo_name": "svalabs/infoxlm-german-question-answering", "arch": "XLMRobertaForQuestionAnswering", "model_type": "xlm-roberta", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm __getitem__ linear tanh output"}, {"repo_name": "hf-internal-testing/tiny-random-BertLMHeadModel", "arch": "BertLMHeadModel", "model_type": "bert", "task": "text-generation", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute to output"}, {"repo_name": "KarelDO/roberta-base.CEBaB_confounding.observational.sa.5-class.seed_43", "arch": "RobertaForNonlinearSequenceClassification", "model_type": "roberta", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "Recognai/zeroshot_selectra_medium", "arch": "ElectraForSequenceClassification", "model_type": "electra", "task": "zero-shot-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "knkarthick/Sentiment-Analysis", "arch": "RobertaForSequenceClassification", "model_type": "roberta", "task": "text-classification", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "silk-road/luotuo-bert", "arch": "BertForCL", "model_type": "bert", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "tyzp-INC/few-shot-multilingual-e5-large-xnli", "arch": "XLMRobertaModel", "model_type": "xlm-roberta", "task": "text-classification", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "ml6team/keyphrase-extraction-distilbert-openkp", "arch": "DistilBertForTokenClassification", "model_type": "distilbert", "task": "token-classification", "layers": "input embedding add layernorm dropout linear view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "EleutherAI/pythia-12b-v0", "arch": "GPTNeoXForCausalLM", "model_type": "gpt_neox", "task": "text-generation", "layers": "input to embedding dropout to layernorm to linear4bit geluactivation linear4bit dropout add add to layernorm to linear4bit view __getitem__ permute output"}, {"repo_name": "almanach/camembert-bio-base", "arch": "CamembertForMaskedLM", "model_type": "camembert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "NDugar/deberta-v2-xlarge-mnli", "arch": "DebertaV2ForSequenceClassification", "model_type": "deberta-v2", "task": "zero-shot-classification", "layers": "input embedding layernorm mul stabledropout add layernorm linear geluactivation linear stabledropout add layernorm add to mul add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation linear stabledropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation linear stabledropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm output"}, {"repo_name": "mobashgr/BC5CDR-disease-WLT-320-BioElectra-pubmed-ENS-30", "arch": "ElectraForTokenClassification", "model_type": "electra", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "eawang/roberta_mc_model_4", "arch": "RobertaForMultipleChoice", "model_type": "roberta", "task": "multiple-choice", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "autosyrup/roberta", "arch": "RobertaForTokenClassification", "model_type": "roberta", "task": "token-classification", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "jjonhwa/18000", "arch": "RobertaForV2QuestionAnswering", "model_type": "roberta", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "hajime9652/xlnet-japanese", "arch": "XLNetLMHeadModel", "model_type": "xlnet", "task": "text-generation", "layers": "input transpose contiguous embedding dropout einsum add einsum reshape __getitem__ reshape index_select add add mul softmax dropout einsum einsum dropout add layernorm linear relu dropout linear dropout add layernorm einsum einsum einsum dropout add layernorm add layernorm einsum add einsum reshape __getitem__ reshape index_select add add mul softmax dropout einsum einsum dropout add layernorm linear relu dropout linear dropout add layernorm einsum add einsum add add mul softmax dropout einsum einsum dropout add layernorm add layernorm einsum add einsum reshape __getitem__ reshape index_select add add mul softmax dropout einsum einsum dropout add layernorm add layernorm einsum add einsum reshape __getitem__ reshape index_select add add mul softmax dropout einsum einsum dropout add layernorm add layernorm einsum einsum einsum dropout add layernorm add layernorm einsum add einsum reshape __getitem__ reshape index_select add add mul softmax dropout einsum einsum dropout add layernorm linear relu dropout linear dropout add layernorm einsum einsum einsum dropout add layernorm add layernorm einsum add einsum add add mul softmax dropout einsum einsum dropout add layernorm add layernorm einsum add einsum reshape __getitem__ reshape index_select add add mul softmax dropout einsum einsum dropout add layernorm linear relu dropout linear dropout add layernorm einsum einsum einsum dropout add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "sentence-transformers/roberta-large-nli-stsb-mean-tokens", "arch": "RobertaModel", "model_type": "roberta", "task": "sentence-similarity", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "Kiet/autotrain-resume_parser-1159242747", "arch": "LongformerForQuestionAnswering", "model_type": "longformer", "task": "question-answering", "layers": "input to pad to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "Palak/microsoft_deberta-large_squad", "arch": "DebertaForQuestionAnswering", "model_type": "deberta", "task": "question-answering", "layers": "input embedding zeros_like debertalayernorm mul stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm linear view permute chunk transpose matmul add masked_fill softmax masked_fill stabledropout matmul permute contiguous view linear stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm output"}, {"repo_name": "ffgcc/esimcse-roberta-base", "arch": "RobertaForCL", "model_type": "roberta", "task": "unknown", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "hlyu/albert_another", "arch": "AlbertModel", "model_type": "albert", "task": "sentence-similarity", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to output"}, {"repo_name": "beomi/KoRWKV-1.5B", "arch": "RwkvForCausalLM", "model_type": "rwkv", "task": "text-generation", "layers": "input to embedding to layernorm layernorm to __getitem__ mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add mul mul mul add div to zeropad2d mul mul add linear4bit sigmoid mul linear4bit add add to add add to add add to add add to layernorm to __getitem__ mul add linear4bit __getitem__ float maximum maximum sub exp mul add div to mul add sub exp mul sub exp mul add mul add mul add mul add add maximum sub exp mul sub exp sub exp mul add add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float add add zeros_like mul add linear4bit sigmoid mul linear4bit add add to add layernorm to __getitem__ mul add linear4bit sigmoid mul add div to layernorm to __getitem__ mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add mul mul mul add div to mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ mul add linear4bit sigmoid mul add to layernorm to __getitem__ mul add linear4bit __getitem__ float maximum maximum sub exp mul add div to mul add sub exp mul sub exp mul add mul add mul add mul add add maximum sub exp mul sub exp sub exp mul add add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float add add zeros_like mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ mul add linear4bit relu square linear4bit mul add to add add to add add to add add to add add div to layernorm to __getitem__ mul add linear4bit __getitem__ float maximum maximum sub exp mul add div to mul add sub exp mul sub exp mul add mul add mul add mul add add maximum sub exp mul sub exp sub exp mul add add maximum sub exp mul add div to mul add sub exp mul add __getitem__ float add add zeros_like mul add linear4bit __getitem__ __getitem__ mul add linear4bit sigmoid mul linear4bit add add to layernorm to __getitem__ mul add linear4bit sigmoid mul linear4bit add layernorm to __getitem__ mul add linear4bit relu square linear4bit mul add to add add to layernorm to __getitem__ mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add mul mul mul add div to mul add linear4bit sigmoid mul linear4bit add add to add layernorm to __getitem__ mul add linear4bit sigmoid mul add to add layernorm to __getitem__ mul add linear4bit relu square linear4bit mul add div to layernorm to __getitem__ zeropad2d mul add linear4bit sigmoid mul linear4bit add add to add add to add add to add add to layernorm to __getitem__ mul add linear4bit __getitem__ mul add div to mul add __getitem__ mul add mul mul mul add div to mul add linear4bit __getitem__ float maximum maximum sub exp mul add sub exp sub exp mul mul add mul add add maximum sub exp sub exp sub exp add add maximum sub exp mul mul add sub exp add __getitem__ float add add zeros_like mul add linear4bit sigmoid mul linear4bit add add to add layernorm to __getitem__ mul add linear4bit relu square linear4bit mul add div layernorm to output"}, {"repo_name": "KshitizPandya/GenzTranscribe-small-gu", "arch": "WhisperForConditionalGeneration", "model_type": "whisper", "task": "automatic-speech-recognition", "layers": "input to to view whisperpositionalembedding embedding add dropout to add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous to output"}, {"repo_name": "Helsinki-NLP/opus-mt-tc-big-zlw-zle", "arch": "MarianMTModel", "model_type": "marian", "task": "translation", "layers": "input to to view embedding mul add dropout to add layernorm add layernorm isinf any clamp to add layernorm add layernorm isinf any clamp to add layernorm linear4bit relu dropout linear4bit dropout add layernorm clamp to add layernorm linear4bit relu dropout linear4bit dropout add layernorm clamp to add layernorm add layernorm isinf any clamp to add layernorm add layernorm isinf any clamp to output"}, {"repo_name": "voidful/bart-base-unit", "arch": "BartModel", "model_type": "bart", "task": "feature-extraction", "layers": "input to to view bartscaledwordembedding add layernorm dropout to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "aplnestrella/pegasus-x-cord19", "arch": "PegasusXForConditionalGeneration", "model_type": "pegasus_x", "task": "text2text-generation", "layers": "input to to __getitem__ view pegasusxscaledwordembedding pegasusxsinusoidalpositionalembedding add dropout pad to add add to add layernorm linear4bit relu dropout linear4bit dropout add to add add to add add to add add to layernorm pad to linear4bit view transpose contiguous view einsum cat add softmax dropout __getitem__ einsum add permute contiguous view linear4bit __getitem__ dropout add add to layernorm to linear4bit mul view transpose contiguous view einsum cat add softmax dropout __getitem__ einsum add permute contiguous view linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add to add add to layernorm to linear4bit mul view transpose contiguous view einsum cat add softmax dropout __getitem__ einsum add permute contiguous view linear4bit dropout add layernorm linear4bit relu dropout linear4bit dropout add to layernorm pad to linear4bit view transpose contiguous view einsum add permute contiguous view linear4bit __getitem__ dropout add add to add add to add add __getitem__ layernorm to output"}, {"repo_name": "cardiffnlp/twitter-xlm-roberta-base", "arch": "XLMRobertaForMaskedLM", "model_type": "xlm-roberta", "task": "fill-mask", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "Helsinki-NLP/opus-mt-ru-uk", "arch": "MarianMTModel", "model_type": "marian", "task": "translation", "layers": "input to to view embedding mul add dropout to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to add layernorm linear4bit silu dropout linear4bit dropout add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to to to linear4bit view transpose contiguous to output"}, {"repo_name": "digitous/Adventien-GPTJ", "arch": "GPTJForCausalLM", "model_type": "gptj", "task": "text-generation", "layers": "input to view embedding dropout to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to add to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to transpose matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit view __getitem__ cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add layernorm view to output"}, {"repo_name": "NDugar/ZSD-microsoft-v2xxlmnli", "arch": "DebertaV2ForSequenceClassification", "model_type": "deberta-v2", "task": "zero-shot-classification", "layers": "input embedding layernorm mul stabledropout add layernorm add layernorm add to mul add layernorm linear geluactivation linear stabledropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation linear stabledropout add layernorm add layernorm linear geluactivation linear stabledropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation linear stabledropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation linear stabledropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation linear stabledropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm output"}, {"repo_name": "hw2942/Erlangshen-Longformer-110M-finetuning-wallstreetcn-morning-news-market-overview-open-000001SH-v1", "arch": "LongformerForSequenceClassification", "model_type": "longformer", "task": "text-classification", "layers": "input to pad to ne int mul long add to embedding add add layernorm dropout to to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm __getitem__ to output"}, {"repo_name": "hf-internal-testing/tiny-random-BartForQuestionAnswering", "arch": "BartForQuestionAnswering", "model_type": "bart", "task": "question-answering", "layers": "input to to bartlearnedpositionalembedding to add layernorm dropout to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to output"}, {"repo_name": "KarelDO/roberta-base.CEBaB_confounding.price_food_ambiance_negative.absa.5-class.seed_43", "arch": "RobertaForNonlinearSequenceClassification", "model_type": "roberta", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "jjonhwa/BIG-26500", "arch": "BertForV2QuestionAnswering", "model_type": "big_bird", "task": "unknown", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "monologg/koelectra-base-v3-gender-bias", "arch": "ElectraForSequenceClassification", "model_type": "electra", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "pythainlp/thainer-corpus-v2-base-model", "arch": "CamembertForTokenClassification", "model_type": "camembert", "task": "token-classification", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "sentence-transformers/nli-roberta-base", "arch": "RobertaModel", "model_type": "roberta", "task": "sentence-similarity", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "tanay/layoutlm-funsd", "arch": "LayoutLMForTokenClassification", "model_type": "layoutlm", "task": "token-classification", "layers": "input to to embedding add add add add add add add add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "derguene/carpooling-camembert-base", "arch": "CamembertModel", "model_type": "camembert", "task": "text-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm __getitem__ linear tanh output"}, {"repo_name": "sngsfydy/MobileNetV2_with_Trainer", "arch": "MobileNetV2ForImageClassification", "model_type": "mobilenet_v2", "task": "image-classification", "layers": "input pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add add add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 output"}, {"repo_name": "khuang2468/esm2_t12_35M_UR50D-finetuned-localization", "arch": "EsmForSequenceClassification", "model_type": "esm", "task": "text-classification", "layers": "input to to embedding masked_fill mul div to mul to to to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add to add to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add layernorm to output"}, {"repo_name": "Jumtra/mpt-7b-base", "arch": "MPTForCausalLM", "model_type": "mpt", "task": "text-generation", "layers": "input embedding add add layernorm linear chunk reshape transpose transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add layernorm linear gelu linear dropout add add add add add add add add add add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add layernorm linear chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add layernorm linear chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add add add add add add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add add add add layernorm linear gelu linear dropout add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add layernorm linear gelu linear dropout add add add add add add add add add layernorm output"}, {"repo_name": "eawang/roberta_mc_model_7", "arch": "RobertaForMultipleChoice", "model_type": "roberta", "task": "multiple-choice", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "giacomomiolo/scibert_reupload", "arch": "BertForPreTraining", "model_type": "bert", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "dhananjay2912/lsg-bart-base-4096-mediqa-chat-taskb-sectionwise-results", "arch": "LSGBartForConditionalGeneration", "model_type": "bart", "task": "text2text-generation", "layers": "input to to bartlearnedpositionalembedding to add layernorm dropout to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "google/pegasus-billsum", "arch": "PegasusForConditionalGeneration", "model_type": "pegasus", "task": "text2text-generation", "layers": "input __getitem__ view embedding mul add dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm output"}, {"repo_name": "SEBIS/code_trans_t5_small_source_code_summarization_sql_multitask_finetune", "arch": "T5Model", "model_type": "t5", "task": "summarization", "layers": "input view embedding dropout add t5layernorm linear relu dropout linear dropout add add t5layernorm linear relu dropout linear dropout add add t5layernorm linear relu dropout linear dropout add t5layernorm linear view transpose transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add t5layernorm linear relu dropout linear dropout add add t5layernorm linear relu dropout linear dropout add t5layernorm linear view transpose transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add t5layernorm linear relu dropout linear dropout add t5layernorm dropout linear view transpose matmul transpose contiguous view linear dropout add add add add t5layernorm linear relu dropout linear dropout add t5layernorm dropout output"}, {"repo_name": "Rocketknight1/esm2_t12_35M_UR50D-finetuned-cytosol-membrane-classification", "arch": "EsmForSequenceClassification", "model_type": "esm", "task": "text-classification", "layers": "input unsqueeze mul to add add add add layernorm linear view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view linear dropout add layernorm linear div erf add mul linear dropout add add layernorm linear div erf add mul linear dropout add layernorm linear view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view linear dropout add add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear div erf add mul linear dropout add add add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear div erf add mul linear dropout add add layernorm linear div erf add mul linear dropout add layernorm linear view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view linear dropout add add layernorm linear view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view linear dropout add layernorm linear div erf add mul linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear div erf add mul linear dropout add layernorm output"}, {"repo_name": "Finnish-NLP/wav2vec2-xlsr-300m-finnish-lm", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "sergunow/movie-chat", "arch": "BlenderbotForConditionalGeneration", "model_type": "blenderbot", "task": "text2text-generation", "layers": "input to to __getitem__ view blenderbotscaledwordembedding add dropout to add add isnan any isinf any to add add layernorm to output"}, {"repo_name": "kundank/dspt-electra-small-dbpedia14", "arch": "ElectraModel", "model_type": "electra", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "surajp/albert-base-sanskrit", "arch": "AlbertModel", "model_type": "albert", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to output"}, {"repo_name": "eduagarcia-temp/brwac_large_v1_2__checkpoint_6", "arch": "RobertaForMaskedLM", "model_type": "roberta", "task": "fill-mask", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "ClinicalNLP/SDOHv7", "arch": "DebertaV2ForSequenceClassification", "model_type": "deberta-v2", "task": "text-classification", "layers": "input embedding layernorm mul stabledropout linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm linear view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view linear stabledropout add layernorm add layernorm output"}, {"repo_name": "gchhablani/fnet-base-finetuned-stsb", "arch": "FNetForSequenceClassification", "model_type": "fnet", "task": "text-classification", "layers": "input embedding add add layernorm linear dropout add layernorm linear newgeluactivation linear dropout add layernorm fnetbasicfouriertransform add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm fnetbasicfouriertransform add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm add layernorm linear newgeluactivation linear dropout add layernorm output"}, {"repo_name": "Minata/plbart-large-finetuned-src_fm-to-testALL", "arch": "PLBartForConditionalGeneration", "model_type": "plbart", "task": "text2text-generation", "layers": "input to to view plbartscaledwordembedding add layernorm dropout to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm add layernorm isnan any to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm add layernorm isnan any to to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any to add layernorm add layernorm isnan any isinf any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm isnan any to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any isinf any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any to to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm add layernorm isnan any to add layernorm add layernorm isnan any to to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any to to to linear4bit view transpose contiguous to output"}, {"repo_name": "heegyu/kogpt-j-base", "arch": "GPTJForCausalLM", "model_type": "gptj", "task": "text-generation", "layers": "input view embedding dropout add add add add layernorm linear view __getitem__ cat permute to to output"}, {"repo_name": "Pollawat/mt5-small-thai-qa-qg", "arch": "MT5ForConditionalGeneration", "model_type": "mt5", "task": "text2text-generation", "layers": "input to to view embedding dropout to to mt5layernorm to linear4bit view transpose matmul transpose contiguous view linear4bit dropout add clamp to add to to mt5layernorm to linear4bit view transpose matmul transpose contiguous view linear4bit dropout add to mt5layernorm to linear4bit newgeluactivation mul dropout to linear dropout add to to mt5layernorm to linear4bit view transpose transpose matmul add float softmax type_as dropout matmul transpose contiguous view linear4bit dropout add to mt5layernorm to linear4bit mul dropout to linear dropout add to to add to mt5layernorm to linear4bit mul dropout to linear dropout add to to mt5layernorm to linear4bit view transpose transpose matmul add float softmax type_as dropout matmul transpose contiguous view linear4bit dropout add to mt5layernorm to linear4bit mul dropout to linear dropout add to to mt5layernorm to linear4bit view transpose transpose matmul add float softmax type_as dropout matmul transpose contiguous view linear4bit dropout add to add to to mt5layernorm to linear4bit view transpose matmul transpose contiguous view linear4bit dropout add to mt5layernorm to linear4bit mul dropout to linear dropout add to to mt5layernorm to linear4bit view transpose transpose matmul add float softmax type_as dropout matmul transpose contiguous view linear4bit dropout add to add mt5layernorm dropout to output"}, {"repo_name": "TransQuest/monotransquest-da-en_de-wiki", "arch": "XLMRobertaForSequenceClassification", "model_type": "xlm-roberta", "task": "text-classification", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "Birchlabs/mosaicml-mpt-7b-chat-qlora", "arch": "MPTForCausalLM", "model_type": "mpt", "task": "text-generation", "layers": "input embedding add add layernorm linear chunk reshape transpose transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add layernorm linear gelu linear dropout add add add add add add add add add add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add layernorm linear chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add layernorm linear chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add add add add add add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add add add add layernorm linear gelu linear dropout add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add layernorm linear gelu linear dropout add add add add add add add add add layernorm output"}, {"repo_name": "dccuchile/albert-base-2-spanish", "arch": "AlbertModel", "model_type": "albert", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to output"}, {"repo_name": "guhuawuli/swin-tiny-patch4-window7-224-finetuned-eurosat", "arch": "SwinForImageClassification", "model_type": "swin", "task": "image-classification", "layers": "input conv2d flatten transpose layernorm dropout add add add add view __getitem__ cat view layernorm linear add add add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear add add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm transpose adaptiveavgpool1d flatten output"}, {"repo_name": "sadakmed/distiluse-base-multilingual-cased-v2", "arch": "DistilBertModel", "model_type": "distilbert", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "mvonwyl/distilbert-base-uncased-finetuned-squad2", "arch": "DistilBertForQuestionAnswering", "model_type": "distilbert", "task": "question-answering", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "HasinMDG/XLM_Roberta_Large_Sentiment_Toward_Entity_on_Topics_Baseline_V2", "arch": "XLMRobertaModel", "model_type": "xlm-roberta", "task": "text-classification", "layers": "input to to ne int mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "facebook/mask2former-swin-tiny-cityscapes-panoptic", "arch": "Mask2FormerForUniversalSegmentation", "model_type": "mask2former", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm dropout add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add view permute permute contiguous view layernorm view permute contiguous conv2d groupnorm flatten transpose cat add linear view softmax view transpose reshape mul sum view transpose contiguous linear dropout add layernorm add layernorm add layernorm linear relu dropout linear dropout add layernorm add layernorm linear relu dropout linear dropout add layernorm linear masked_fill view split flatten transpose reshape grid_sample stack flatten mul sum view transpose contiguous linear dropout add layernorm linear relu dropout linear dropout add layernorm linear masked_fill view split flatten transpose reshape grid_sample stack flatten mul sum view transpose contiguous linear dropout add layernorm add layernorm linear masked_fill view split flatten transpose reshape grid_sample stack flatten mul sum view transpose contiguous linear dropout add layernorm linear relu dropout linear dropout add layernorm split transpose view sequential flatten add permute multi_head_attention_forward dropout add layernorm add layernorm linear relu dropout linear dropout add layernorm add multi_head_attention_forward dropout add layernorm add layernorm linear relu dropout linear dropout add layernorm add multi_head_attention_forward dropout add layernorm permute add linear mul view transpose contiguous view bmm softmax view view dropout bmm view transpose reshape linear permute dropout add layernorm add layernorm add multi_head_attention_forward dropout add layernorm permute add linear mul view transpose contiguous view bmm softmax view view dropout bmm view transpose reshape linear permute dropout add layernorm add layernorm layernorm transpose linear relu linear relu linear einsum output"}, {"repo_name": "bofenghuang/whisper-large-v2-cv11-french", "arch": "WhisperForConditionalGeneration", "model_type": "whisper", "task": "automatic-speech-recognition", "layers": "input view whisperpositionalembedding embedding add dropout layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add add add layernorm linear geluactivation dropout linear dropout add add add add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add add add add add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm output"}, {"repo_name": "uclanlp/plbart-java-en_XX", "arch": "PLBartForConditionalGeneration", "model_type": "plbart", "task": "text2text-generation", "layers": "input to to view plbartscaledwordembedding add layernorm dropout to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "nlpso/m1_ind_layers_ref_ptrn_cmbert_iob2_level_1", "arch": "CamembertForTokenClassification", "model_type": "camembert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "danschr/deberta-large-finetuned-BS_8-EPOCHS_5-LR_5e-05-ACC_GRAD_4-MAX_LENGTH_200", "arch": "DebertaForMaskedLM", "model_type": "deberta", "task": "fill-mask", "layers": "input embedding zeros_like debertalayernorm mul stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm linear view permute chunk transpose matmul add masked_fill softmax masked_fill stabledropout matmul permute contiguous view linear stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm output"}, {"repo_name": "spursyy/mT5_multilingual_XLSum_rust", "arch": "MT5ForConditionalGeneration", "model_type": "mt5", "task": "text2text-generation", "layers": "input view embedding dropout add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add add mt5layernorm linear view transpose matmul transpose contiguous view linear dropout add add mt5layernorm linear view transpose matmul transpose contiguous view linear dropout add add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add add add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add mt5layernorm linear mul dropout linear dropout add add mt5layernorm linear mul dropout linear dropout add mt5layernorm linear view transpose matmul add float type_as dropout matmul transpose contiguous view linear dropout add mt5layernorm linear mul dropout linear dropout add add mt5layernorm linear mul dropout linear dropout add mt5layernorm dropout output"}, {"repo_name": "hf-internal-testing/tiny-random-SwinForImageClassification", "arch": "SwinForImageClassification", "model_type": "swin", "task": "image-classification", "layers": "input conv2d flatten transpose layernorm dropout add add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm transpose adaptiveavgpool1d flatten output"}, {"repo_name": "Enoch2090/MAGI", "arch": "DistilBertModel", "model_type": "distilbert", "task": "feature-extraction", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "Xmm/led-large-16384-cnn_dailymail", "arch": "LEDForConditionalGeneration", "model_type": "led", "task": "text2text-generation", "layers": "input pad view embedding cat add layernorm dropout transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear div view transpose type_as masked_fill new_ones transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ full_like where __getitem__ full_like where add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ full_like where __getitem__ full_like where add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm __getitem__ output"}, {"repo_name": "philschmid/t5-11b-sharded", "arch": "T5WithLMHeadModel", "model_type": "t5", "task": "text2text-generation", "layers": "input to to embedding to add mistralrmsnorm to linear4bit mul linear4bit add to mistralrmsnorm to linear4bit view transpose output"}, {"repo_name": "microsoft/longcoder-base", "arch": "LongformerModel", "model_type": "longformer", "task": "feature-extraction", "layers": "input to pad to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "yangheng/deberta-v3-base-absa-v1.1", "arch": "DebertaForSequenceClassification", "model_type": "deberta-v2", "task": "text-classification", "layers": "input to to embedding layernorm mul stabledropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to output"}, {"repo_name": "hf-tiny-model-private/tiny-random-WavLMForSequenceClassification", "arch": "WavLMForSequenceClassification", "model_type": "wavlm", "task": "audio-classification", "layers": "input cumsum __getitem__ sub div add sub div add sub div add to sub"}, {"repo_name": "Gabriel/bart-base-cnn-xsum-cite-swe", "arch": "BartForConditionalGeneration", "model_type": "bart", "task": "text2text-generation", "layers": "input to to bartlearnedpositionalembedding to add layernorm dropout to add layernorm add layernorm isinf any isnan any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm isinf any isnan any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isinf any isnan any to to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "facebook/convnext-base-224", "arch": "ConvNextForImageClassification", "model_type": "convnext", "task": "image-classification", "layers": "input conv2d convnextlayernorm conv2d permute convnextlayernorm linear geluactivation linear mul permute add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d conv2d permute convnextlayernorm linear geluactivation linear mul permute add add add convnextlayernorm conv2d add add add add add add add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add add add add add add add add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add add convnextlayernorm conv2d add add add output"}, {"repo_name": "ThisIsMyUsername69/gpt-j-6B-16bit", "arch": "GPTJForCausalLM", "model_type": "gptj", "task": "text-generation", "layers": "input view embedding dropout add add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view permute matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add add add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add add layernorm linear view __getitem__ mul add cat permute to to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add add add add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear view __getitem__ mul add cat permute to to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add add add add add layernorm linear view __getitem__ mul add cat permute to to transpose matmul where div add softmax to dropout matmul permute contiguous view linear dropout add add layernorm view output"}, {"repo_name": "madlag/albert-base-v2-squad", "arch": "AlbertForQuestionAnswering", "model_type": "albert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to output"}, {"repo_name": "yujiepan/bert-base-uncased-sst2", "arch": "NNCFNetwork", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "tuner007/pegasus_paraphrase", "arch": "PegasusForConditionalGeneration", "model_type": "pegasus", "task": "text2text-generation", "layers": "input __getitem__ view embedding mul add dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm output"}, {"repo_name": "Eitanli/my_awesome_swag_model", "arch": "BertForMultipleChoice", "model_type": "bert", "task": "multiple-choice", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "WillHeld/roberta-base-coqa", "arch": "RobertaForQuestionAnswering", "model_type": "roberta", "task": "question-answering", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "nickmuchi/swin-tiny-patch4-window7-224-finetuned-eurosat", "arch": "SwinForImageClassification", "model_type": "swin", "task": "image-classification", "layers": "input conv2d flatten transpose layernorm dropout add add add add view __getitem__ cat view layernorm linear add add add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear add add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view view swindroppath add layernorm linear geluactivation linear dropout add layernorm transpose adaptiveavgpool1d flatten output"}, {"repo_name": "gretelai/mpt-7b", "arch": "MPTForCausalLM", "model_type": "mpt", "task": "text-generation", "layers": "input embedding add add layernorm linear chunk reshape transpose transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add layernorm linear gelu linear dropout add add add add add add add add add add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add layernorm linear chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add layernorm linear chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add add add add add add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add add add add layernorm linear gelu linear dropout add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add layernorm linear gelu linear dropout add add add add add add add add add layernorm output"}, {"repo_name": "alexziweiwang/mtl_manual_270012_epoch1", "arch": "Wav2Vec2ForCTCnCLS", "model_type": "wav2vec2", "task": "unknown", "layers": "input __getitem__ conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation transpose layernorm linear dropout transpose conv1d wav2vec2samepadlayer geluactivation transpose add dropout layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add layernorm linear view transpose contiguous scaled_dot_product_attention transpose reshape linear dropout add add add layernorm linear geluactivation dropout linear dropout add add layernorm linear geluactivation dropout linear dropout add add add layernorm output"}, {"repo_name": "Salesforce/codegen-16B-multi", "arch": "CodeGenForCausalLM", "model_type": "codegen", "task": "text-generation", "layers": "input view embedding dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm view output"}, {"repo_name": "sultan/BioM-ALBERT-xxlarge-SQuAD2", "arch": "AlbertForQuestionAnswering", "model_type": "albert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to output"}, {"repo_name": "nreimers/albert-small-v2", "arch": "AlbertModel", "model_type": "albert", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to output"}, {"repo_name": "sjrhuschlee/deberta-v3-large-squad2", "arch": "DebertaV2ForQuestionAnswering", "model_type": "deberta-v2", "task": "question-answering", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to to dropout linear linear mul to add view permute contiguous view bmm view permute contiguous view to to dropout linear linear mul to add stabledropout add layernorm to add layernorm to to to to dropout linear linear mul to add view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit clone add stabledropout add layernorm to to dropout linear linear mul to add geluactivation to to dropout linear linear mul to add stabledropout add layernorm to to to to dropout linear linear mul to add view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to to dropout linear linear mul to add stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit clone add view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to to dropout linear linear mul to add stabledropout add layernorm to add layernorm to to to add layernorm to to dropout linear linear mul to add geluactivation to to dropout linear linear mul to add stabledropout add layernorm to to to linear4bit clone add view permute contiguous view bmm view permute contiguous view to linear4bit clone add stabledropout add layernorm to linear4bit clone add geluactivation to to dropout linear linear mul to add stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit clone add view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to to dropout linear linear mul to add stabledropout add layernorm to linear4bit clone add geluactivation to to dropout linear linear mul to add stabledropout add layernorm to to to to dropout linear linear mul to add view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to to dropout linear linear mul to add stabledropout add layernorm to to dropout linear linear mul to add geluactivation to to dropout linear linear mul to add stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit clone add view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to to dropout linear linear mul to add stabledropout add layernorm to to dropout linear linear mul to add geluactivation to to dropout linear linear mul to add stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to to dropout linear linear mul to add view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit clone add stabledropout add layernorm to to dropout linear linear mul to add geluactivation to to dropout linear linear mul to add stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to to dropout linear linear mul to add geluactivation to to dropout linear linear mul to add stabledropout add layernorm to to to to dropout linear linear mul to add view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to to dropout linear linear mul to add stabledropout add layernorm to to dropout linear linear mul to add geluactivation to to dropout linear linear mul to add stabledropout add layernorm to to to linear4bit clone add view permute contiguous view bmm view permute contiguous view to to dropout linear linear mul to add stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit clone add geluactivation to to dropout linear linear mul to add stabledropout add layernorm to to to add layernorm to linear4bit clone add geluactivation to linear4bit clone add stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim", "arch": "Wav2Vec2ForSpeechClassification", "model_type": "wav2vec2", "task": "audio-classification", "layers": "input to to __getitem__ to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation transpose to layernorm to output"}, {"repo_name": "sgangireddy/whisper-largev2-mls-dutch", "arch": "WhisperForConditionalGeneration", "model_type": "whisper", "task": "automatic-speech-recognition", "layers": "input to to view embedding add dropout to add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add add to add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add to add add layernorm linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to output"}, {"repo_name": "Kowsher/bangla-bert", "arch": "BertForMaskedLM", "model_type": "bert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "jonatasgrosman/exp_w2v2t_es_unispeech_s990", "arch": "UniSpeechForCTC", "model_type": "unispeech", "task": "automatic-speech-recognition", "layers": "input to to __getitem__ to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation transpose to layernorm to output"}, {"repo_name": "Matthijs/mobilenet_v2_1.4_224", "arch": "MobileNetV2ForImageClassification", "model_type": "mobilenet_v2", "task": "image-classification", "layers": "input pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d add pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d relu6 pad conv2d batchnorm2d pad conv2d batchnorm2d relu6 output"}, {"repo_name": "dccuchile/albert-base-8-spanish-distilled-qa-sqac", "arch": "AlbertForQuestionAnswering", "model_type": "albert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit geluactivation linear4bit add layernorm to output"}, {"repo_name": "Ayaka/bart-base-cantonese", "arch": "BartModel", "model_type": "bart", "task": "fill-mask", "layers": "input to to bartlearnedpositionalembedding to add layernorm dropout to add layernorm add layernorm isinf any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isinf any isnan any to add layernorm add layernorm isinf any to add layernorm add layernorm isinf any isnan any to add layernorm add layernorm isinf any isnan any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isinf any isnan any to output"}, {"repo_name": "ktrapeznikov/biobert_v1.1_pubmed_squad_v2", "arch": "BertForQuestionAnswering", "model_type": "bert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "Salesforce/codegen2-1B", "arch": "CodeGenForCausalLM", "model_type": "codegen", "task": "text-generation", "layers": "input view embedding dropout add add add layernorm linear newgeluactivation linear dropout add add layernorm linear newgeluactivation linear dropout add add layernorm linear newgeluactivation linear dropout add add layernorm linear reshape split reshape reshape __getitem__ __getitem__ neg stack flatten mul add cat permute to matmul div where add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear newgeluactivation linear dropout add add add layernorm linear reshape split reshape reshape __getitem__ __getitem__ neg stack flatten mul add cat permute to matmul div where add softmax to dropout matmul permute contiguous view linear dropout add add add add add layernorm linear reshape split reshape reshape __getitem__ __getitem__ neg stack flatten mul add cat permute to matmul div where add softmax to dropout matmul permute contiguous view linear dropout add add layernorm linear reshape split reshape reshape permute output"}, {"repo_name": "hf-internal-testing/tiny-random-T5EncoderModel", "arch": "T5EncoderModel", "model_type": "t5", "task": "unknown", "layers": "input to to view embedding dropout to to add isinf any where neg clamp to add to to add to add to to add to add to to add to add to to t5layernorm to linear4bit view transpose matmul add type_as dropout matmul transpose contiguous view linear4bit dropout add to add t5layernorm dropout to output"}, {"repo_name": "shahrukhx01/muv2x-simcse-smole-bert-mlm", "arch": "BertForCL", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "ckiplab/albert-base-chinese-ner", "arch": "AlbertForTokenClassification", "model_type": "albert", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit relu linear4bit add layernorm to output"}, {"repo_name": "vasudevgupta/bigbird-pegasus-large-pubmed", "arch": "BigBirdPegasusForConditionalGeneration", "model_type": "bigbird_pegasus", "task": "text2text-generation", "layers": "input to to view bigbirdpegasusscaledwordembedding add dropout to add add isinf any to add add isinf any to add layernorm linear4bit fastgeluactivation linear4bit dropout add isinf any to add layernorm linear4bit fastgeluactivation linear4bit dropout add isinf any isnan any to add layernorm linear4bit fastgeluactivation linear4bit dropout add isinf any to add layernorm linear4bit fastgeluactivation linear4bit dropout add isinf any to add add isinf any to add add isinf any to add add isinf any isnan any to add layernorm linear4bit fastgeluactivation linear4bit dropout add isinf any to add add isinf any to add add isinf any isnan any to add layernorm linear4bit fastgeluactivation linear4bit dropout add isinf any isnan any to add layernorm linear4bit fastgeluactivation linear4bit dropout add isinf any isnan any to add add isinf any to add add isinf any layernorm to output"}, {"repo_name": "pravesh/bert-sentiment-analysis-nncf", "arch": "NNCFNetwork", "model_type": "distilbert", "task": "unknown", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm add layernorm to add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "vymn/Lina-Brain", "arch": "BlenderbotForConditionalGeneration", "model_type": "blenderbot", "task": "text2text-generation", "layers": "input to to __getitem__ view blenderbotscaledwordembedding add dropout to add add to add add isnan any isinf any layernorm to to to linear4bit view transpose contiguous to output"}, {"repo_name": "imvladikon/bert-base-uncased-jigsaw", "arch": "BertForMultiLabelSequenceClassification", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "mrm8488/bert-mini-finetuned-age_news-classification", "arch": "BertForSequenceClassification", "model_type": "bert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "panghee/pangheezoa", "arch": "RobertaForCausalLM", "model_type": "roberta", "task": "text-generation", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "mosaicml/mpt-7b-storywriter", "arch": "MPTForCausalLM", "model_type": "mpt", "task": "text-generation", "layers": "input embedding add add layernorm linear chunk reshape transpose transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add layernorm linear gelu linear dropout add add add add add add add add add add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add layernorm linear chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add layernorm linear chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add add add add add add add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add add add add add add add layernorm linear gelu linear dropout add add add layernorm linear chunk reshape transpose matmul permute contiguous view linear dropout add layernorm linear gelu linear dropout add add add add add add add add add layernorm output"}, {"repo_name": "galsenai/wavlm-large-waxal-keyword-spotting", "arch": "WavLMForSequenceClassification", "model_type": "wavlm", "task": "audio-classification", "layers": "input __getitem__ conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation conv1d transpose layernorm transpose geluactivation transpose layernorm output"}, {"repo_name": "jonatasgrosman/exp_w2v2t_es_unispeech-ml_s474", "arch": "UniSpeechForCTC", "model_type": "unispeech", "task": "automatic-speech-recognition", "layers": "input to to __getitem__ to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation transpose to layernorm linear4bit dropout to add dropout to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to add add to add add to add add to add add to add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to add add to add add to add add to add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to add add to add add to add add to add add to add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to add add layernorm to output"}, {"repo_name": "TokenfreeEMNLPSubmission/canine-base-finetuned-pos-ud-vietnamese-vtb", "arch": "CanineForTokenClassification", "model_type": "canine", "task": "token-classification", "layers": "input reshape float mul __getitem__ unsqueeze float __rsub__ mul add softmax dropout matmul permute contiguous view cat linear dropout add layernorm linear geluactivation linear dropout add layernorm cat transpose constantpad1d conv1d transpose geluactivation layernorm dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "mfreihaut/finetuned-audio-transcriber", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "Alireza1044/albert-base-v2-mnli", "arch": "AlbertForSequenceClassification", "model_type": "albert", "task": "text-classification", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm output"}, {"repo_name": "IbrahimSalah/syllable_wav2vec_17_.125", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "reralle/wavlm-basic_s-f-o_8batch_10sec_0.0001lr_unfrozen", "arch": "WavLMForSequenceClassification", "model_type": "wavlm", "task": "audio-classification", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "bhadresh-savani/bert-base-go-emotion", "arch": "DistilBertForMultilabelSequenceClassification", "model_type": "bert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "debajyotidatta/eduopt", "arch": "OPTForSequenceClassification", "model_type": "opt", "task": "text-classification", "layers": "input to to view embedding linear4bit add to add layernorm reshape add view layernorm to to linear4bit view transpose contiguous to output"}, {"repo_name": "DunnBC22/wav2vec2-base-Speech_Emotion_Recognition", "arch": "Wav2Vec2ForSequenceClassification", "model_type": "wav2vec2", "task": "audio-classification", "layers": "input __getitem__ conv1d groupnorm geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation transpose layernorm output"}, {"repo_name": "twmkn9/albert-base-v2-squad2", "arch": "AlbertForQuestionAnswering", "model_type": "albert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul transpose flatten linear4bit dropout add layernorm add layernorm to output"}, {"repo_name": "stevhliu/my_awesome_minds_model", "arch": "Wav2Vec2ForSequenceClassification", "model_type": "wav2vec2", "task": "audio-classification", "layers": "input __getitem__ conv1d groupnorm geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation conv1d geluactivation transpose layernorm output"}, {"repo_name": "vblagoje/dpr-question_encoder-single-lfqa-wiki", "arch": "DPRQuestionEncoder", "model_type": "dpr", "task": "feature-extraction", "layers": "input to to to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm __getitem__ linear to output"}, {"repo_name": "alexgshaw/hyperpartisan-classifier", "arch": "DistilBertForSequenceClassification", "model_type": "distilbert", "task": "text-classification", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "hf-tiny-model-private/tiny-random-LongformerModel", "arch": "LongformerModel", "model_type": "longformer", "task": "feature-extraction", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view new_zeros view transpose new_ones tril flip __getitem__ flip expand bool where expand bool where __getitem__ full_like __getitem__ full_like add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ full_like where __getitem__ full_like where add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to output"}, {"repo_name": "google/multiberts-seed_0-step_1900k", "arch": "BertForPreTraining", "model_type": "bert", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "facebook/maskformer-resnet50-ade20k-full", "arch": "MaskFormerForInstanceSegmentation", "model_type": "maskformer", "task": "unknown", "layers": "input conv2d batchnorm2d relu maxpool2d conv2d batchnorm2d add relu add relu add relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d groupnorm add conv2d groupnorm relu interpolate add conv2d groupnorm relu conv2d output"}, {"repo_name": "hw2942/Erlangshen-Longformer-110M-finetuning-wallstreetcn-morning-news-close-000001.SH-v1", "arch": "LongformerForSequenceClassification", "model_type": "longformer", "task": "text-classification", "layers": "input to pad to ne int mul long add to embedding add add layernorm dropout to to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ where full_like __getitem__ where full_like new_ones tril flip __getitem__ flip expand bool expand bool add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "LLukas22/all-MiniLM-L12-v2-qa-all", "arch": "BertForQuestionAnswering", "model_type": "bert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "s-nlp/xlmr_formality_classifier", "arch": "XLMRobertaForSequenceClassification", "model_type": "xlm-roberta", "task": "text-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "zhangfx7/deberta-base-finetuned-squad-pruned0.1", "arch": "DebertaForQuestionAnswering", "model_type": "deberta", "task": "question-answering", "layers": "input embedding debertalayernorm mul stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm output"}, {"repo_name": "approach0/dpr-cotmae-020", "arch": "DprEncoder", "model_type": "bert", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "Tejas3/Xlnet_base_80", "arch": "XLNetForSequenceClassification", "model_type": "xlnet", "task": "text-classification", "layers": "input transpose contiguous embedding dropout einsum einsum einsum dropout add layernorm add layernorm add layernorm add layernorm einsum add einsum add add mul softmax dropout einsum einsum dropout add layernorm add layernorm einsum einsum add add mul softmax dropout einsum einsum dropout add layernorm add layernorm add layernorm add layernorm einsum einsum einsum dropout add layernorm add layernorm einsum einsum add add mul softmax dropout einsum einsum dropout add layernorm add layernorm einsum einsum add add mul softmax dropout einsum einsum dropout add layernorm add layernorm einsum einsum add add mul softmax dropout einsum einsum dropout add layernorm add layernorm add layernorm add layernorm einsum einsum einsum dropout add layernorm add layernorm einsum einsum add add mul softmax dropout einsum einsum dropout add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "pcuenq/distilbert-base-uncased", "arch": "DistilBertModel", "model_type": "distilbert", "task": "feature-extraction", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "dhananjay2912/lsg-bart-base-4096-mediqa-chat-taskb-sectionwise-assessment_and_plan", "arch": "LSGBartForConditionalGeneration", "model_type": "bart", "task": "text2text-generation", "layers": "input to to view bartscaledwordembedding add layernorm dropout to add layernorm add layernorm isnan any to add layernorm add layernorm isnan any to add layernorm add layernorm isnan any isinf any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to output"}, {"repo_name": "jjonhwa/BIG-27500", "arch": "BertForV2QuestionAnswering", "model_type": "big_bird", "task": "unknown", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "tner/xlm-roberta-base-conll2003", "arch": "XLMRobertaForTokenClassification", "model_type": "xlm-roberta", "task": "token-classification", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "hf-internal-testing/tiny-random-DebertaForMaskedLM", "arch": "DebertaForMaskedLM", "model_type": "deberta", "task": "fill-mask", "layers": "input embedding add add debertalayernorm mul stabledropout add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm add debertalayernorm linear geluactivation linear stabledropout add debertalayernorm output"}, {"repo_name": "Hugh1111111/ModelTraining", "arch": "ResNetForImageClassification", "model_type": "resnet", "task": "image-classification", "layers": "input conv2d batchnorm2d relu maxpool2d conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu conv2d batchnorm2d add relu add relu add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d add relu add relu conv2d batchnorm2d relu conv2d batchnorm2d relu conv2d batchnorm2d add relu output"}, {"repo_name": "nbroad/longformer-base-health-fact", "arch": "LongformerForSequenceClassification", "model_type": "longformer", "task": "text-classification", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "nandezgarcia/roberta-base-bne-finetuned-recores-long", "arch": "RobertaForMultipleChoice", "model_type": "roberta", "task": "multiple-choice", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "asparius/electra-small-initial", "arch": "ElectraModel", "model_type": "electra", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "classla/sloberta-frenk-hate", "arch": "CamembertForSequenceClassification", "model_type": "camembert", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "classla/bert-base-german-dbmdz-uncased-geo", "arch": "BertForMultiLabelSequenceClassification", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "Gladiator/microsoft-deberta-v3-large_ner_wikiann", "arch": "DebertaV2ForTokenClassification", "model_type": "deberta-v2", "task": "token-classification", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to linear4bit view permute contiguous view transpose div bmm add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "textattack/facebook-bart-large-MNLI", "arch": "BartForSequenceClassification", "model_type": "bart", "task": "text-classification", "layers": "input to to view bartscaledwordembedding add layernorm dropout to add layernorm add layernorm isinf any to add layernorm add layernorm to add layernorm add layernorm isinf any to add layernorm add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to to linear4bit view transpose contiguous output"}, {"repo_name": "Rajaram1996/FacialEmoRecog", "arch": "ViTForImageClassification", "model_type": "vit", "task": "image-classification", "layers": "input conv2d flatten transpose cat add dropout add add add add layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout add add add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add add layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout add add add add add add layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout add add add add add add layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout add add layernorm __getitem__ linear tanh output"}, {"repo_name": "Rocketknight1/esm2_t6_8M_UR50D", "arch": "EsmForMaskedLM", "model_type": "esm", "task": "fill-mask", "layers": "input to to eq sum float div __rsub__ __getitem__ div to mul to to to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add to add to to layernorm to linear4bit view permute mul rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add to add to to to add to add to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add to add layernorm to output"}, {"repo_name": "facebook/mask2former-swin-large-cityscapes-instance", "arch": "Mask2FormerForUniversalSegmentation", "model_type": "mask2former", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm dropout add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add view permute permute contiguous view layernorm view permute contiguous conv2d groupnorm add conv2d groupnorm relu conv2d output"}, {"repo_name": "Primer/bart-squad2", "arch": "BartForQuestionAnswering", "model_type": "bart", "task": "question-answering", "layers": "input to to bartlearnedpositionalembedding to add layernorm dropout to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm isinf any to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm to add layernorm add layernorm isinf any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm isinf any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm isinf any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to to linear4bit view transpose contiguous output"}, {"repo_name": "facebook/convnextv2-large-22k-384", "arch": "ConvNextV2ForImageClassification", "model_type": "convnextv2", "task": "image-classification", "layers": "input conv2d convnextv2layernorm conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add conv2d permute convnextv2layernorm linear geluactivation convnextv2grn linear permute add add convnextv2layernorm conv2d add add add convnextv2layernorm conv2d add add add add add add add add add add add add add add add add add add add add add add add add add add add convnextv2layernorm conv2d add add add mean layernorm output"}, {"repo_name": "jjonhwa/500", "arch": "RobertaForV2QuestionAnswering", "model_type": "roberta", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "mtyrrell/CPU_Transport_GHG_Classifier", "arch": "MPNetForSequenceClassification", "model_type": "mpnet", "task": "text-classification", "layers": "input to to ne int type_as mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "inhee/8_BigBird_train_korquad-1-2_aihub_final", "arch": "BigBirdForQuestionAnswering", "model_type": "big_bird", "task": "question-answering", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "Alireza1044/mobilebert_rte", "arch": "MobileBertForSequenceClassification", "model_type": "mobilebert", "task": "text-classification", "layers": "input to to embedding cat linear4bit add add nonorm dropout to to to to to add nonorm to to to to add nonorm to to to to add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to to add nonorm to to to linear4bit nonorm to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to add nonorm to linear4bit dropout add nonorm to to to to linear4bit view permute matmul permute contiguous view to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to to add nonorm to to add nonorm to add nonorm to linear4bit dropout add nonorm to to to linear4bit nonorm to to add nonorm to to add nonorm to to add nonorm to to linear4bit relu to linear4bit add nonorm to linear4bit relu to linear4bit add nonorm to linear4bit dropout add nonorm to output"}, {"repo_name": "facebook/maskformer-swin-small-coco", "arch": "MaskFormerForInstanceSegmentation", "model_type": "maskformer", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm dropout layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view maskformerswindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view maskformerswindroppath add layernorm linear geluactivation linear dropout add layernorm permute view contiguous conv2d groupnorm add conv2d groupnorm relu conv2d output"}, {"repo_name": "neurae/opt-dnd-intents", "arch": "OPTForSequenceClassification", "model_type": "opt", "task": "text-classification", "layers": "input to to view embedding linear4bit add to add layernorm reshape linear4bit relu linear4bit dropout add view layernorm to add layernorm reshape add view layernorm to to linear4bit mul view transpose contiguous view bmm view add max view softmax to dropout bmm view transpose reshape linear4bit dropout add layernorm reshape add view layernorm to add layernorm reshape add view layernorm to add layernorm reshape add view layernorm to add layernorm reshape linear4bit relu linear4bit dropout add view layernorm to add layernorm reshape add view layernorm to to linear4bit view transpose contiguous to output"}, {"repo_name": "russab0/distilbert-qa", "arch": "DistilBertForMultipleChoice", "model_type": "distilbert", "task": "multiple-choice", "layers": "input to to embedding add layernorm dropout to to to to to linear4bit view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to output"}, {"repo_name": "TokenfreeEMNLPSubmission/canine-base-finetuned-pos-ud-hindi-hdtb", "arch": "CanineForTokenClassification", "model_type": "canine", "task": "token-classification", "layers": "input reshape float mul __getitem__ unsqueeze float __rsub__ mul add softmax dropout matmul permute contiguous view cat linear dropout add layernorm linear geluactivation linear dropout add layernorm cat transpose constantpad1d conv1d transpose geluactivation layernorm dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "gwlms/teams-base-dewiki-v1-discriminator", "arch": "ElectraForPreTraining", "model_type": "electra", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "ckiplab/albert-tiny-chinese", "arch": "AlbertForMaskedLM", "model_type": "albert", "task": "fill-mask", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm linear geluactivation linear add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm linear geluactivation linear add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm linear geluactivation linear add layernorm __getitem__ linear tanh output"}, {"repo_name": "DeeeTeeee01/mytest_trainer_base-cased", "arch": "XLNetForSequenceClassification", "model_type": "xlnet", "task": "text-classification", "layers": "input transpose contiguous embedding dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "Abris/input_classifier_v3_muppet", "arch": "RobertaForSequenceClassification", "model_type": "roberta", "task": "text-classification", "layers": "input to to ne int type_as add mul long add embedding add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "scaredmeow/xlnet-tagalog-sentiment-hatespeech", "arch": "XLNetForSequenceClassification", "model_type": "xlnet", "task": "text-classification", "layers": "input transpose contiguous embedding dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "Mekatebi/NMA_DL_2023_Project", "arch": "RobertaForSequenceClassification", "model_type": "roberta", "task": "text-classification", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "colinglab/BureauBERTo", "arch": "CamembertForMaskedLM", "model_type": "camembert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "guidecare/all-mpnet-base-v2-feature-extraction", "arch": "MPNetModel", "model_type": "mpnet", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "Taige/xlnet-edu", "arch": "XLNetForSequenceClassification", "model_type": "xlnet", "task": "text-classification", "layers": "input transpose contiguous embedding dropout add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm dropout permute contiguous output"}, {"repo_name": "ArtifactAI/led_large_16384_arxiv_summarization", "arch": "LEDForConditionalGeneration", "model_type": "led", "task": "text2text-generation", "layers": "input pad view embedding cat add layernorm dropout transpose linear div view transpose type_as masked_fill new_ones transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm transpose linear view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose new_ones tril flip __getitem__ expand bool where flip expand bool where __getitem__ full_like __getitem__ full_like add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm add layernorm transpose linear div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ full_like where __getitem__ full_like where add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm add layernorm add layernorm linear geluactivation dropout linear dropout add layernorm add layernorm add layernorm transpose linear div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ __getitem__ new_zeros view transpose __getitem__ full_like where __getitem__ full_like where add softmax masked_fill type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm add layernorm transpose linear view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm __getitem__ linear view transpose contiguous view bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm linear mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear dropout add layernorm linear mul view transpose contiguous view bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear geluactivation dropout linear dropout add layernorm output"}, {"repo_name": "chilliadgl/RG_fake_signatures", "arch": "DistilBertForTokenClassification", "model_type": "distilbert", "task": "token-classification", "layers": "input embedding add layernorm dropout linear view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "henryk/bert-base-multilingual-cased-finetuned-dutch-squad2", "arch": "BertForQuestionAnswering", "model_type": "bert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "jonatasgrosman/exp_w2v2t_zh-cn_unispeech_s189", "arch": "UniSpeechForCTC", "model_type": "unispeech", "task": "automatic-speech-recognition", "layers": "input to to __getitem__ to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation transpose to layernorm linear4bit dropout to to transpose to conv1d unispeechsamepadlayer geluactivation transpose add dropout to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to add add to add add to add add to add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add add to add add to add add to layernorm to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to add add layernorm to output"}, {"repo_name": "facebook/mask2former-swin-large-ade-semantic", "arch": "Mask2FormerForUniversalSegmentation", "model_type": "mask2former", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm dropout add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear add layernorm linear geluactivation linear dropout add add layernorm linear geluactivation linear dropout add view permute permute contiguous view layernorm view permute contiguous mask2formersinepositionembedding flatten transpose add cat add linear view softmax view transpose reshape mul sum view transpose contiguous linear dropout add layernorm add layernorm add layernorm add layernorm split transpose view mask2formersinepositionembedding flatten permute add multi_head_attention_forward dropout add layernorm permute add linear mul view transpose contiguous view bmm softmax view view dropout bmm view transpose reshape linear permute dropout add layernorm add layernorm layernorm transpose linear relu linear relu linear einsum output"}, {"repo_name": "dhananjay2912/lsg-bart-base-4096-pubmed-mediqa-chat-taskb-sectionwise-combiner-pubmed", "arch": "LSGBartForConditionalGeneration", "model_type": "bart", "task": "text2text-generation", "layers": "input to to view bartscaledwordembedding add layernorm dropout to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to add layernorm add layernorm isnan any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any isinf any to add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm isnan any isinf any to add layernorm add layernorm isnan any isinf any to to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to add layernorm add layernorm add layernorm to output"}, {"repo_name": "ClaudeYang/awesome_fb_model", "arch": "BartForSequenceClassification", "model_type": "bart", "task": "zero-shot-classification", "layers": "input to to view bartscaledwordembedding add layernorm dropout to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm linear4bit geluactivation dropout linear4bit dropout add layernorm to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm isinf any to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm to to linear4bit view transpose contiguous scaled_dot_product_attention transpose reshape linear4bit dropout add layernorm add layernorm isinf any to to to linear4bit view transpose contiguous output"}, {"repo_name": "facebook/nllb-200-distilled-600M", "arch": "M2M100ForConditionalGeneration", "model_type": "m2m_100", "task": "translation", "layers": "input __getitem__ view embedding mul m2m100sinusoidalpositionalembedding to add dropout layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add layernorm linear relu dropout linear dropout add layernorm linear view transpose contiguous reshape bmm view transpose reshape linear dropout add add layernorm linear view transpose contiguous output"}, {"repo_name": "thunlp/Lawformer", "arch": "LongformerForMaskedLM", "model_type": "longformer", "task": "fill-mask", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "deepset/all-mpnet-base-v2-table", "arch": "MPNetModel", "model_type": "mpnet", "task": "sentence-similarity", "layers": "input embedding add layernorm dropout linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "andypyc/news_classifier-distilbert-base-uncased-subject-only", "arch": "DistilBertForSequenceClassification", "model_type": "distilbert", "task": "text-classification", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to to to to linear4bit view transpose div matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm add layernorm to add layernorm add layernorm to add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm add layernorm to output"}, {"repo_name": "imohammad12/GRS-Grammar-Checker-DeBerta", "arch": "DebertaForSequenceClassification", "model_type": "deberta", "task": "text-classification", "layers": "input embedding debertalayernorm mul stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm output"}, {"repo_name": "jjonhwa/BIG-24500", "arch": "BertForV2QuestionAnswering", "model_type": "big_bird", "task": "unknown", "layers": "input to to embedding add add dropout layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "carolanderson/roberta-base-food-ner", "arch": "RobertaForTokenClassification", "model_type": "roberta", "task": "token-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "Corianas/gpt-j-6B-Dolly", "arch": "GPTJForCausalLM", "model_type": "gptj", "task": "text-generation", "layers": "input to view embedding dropout to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add to add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit view __getitem__ cat permute to matmul where div add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit view permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add layernorm view to output"}, {"repo_name": "facebook/mms-1b-all", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to to __getitem__ to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation to conv1d transpose layernorm transpose geluactivation transpose to layernorm linear4bit dropout to add dropout to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to add add add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to add add add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit mul view transpose contiguous view bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape transpose bmm view add view softmax dropout bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to add layernorm to linear4bit geluactivation dropout linear4bit dropout add add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear4bit add to layernorm to linear4bit view transpose contiguous reshape bmm view transpose reshape linear4bit dropout add layernorm to linear4bit geluactivation dropout linear4bit dropout add to layernorm linear4bit relu linear add layernorm to output"}, {"repo_name": "SparkBeyond/roberta-large-sts-b", "arch": "RobertaForSequenceClassification", "model_type": "roberta", "task": "text-classification", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "EchoStreet/mpt-7b", "arch": "MPTForCausalLM", "model_type": "mpt", "task": "text-generation", "layers": "input to embedding to add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul permute contiguous view linear4bit dropout add to add to layernorm to linear4bit chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add to add to layernorm to linear4bit chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul permute contiguous view linear4bit dropout add to add to layernorm to linear4bit chunk reshape transpose matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to add layernorm to linear4bit gelu linear4bit dropout add to add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add to add to add to add to add to add to layernorm to linear4bit chunk reshape transpose transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to add to add to layernorm to linear4bit chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit gelu linear4bit dropout add to layernorm to linear4bit chunk reshape transpose matmul permute contiguous view linear4bit dropout add to add to layernorm to linear4bit chunk reshape transpose matmul mul add masked_fill float softmax to dropout matmul permute contiguous view linear4bit dropout add to add layernorm to output"}, {"repo_name": "spacemanidol/esci-us-mpnet-crossencoder", "arch": "MPNetForSequenceClassification", "model_type": "mpnet", "task": "text-classification", "layers": "input to to ne int type_as mul long add embedding add layernorm dropout to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add add softmax dropout matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view linear4bit dropout add layernorm to add layernorm to to add layernorm to add layernorm to to add layernorm to add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "jinhybr/OCR-LayoutLMv3-Invoice", "arch": "LayoutLMv3ForTokenClassification", "model_type": "layoutlmv3", "task": "token-classification", "layers": "input conv2d flatten transpose cat add dropout layernorm layernorm dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute transpose matmul add add div amax unsqueeze sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute div matmul add add div sub mul softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "hustvl/yolos-small-dwr", "arch": "YolosForObjectDetection", "model_type": "yolos", "task": "object-detection", "layers": "input conv2d flatten transpose cat add dropout layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add add add add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add add add add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add add add layernorm linear geluactivation linear dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add add add add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add add layernorm linear view permute scaled_dot_product_attention permute contiguous view linear dropout add add layernorm output"}, {"repo_name": "pzelasko/longformer-swda-nolower", "arch": "LongformerForTokenClassification", "model_type": "longformer", "task": "token-classification", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "kredor/punctuate-all", "arch": "XLMRobertaForTokenClassification", "model_type": "xlm-roberta", "task": "token-classification", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm output"}, {"repo_name": "hf-internal-testing/tiny-random-BigBirdPegasusForConditionalGeneration", "arch": "BigBirdPegasusForConditionalGeneration", "model_type": "bigbird_pegasus", "task": "text2text-generation", "layers": "input to to view bigbirdpegasusscaledwordembedding add dropout to add add to add add isinf any isnan any layernorm to output"}, {"repo_name": "bhavikardeshna/xlm-roberta-base-vietnamese", "arch": "XLMRobertaForQuestionAnswering", "model_type": "xlm-roberta", "task": "question-answering", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "google/pegasus-newsroom", "arch": "PegasusForConditionalGeneration", "model_type": "pegasus", "task": "text2text-generation", "layers": "input __getitem__ view embedding mul add dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm linear view transpose contiguous reshape transpose bmm softmax dropout bmm view transpose reshape linear dropout add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add add layernorm output"}, {"repo_name": "reciprocate/vicuna-13b_rm_oasst-hh", "arch": "LlamaForSequenceClassification", "model_type": "llama", "task": "text-classification", "layers": "input embedding llamarmsnorm linear view transpose scaled_dot_product_attention transpose contiguous view linear add add add llamarmsnorm linear mul linear add add llamarmsnorm linear mul linear add add llamarmsnorm linear mul linear add llamarmsnorm linear view transpose scaled_dot_product_attention transpose contiguous view linear add llamarmsnorm linear silu mul linear add add llamarmsnorm linear mul linear add add add add add add llamarmsnorm linear mul linear add llamarmsnorm linear view transpose __getitem__ neg cat mul add scaled_dot_product_attention transpose contiguous view linear add add llamarmsnorm linear view transpose __getitem__ neg cat mul add scaled_dot_product_attention transpose contiguous view linear add llamarmsnorm linear mul linear add llamarmsnorm linear view transpose scaled_dot_product_attention transpose contiguous view linear add add add add add add add llamarmsnorm linear mul linear add add add add llamarmsnorm linear mul linear add llamarmsnorm linear view transpose scaled_dot_product_attention transpose contiguous view linear add add add llamarmsnorm linear mul linear add add add add add add add llamarmsnorm linear view transpose __getitem__ neg cat mul add scaled_dot_product_attention transpose contiguous view linear add add add llamarmsnorm linear mul linear add add llamarmsnorm linear mul linear add llamarmsnorm linear view transpose scaled_dot_product_attention transpose contiguous view linear add llamarmsnorm linear mul linear add llamarmsnorm linear view transpose scaled_dot_product_attention transpose contiguous view linear add add add llamarmsnorm linear mul linear add add llamarmsnorm linear mul linear add add add add add add llamarmsnorm linear mul linear add add llamarmsnorm linear mul linear add llamarmsnorm linear view transpose scaled_dot_product_attention transpose contiguous view linear add add llamarmsnorm linear view transpose scaled_dot_product_attention transpose contiguous view linear add llamarmsnorm linear mul linear add llamarmsnorm linear view transpose scaled_dot_product_attention transpose contiguous view linear add add add add add llamarmsnorm linear mul linear add llamarmsnorm linear view transpose __getitem__ neg cat mul add scaled_dot_product_attention transpose contiguous view linear add add llamarmsnorm linear view transpose scaled_dot_product_attention transpose contiguous view linear add llamarmsnorm linear mul linear add llamarmsnorm output"}, {"repo_name": "austin/mimic-pubmed-deberta-small", "arch": "DebertaV2ForMaskedLM", "model_type": "deberta-v2", "task": "fill-mask", "layers": "input to to embedding zeros_like layernorm mul stabledropout to to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather transpose div add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute contiguous view bmm gather div add add add view masked_fill softmax masked_fill stabledropout view bmm view permute contiguous view to linear4bit stabledropout add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit stabledropout add layernorm to output"}, {"repo_name": "LucasS/albertABSA", "arch": "AlbertForQuestionAnswering", "model_type": "albert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to linear4bit to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to to to linear4bit view permute matmul transpose flatten linear4bit dropout add layernorm linear4bit newgeluactivation linear4bit add layernorm to output"}, {"repo_name": "seeksery/DialoGPT-calig3", "arch": "GPT2LMHeadModel", "model_type": "gpt2", "task": "text-generation", "layers": "input view embedding add dropout layernorm conv1d split view permute output"}, {"repo_name": "Matthijs/mobilevit-xx-small", "arch": "MobileViTForImageClassification", "model_type": "mobilevit", "task": "image-classification", "layers": "input conv2d batchnorm2d silu conv2d batchnorm2d silu conv2d batchnorm2d silu conv2d batchnorm2d add conv2d batchnorm2d silu conv2d batchnorm2d silu conv2d batchnorm2d conv2d batchnorm2d silu conv2d batchnorm2d silu conv2d batchnorm2d add conv2d batchnorm2d silu conv2d batchnorm2d silu conv2d batchnorm2d add conv2d batchnorm2d silu conv2d batchnorm2d silu conv2d batchnorm2d conv2d batchnorm2d silu conv2d reshape transpose reshape transpose reshape layernorm linear view permute matmul permute contiguous view linear dropout add add layernorm linear view permute transpose matmul div softmax dropout matmul permute contiguous view linear dropout add add layernorm contiguous view transpose reshape transpose reshape conv2d batchnorm2d silu cat conv2d batchnorm2d silu conv2d batchnorm2d silu conv2d batchnorm2d silu conv2d batchnorm2d cat conv2d batchnorm2d silu conv2d batchnorm2d silu conv2d batchnorm2d silu conv2d batchnorm2d cat conv2d batchnorm2d silu conv2d batchnorm2d silu mean output"}, {"repo_name": "EMBEDDIA/sloberta", "arch": "CamembertForMaskedLM", "model_type": "camembert", "task": "fill-mask", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "firqaaa/indo-dpr-ctx_encoder-single-squad-base", "arch": "DPRContextEncoder", "model_type": "dpr", "task": "feature-extraction", "layers": "input to to to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to output"}, {"repo_name": "anakin87/electra-italian-xxl-cased-squad-it", "arch": "ElectraForQuestionAnswering", "model_type": "electra", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm add layernorm add layernorm output"}, {"repo_name": "lgrobol/electra-minuscule-generator", "arch": "ElectraForMaskedLM", "model_type": "electra", "task": "fill-mask", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm output"}, {"repo_name": "facebook/mask2former-swin-tiny-ade-semantic", "arch": "Mask2FormerForUniversalSegmentation", "model_type": "mask2former", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm dropout layernorm view pad view permute contiguous view view linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add add add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute transpose matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add view __getitem__ cat view layernorm linear layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add add layernorm view pad roll view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add add layernorm view pad view permute contiguous view view linear view permute matmul permute contiguous view linear dropout view view permute contiguous view __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add layernorm view pad roll view permute contiguous view view linear view permute matmul div add view add view softmax dropout matmul permute contiguous view linear dropout view view permute contiguous view roll __getitem__ contiguous view swindroppath add layernorm linear geluactivation linear dropout add add add add layernorm linear geluactivation linear dropout add view permute permute contiguous view layernorm view permute contiguous conv2d groupnorm flatten transpose cat linear masked_fill view split flatten transpose reshape grid_sample stack flatten mul sum view transpose contiguous linear dropout add layernorm add layernorm add linear view softmax view transpose reshape mul sum view transpose contiguous linear dropout add layernorm add layernorm linear masked_fill view split flatten transpose reshape grid_sample stack flatten mul sum view transpose contiguous linear dropout add layernorm add layernorm add layernorm add layernorm add linear view softmax view transpose reshape mul sum view transpose contiguous linear dropout add layernorm add layernorm add linear view softmax view transpose reshape mul sum view transpose contiguous linear dropout add layernorm add layernorm split transpose view mask2formersinepositionembedding flatten permute add multi_head_attention_forward dropout add layernorm add layernorm linear relu dropout linear dropout add layernorm layernorm transpose linear relu linear relu linear einsum output"}, {"repo_name": "soheeyang/rdr-ctx_encoder-single-nq-base", "arch": "DPRContextEncoder", "model_type": "dpr", "task": "unknown", "layers": "input to to to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to output"}, {"repo_name": "davanstrien/distilbert-base-cased_fine_tuned_food_ner", "arch": "DistilBertForTokenClassification", "model_type": "distilbert", "task": "token-classification", "layers": "input to to embedding add layernorm dropout to to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose div matmul masked_fill softmax dropout matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to to to to linear4bit view transpose matmul transpose contiguous view linear4bit add layernorm to linear4bit geluactivation linear4bit dropout add layernorm to output"}, {"repo_name": "haohuang40/esm2_t12_35M_UR50D-finetuned-secondary-structure", "arch": "EsmForTokenClassification", "model_type": "esm", "task": "token-classification", "layers": "input to to embedding masked_fill mul div to mul to to to to layernorm to linear4bit view permute rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add to add to to layernorm to linear4bit view permute mul rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to to add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute mul rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute mul rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute mul rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute rotaryembedding transpose matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add to to layernorm to linear4bit view permute mul rotaryembedding matmul add softmax dropout to matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit div erf add mul to linear4bit dropout add layernorm to output"}, {"repo_name": "Salesforce/codegen-2B-nl", "arch": "CodeGenForCausalLM", "model_type": "codegen", "task": "text-generation", "layers": "input to to view embedding dropout to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to add to add to layernorm to linear4bit reshape split reshape reshape permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit reshape split reshape reshape permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit reshape split reshape reshape __getitem__ __getitem__ neg stack flatten mul add cat permute to transpose matmul div where add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit reshape split reshape reshape permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit reshape split reshape reshape permute matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit reshape split reshape reshape permute matmul permute contiguous view linear4bit dropout add add to add to layernorm to linear4bit reshape split reshape reshape permute matmul permute contiguous view linear4bit dropout add add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit reshape split reshape reshape __getitem__ __getitem__ stack flatten mul add cat permute to transpose matmul div where add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit reshape split reshape reshape __getitem__ __getitem__ stack flatten mul add cat permute to transpose matmul div where add softmax to dropout matmul permute contiguous view linear4bit dropout add add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to layernorm to linear4bit reshape split reshape reshape permute matmul permute contiguous view linear4bit dropout add add to add to layernorm to linear4bit reshape split reshape reshape permute matmul permute contiguous view linear4bit dropout add add to add to layernorm to linear4bit newgeluactivation linear4bit dropout add add to add layernorm view linear to to output"}, {"repo_name": "chambliss/distilbert-for-food-extraction", "arch": "DistilBertForTokenClassification", "model_type": "distilbert", "task": "token-classification", "layers": "input embedding add layernorm dropout linear view transpose transpose matmul masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm linear view transpose transpose matmul expand_as masked_fill softmax dropout matmul transpose contiguous view linear add layernorm add layernorm output"}, {"repo_name": "shahruk10/wav2vec2-xls-r-300m-bengali-commonvoice", "arch": "Wav2Vec2ForCTC", "model_type": "wav2vec2", "task": "automatic-speech-recognition", "layers": "input to cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "raybin/model_out", "arch": "BertForMultiLabelSequenceClassification", "model_type": "bert", "task": "unknown", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "platzi/platzi-beans-beit-model-eduardo-ag", "arch": "BeitForImageClassification", "model_type": "beit", "task": "image-classification", "layers": "input conv2d flatten transpose cat dropout layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul add layernorm linear geluactivation linear dropout mul add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute matmul permute contiguous view linear dropout mul beitdroppath add layernorm linear geluactivation linear dropout mul beitdroppath add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout mul beitdroppath add add __getitem__ mean layernorm output"}, {"repo_name": "djsull/kobigbird-spam-multi-label", "arch": "BigBirdForSequenceClassification", "model_type": "big_bird", "task": "text-classification", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to output"}, {"repo_name": "abdulsalama/solar-swin-large-coco-2023042423", "arch": "MaskFormerForInstanceSegmentation", "model_type": "maskformer", "task": "unknown", "layers": "input conv2d flatten transpose layernorm dropout add add add add layernorm permute view contiguous conv2d groupnorm add conv2d groupnorm relu conv2d output"}, {"repo_name": "hfl/chinese-pert-large-mrc", "arch": "BertForQuestionAnswering", "model_type": "bert", "task": "question-answering", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "am-infoweb/MRR-sqv2-long", "arch": "LongformerForQuestionAnswering", "model_type": "longformer", "task": "question-answering", "layers": "input to pad to embedding add add layernorm dropout to to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape pad as_strided einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit view transpose transpose reshape view as_strided einsum pad view __getitem__ __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to transpose linear4bit div view transpose type_as masked_fill transpose reshape view as_strided einsum pad view __getitem__ new_zeros view transpose add type_as dropout transpose reshape pad view __getitem__ view __getitem__ einsum view transpose transpose reshape contiguous transpose to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm __getitem__ to __getitem__ linear tanh to output"}, {"repo_name": "shaheen1998/segformer-b0-finetuned-segments-sidewalk-2", "arch": "SegformerForSemanticSegmentation", "model_type": "segformer", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm layernorm permute reshape conv2d reshape permute layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout add add add layernorm reshape permute contiguous conv2d flatten transpose layernorm layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout segformerdroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add layernorm linear view permute matmul div softmax dropout matmul permute contiguous view linear dropout segformerdroppath add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add layernorm reshape permute contiguous conv2d flatten transpose layernorm add add add add layernorm reshape permute contiguous conv2d flatten transpose layernorm add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add add layernorm linear transpose view conv2d flatten transpose geluactivation dropout linear dropout segformerdroppath add layernorm reshape permute contiguous output"}, {"repo_name": "hf-internal-testing/tiny-random-Speech2TextForConditionalGeneration", "arch": "Speech2TextForConditionalGeneration", "model_type": "speech_to_text", "task": "automatic-speech-recognition", "layers": "input to to view speech2textsinusoidalpositionalembedding add dropout to add add layernorm linear4bit relu dropout linear4bit dropout add to add add add layernorm to output"}, {"repo_name": "hf-tiny-model-private/tiny-random-RobertaModel", "arch": "RobertaModel", "model_type": "roberta", "task": "feature-extraction", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm __getitem__ linear tanh output"}, {"repo_name": "nielsr/convnext-xlarge-224-22k-1k", "arch": "ConvNextForImageClassification", "model_type": "convnext", "task": "image-classification", "layers": "input conv2d convnextlayernorm conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add conv2d permute convnextlayernorm linear geluactivation linear mul permute add convnextlayernorm conv2d add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add add add add conv2d permute convnextlayernorm linear geluactivation linear mul permute add add add add add add add add add add convnextlayernorm conv2d add add add output"}, {"repo_name": "approach0/dpr-vanilla-bert-520", "arch": "DprEncoder", "model_type": "bert", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "Karim-Gamal/BERT-base-finetuned-SemEval-2018-emojis-cen-2", "arch": "BertForSequenceClassification", "model_type": "bert", "task": "text-classification", "layers": "input to to embedding add layernorm dropout to to to to linear4bit view permute transpose matmul add div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul add div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul add div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul add div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul add div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul add div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul add div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul add div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute matmul add div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul add div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to __getitem__ linear tanh to output"}, {"repo_name": "LucasS/bigbirdABSA", "arch": "BigBirdForQuestionAnswering", "model_type": "big_bird", "task": "question-answering", "layers": "input to to embedding add add dropout layernorm to to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit newgeluactivation to linear4bit dropout add layernorm to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "nlpconnect/roberta-base-squad2-nq", "arch": "RobertaForQuestionAnswering", "model_type": "roberta", "task": "question-answering", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm output"}, {"repo_name": "LeBenchmark/wav2vec-FR-1K-Male-base", "arch": "Wav2Vec2ForPreTraining", "model_type": "wav2vec2", "task": "pretraining", "layers": "input cumsum __getitem__ sub div add sub div add sub div add sub div add sub div add sub div add sub div add to sub"}, {"repo_name": "samchain/EconoBert", "arch": "BertForPreTraining", "model_type": "bert", "task": "pretraining", "layers": "input to to embedding add add layernorm dropout to to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute scaled_dot_product_attention transpose reshape to linear4bit dropout add layernorm to add layernorm to output"}, {"repo_name": "masakhane/afri-mt5-base", "arch": "MT5ForConditionalGeneration", "model_type": "mt5", "task": "text2text-generation", "layers": "input to to view embedding dropout to to add clamp to add to to add to mt5layernorm to linear4bit mul dropout to linear dropout add to to add to add to to mt5layernorm to linear4bit view transpose transpose matmul add type_as dropout matmul transpose contiguous view linear4bit dropout add to mt5layernorm to linear4bit newgeluactivation mul dropout to linear dropout add to to add to add to to add to add to to add to add to to mt5layernorm to linear4bit view transpose matmul add type_as dropout matmul transpose contiguous view linear4bit dropout add to add to to add to mt5layernorm to linear4bit mul dropout to linear dropout add to to mt5layernorm to linear4bit view transpose matmul add type_as dropout matmul transpose contiguous view linear4bit dropout add to add to to add to add to to add to add mt5layernorm dropout to to to to linear4bit view transpose matmul transpose contiguous view linear4bit dropout add to mt5layernorm to linear4bit mul dropout to linear dropout add to to add to mt5layernorm to linear4bit view transpose matmul add type_as dropout matmul transpose contiguous view linear4bit dropout add to mt5layernorm to linear4bit newgeluactivation mul dropout to linear dropout add to to add to mt5layernorm to linear4bit view transpose matmul add type_as dropout matmul transpose contiguous view linear4bit dropout add to add to to add to add to add mt5layernorm dropout to output"}, {"repo_name": "sohamtiwari3120/scideberta-cs-tdm-pretrained-finetuned-ner", "arch": "DebertaForTokenClassification", "model_type": "deberta", "task": "token-classification", "layers": "input embedding debertalayernorm mul stabledropout add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm add debertalayernorm output"}, {"repo_name": "erayyildiz/electra-turkish-cased", "arch": "ElectraModel", "model_type": "electra", "task": "feature-extraction", "layers": "input to to embedding add add layernorm dropout linear4bit to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to add layernorm to output"}, {"repo_name": "Deopusi/model1_5000_0.5_0_0_50", "arch": "DistilBertForMultilabelSequenceClassification", "model_type": "distilbert", "task": "unknown", "layers": "input to to embedding add layernorm dropout to to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to add layernorm add layernorm to output"}, {"repo_name": "facebook/mask2former-swin-base-coco-panoptic", "arch": "Mask2FormerForUniversalSegmentation", "model_type": "mask2former", "task": "image-segmentation", "layers": "input conv2d flatten transpose layernorm dropout add add add add view permute permute contiguous view layernorm view permute contiguous conv2d groupnorm add conv2d groupnorm relu conv2d output"}, {"repo_name": "Alireza1044/albert-base-v2-qnli", "arch": "AlbertForSequenceClassification", "model_type": "albert", "task": "text-classification", "layers": "input unsqueeze unsqueeze to __rsub__ mul add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute transpose matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm linear view permute matmul transpose flatten linear dropout add layernorm add layernorm output"}, {"repo_name": "Blaxzter/LaBSE-sentence-embeddings", "arch": "BertModel", "model_type": "bert", "task": "sentence-similarity", "layers": "input __getitem__ to __rsub__ mul add softmax dropout matmul permute contiguous view linear dropout add layernorm add layernorm linear view permute matmul div add softmax dropout matmul permute contiguous view linear dropout add layernorm linear geluactivation linear dropout add layernorm linear view permute matmul permute contiguous view linear dropout add layernorm add layernorm __getitem__ linear tanh output"}, {"repo_name": "AIDA-UPM/MSTSb_stsb-xlm-r-multilingual", "arch": "XLMRobertaModel", "model_type": "xlm-roberta", "task": "sentence-similarity", "layers": "input to to ne int cumsum type_as add mul long add embedding add layernorm dropout to to to to add layernorm to add layernorm to to to add layernorm to add layernorm to to to linear4bit view permute matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to linear4bit view permute transpose matmul div add softmax dropout matmul permute contiguous view to linear4bit dropout add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to linear4bit geluactivation to linear4bit dropout add layernorm to to to add layernorm to add layernorm to output"}]