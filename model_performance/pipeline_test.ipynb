{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model_collection/filtered_models.json', 'r') as f:\n",
    "    filtered_models = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlbertForMaskedLM 21\n",
      "AlbertForPreTraining 10\n",
      "AlbertForQuestionAnswering 23\n",
      "AlbertForSequenceClassification 54\n",
      "AlbertForTokenClassification 11\n",
      "AlbertModel 7\n",
      "AutoModel 5\n",
      "BartForConditionalGeneration 354\n",
      "BartForQuestionAnswering 27\n",
      "BartForSequenceClassification 18\n",
      "BeitForImageClassification 33\n",
      "BeitForMaskedImageModeling 2\n",
      "BeitForSemanticSegmentation 2\n",
      "BertForMaskedLM 299\n",
      "BertForMultipleChoice 13\n",
      "BertForPreTraining 107\n",
      "BertForQuestionAnswering 363\n",
      "BertForSequenceClassification 724\n",
      "BertForTokenClassification 465\n",
      "BertModel 92\n",
      "BigBirdForMaskedLM 4\n",
      "BigBirdForPreTraining 1\n",
      "BigBirdForQuestionAnswering 39\n",
      "BigBirdForSequenceClassification 5\n",
      "BigBirdPegasusForConditionalGeneration 6\n",
      "BlenderbotForConditionalGeneration 9\n",
      "BlenderbotSmallForConditionalGeneration 2\n",
      "BloomForCausalLM 9\n",
      "BloomForQuestionAnswering 2\n",
      "BloomModel 18\n",
      "CLIPModel 2\n",
      "CTRLLMHeadModel 5\n",
      "CamembertForMaskedLM 2\n",
      "CamembertForQuestionAnswering 3\n",
      "CamembertForSequenceClassification 26\n",
      "CamembertForTokenClassification 6\n",
      "CamembertModel 2\n",
      "CanineForSequenceClassification 5\n",
      "CanineModel 2\n",
      "CodeGenForCausalLM 4\n",
      "ConvBertForMaskedLM 3\n",
      "ConvBertForSequenceClassification 3\n",
      "ConvBertModel 5\n",
      "ConvNextForImageClassification 40\n",
      "CvtForImageClassification 8\n",
      "DPRContextEncoder 6\n",
      "DPRQuestionEncoder 7\n",
      "DPRReader 2\n",
      "DPTForSemanticSegmentation 2\n",
      "Data2VecAudioForCTC 7\n",
      "Data2VecAudioModel 2\n",
      "Data2VecTextForQuestionAnswering 2\n",
      "Data2VecTextForSequenceClassification 4\n",
      "Data2VecTextModel 1\n",
      "Data2VecVisionForImageClassification 2\n",
      "Data2VecVisionModel 2\n",
      "DebertaForQuestionAnswering 5\n",
      "DebertaForSequenceClassification 11\n",
      "DebertaForTokenClassification 5\n",
      "DebertaV2ForMaskedLM 5\n",
      "DebertaV2ForQuestionAnswering 13\n",
      "DebertaV2ForSequenceClassification 41\n",
      "DebertaV2ForTokenClassification 30\n",
      "DeformableDetrForObjectDetection 5\n",
      "DeiTForImageClassification 2\n",
      "DeiTForImageClassificationWithTeacher 4\n",
      "DetrForObjectDetection 12\n",
      "DetrForSegmentation 3\n",
      "DinatForImageClassification 5\n",
      "DistilBertForMaskedLM 166\n",
      "DistilBertForMultipleChoice 2\n",
      "DistilBertForQuestionAnswering 185\n",
      "DistilBertForSequenceClassification 804\n",
      "DistilBertForTokenClassification 106\n",
      "DistilBertModel 10\n",
      "EfficientFormerForImageClassificationWithTeacher 2\n",
      "ElectraForMaskedLM 24\n",
      "ElectraForPreTraining 34\n",
      "ElectraForQuestionAnswering 31\n",
      "ElectraForSequenceClassification 41\n",
      "ElectraForTokenClassification 19\n",
      "ElectraModel 1\n",
      "EncoderDecoderModel 92\n",
      "FNetForPreTraining 2\n",
      "FNetForSequenceClassification 15\n",
      "FSMTForConditionalGeneration 11\n",
      "FlaubertForSequenceClassification 2\n",
      "FlaubertModel 3\n",
      "FlaubertWithLMHeadModel 4\n",
      "FunnelBaseModel 5\n",
      "FunnelForSequenceClassification 2\n",
      "FunnelModel 5\n",
      "GPT2ForSequenceClassification 4\n",
      "GPT2LMHeadModel 375\n",
      "GPT2Model 6\n",
      "GPTJForCausalLM 17\n",
      "GPTJModel 2\n",
      "GPTNeoForCausalLM 27\n",
      "GPTNeoModel 4\n",
      "GPTNeoXForCausalLM 3\n",
      "GPTNeoXJapaneseForCausalLM 1\n",
      "HubertForCTC 38\n",
      "HubertForSequenceClassification 12\n",
      "HubertModel 4\n",
      "LEDForConditionalGeneration 22\n",
      "LayoutLMForTokenClassification 9\n",
      "LayoutLMv2ForTokenClassification 9\n",
      "LayoutLMv3ForTokenClassification 33\n",
      "LevitForImageClassificationWithTeacher 5\n",
      "LiltForTokenClassification 9\n",
      "LongT5ForConditionalGeneration 25\n",
      "LongformerForMaskedLM 7\n",
      "LongformerForMultipleChoice 2\n",
      "LongformerForQuestionAnswering 5\n",
      "LongformerForSequenceClassification 5\n",
      "M2M100ForConditionalGeneration 21\n",
      "MBartForConditionalGeneration 87\n",
      "MCTCTForCTC 1\n",
      "MPNetForMaskedLM 3\n",
      "MPNetForSequenceClassification 1\n",
      "MPNetModel 7\n",
      "MT5ForConditionalGeneration 281\n",
      "MarianMTModel 216\n",
      "MarkupLMForQuestionAnswering 2\n",
      "Mask2FormerForUniversalSegmentation 29\n",
      "MaskFormerForInstanceSegmentation 9\n",
      "MobileBertForQuestionAnswering 4\n",
      "MobileBertForSequenceClassification 11\n",
      "MobileNetV1ForImageClassification 4\n",
      "MobileNetV2ForImageClassification 6\n",
      "MobileNetV2ForSemanticSegmentation 2\n",
      "MobileViTForImageClassification 4\n",
      "MobileViTForSemanticSegmentation 4\n",
      "NatForImageClassification 2\n",
      "OPTForCausalLM 25\n",
      "OPTForSequenceClassification 2\n",
      "OneFormerForUniversalSegmentation 7\n",
      "PegasusForConditionalGeneration 105\n",
      "PerceiverForMaskedLM 1\n",
      "PerceiverForSequenceClassification 4\n",
      "PoolFormerForImageClassification 5\n",
      "ProphetNetForConditionalGeneration 2\n",
      "RagTokenForGeneration 2\n",
      "ReformerForMaskedLM 5\n",
      "RegNetForImageClassification 27\n",
      "ResNetForImageClassification 24\n",
      "ResNetModel 2\n",
      "RobertaForMaskedLM 228\n",
      "RobertaForMultipleChoice 5\n",
      "RobertaForQuestionAnswering 1\n",
      "RobertaForSequenceClassification 329\n",
      "RobertaForTokenClassification 164\n",
      "RobertaModel 295\n",
      "SEWDForCTC 4\n",
      "SEWDModel 8\n",
      "SEWForCTC 3\n",
      "SEWModel 3\n",
      "SegformerForImageClassification 9\n",
      "SegformerForSemanticSegmentation 35\n",
      "Speech2TextForConditionalGeneration 20\n",
      "SpeechEncoderDecoderModel 47\n",
      "SplinterForQuestionAnswering 14\n",
      "SqueezeBertForQuestionAnswering 3\n",
      "SwinForImageClassification 112\n",
      "Swinv2ForImageClassification 12\n",
      "SwitchTransformersForConditionalGeneration 12\n",
      "T5EncoderModel 4\n",
      "T5Model 5\n",
      "TapasForQuestionAnswering 19\n",
      "TapasForSequenceClassification 6\n",
      "UniSpeechForCTC 58\n",
      "UniSpeechSatForCTC 16\n",
      "VanForImageClassification 5\n",
      "ViTForImageClassification 156\n",
      "ViTMAEForPreTraining 3\n",
      "ViTMSNModel 5\n",
      "ViTModel 10\n",
      "VisionEncoderDecoderModel 37\n",
      "VisionTextDualEncoderModel 4\n",
      "Wav2Vec2ConformerForCTC 8\n",
      "Wav2Vec2ConformerForPreTraining 2\n",
      "Wav2Vec2ForCTC 1384\n",
      "Wav2Vec2ForPreTraining 39\n",
      "Wav2Vec2ForSequenceClassification 58\n",
      "Wav2Vec2Model 16\n",
      "WavLMForCTC 29\n",
      "WhisperForConditionalGeneration 507\n",
      "XCLIPModel 17\n",
      "XLMRobertaForQuestionAnswering 53\n",
      "XLMRobertaForSequenceClassification 80\n",
      "XLMRobertaForTokenClassification 213\n",
      "XLMRobertaModel 17\n",
      "XLNetForSequenceClassification 9\n",
      "XLNetLMHeadModel 4\n",
      "YolosForObjectDetection 7\n",
      "10189 195\n"
     ]
    }
   ],
   "source": [
    "total_cnt_model = 0\n",
    "cnt_model = 0\n",
    "cnt_type = 0\n",
    "for model_type in filtered_models:\n",
    "\n",
    "    cnt_model = 0\n",
    "    model_list = []\n",
    "    for model in filtered_models[model_type]:\n",
    "        if model is not None:\n",
    "            if model not in model_list:\n",
    "                cnt_model += 1\n",
    "                model_list.append(model)\n",
    "    if cnt_model > 0:\n",
    "        cnt_type += 1\n",
    "        print(model_type, cnt_model)\n",
    "        total_cnt_model += cnt_model\n",
    "\n",
    "print(total_cnt_model, cnt_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlbertModel 7\n",
      "AutoModel 5\n",
      "BertModel 92\n",
      "BloomModel 18\n",
      "CLIPModel 2\n",
      "CTRLLMHeadModel 5\n",
      "CamembertModel 2\n",
      "CanineModel 2\n",
      "ConvBertModel 5\n",
      "Data2VecAudioModel 2\n",
      "Data2VecTextModel 1\n",
      "Data2VecVisionModel 2\n",
      "DistilBertModel 10\n",
      "ElectraModel 1\n",
      "EncoderDecoderModel 92\n",
      "FlaubertModel 3\n",
      "FlaubertWithLMHeadModel 4\n",
      "FunnelBaseModel 5\n",
      "FunnelModel 5\n",
      "GPT2LMHeadModel 375\n",
      "GPT2Model 6\n",
      "GPTJModel 2\n",
      "GPTNeoModel 4\n",
      "HubertModel 4\n",
      "MPNetModel 7\n",
      "MarianMTModel 216\n",
      "ResNetModel 2\n",
      "RobertaModel 295\n",
      "SEWDModel 8\n",
      "SEWModel 3\n",
      "SpeechEncoderDecoderModel 47\n",
      "T5EncoderModel 4\n",
      "T5Model 5\n",
      "ViTMSNModel 5\n",
      "ViTModel 10\n",
      "VisionEncoderDecoderModel 37\n",
      "VisionTextDualEncoderModel 4\n",
      "Wav2Vec2Model 16\n",
      "XCLIPModel 17\n",
      "XLMRobertaModel 17\n",
      "XLNetLMHeadModel 4\n",
      "1351 41\n"
     ]
    }
   ],
   "source": [
    "total_cnt_model = 0\n",
    "cnt_model = 0\n",
    "cnt_type = 0\n",
    "for model_type in filtered_models:\n",
    "    if model_type.endswith('Model'):\n",
    "        cnt_model = 0\n",
    "        model_list = []\n",
    "        for model in filtered_models[model_type]:\n",
    "            if model is not None:\n",
    "                if model not in model_list:\n",
    "                    cnt_model += 1\n",
    "                    model_list.append(model)\n",
    "        if cnt_model > 0:\n",
    "            cnt_type += 1\n",
    "            print(model_type, cnt_model)\n",
    "            total_cnt_model += cnt_model\n",
    "\n",
    "print(total_cnt_model, cnt_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTMTorrent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
